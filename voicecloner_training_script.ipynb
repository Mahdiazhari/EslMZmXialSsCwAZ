{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install TTS","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:01:50.783666Z","iopub.execute_input":"2023-02-11T14:01:50.784024Z","iopub.status.idle":"2023-02-11T14:02:56.049333Z","shell.execute_reply.started":"2023-02-11T14:01:50.783994Z","shell.execute_reply":"2023-02-11T14:02:56.048116Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting TTS\n  Downloading TTS-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (604 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.1/604.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from TTS) (6.0)\nCollecting trainer==0.0.20\n  Downloading trainer-0.0.20-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gruut[de]==2.2.3\n  Downloading gruut-2.2.3.tar.gz (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from TTS) (3.8.1)\nRequirement already satisfied: fsspec>=2021.04.0 in /opt/conda/lib/python3.7/site-packages (from TTS) (2023.1.0)\nRequirement already satisfied: soundfile in /opt/conda/lib/python3.7/site-packages (from TTS) (0.11.0)\nCollecting g2pkk>=0.1.1\n  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\nCollecting inflect==5.6.0\n  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\nCollecting numba==0.55.1\n  Downloading numba-0.55.1-1-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pysbd\n  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unidic-lite==1.0.8\n  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from TTS) (1.3.5)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from TTS) (3.5.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from TTS) (23.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from TTS) (4.64.0)\nCollecting librosa==0.8.0\n  Downloading librosa-0.8.0.tar.gz (183 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.7/site-packages (from TTS) (0.42.1)\nRequirement already satisfied: numpy==1.21.6 in /opt/conda/lib/python3.7/site-packages (from TTS) (1.21.6)\nCollecting mecab-python3==1.0.5\n  Downloading mecab_python3-1.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (574 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m574.6/574.6 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flask in /opt/conda/lib/python3.7/site-packages (from TTS) (2.2.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (from TTS) (0.11.0)\nCollecting jamo\n  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\nCollecting umap-learn==0.5.1\n  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from TTS) (1.11.0)\nRequirement already satisfied: scipy>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from TTS) (1.7.3)\nCollecting cython==0.29.28\n  Downloading Cython-0.29.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting coqpit>=0.0.16\n  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\nCollecting anyascii\n  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pypinyin\n  Downloading pypinyin-0.48.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Babel<3.0.0,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from gruut[de]==2.2.3->TTS) (2.10.3)\nCollecting dateparser~=1.1.0\n  Downloading dateparser-1.1.7-py2.py3-none-any.whl (293 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0\n  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gruut_lang_en~=2.0.0\n  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jsonlines~=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gruut[de]==2.2.3->TTS) (1.2.0)\nRequirement already satisfied: networkx<3.0.0,>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from gruut[de]==2.2.3->TTS) (2.5)\nCollecting num2words<1.0.0,>=0.5.10\n  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-crfsuite~=0.9.7\n  Downloading python_crfsuite-0.9.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (966 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.9/966.9 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from gruut[de]==2.2.3->TTS) (5.10.2)\nCollecting gruut_lang_de~=2.0.0\n  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (3.0.0)\nRequirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (1.0.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (1.0.1)\nRequirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (5.1.1)\nRequirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (0.4.2)\nRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.7/site-packages (from librosa==0.8.0->TTS) (1.6.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba==0.55.1->TTS) (59.8.0)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba==0.55.1->TTS) (0.38.1)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from trainer==0.0.20->TTS) (3.19.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from trainer==0.0.20->TTS) (5.9.1)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from trainer==0.0.20->TTS) (2.5.1)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn==0.5.1->TTS) (0.5.8)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile->TTS) (1.15.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->TTS) (4.1.1)\nRequirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from flask->TTS) (8.1.3)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from flask->TTS) (2.1.2)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from flask->TTS) (6.0.0)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from flask->TTS) (2.2.2)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from flask->TTS) (3.1.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (1.4.3)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (4.33.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (2.8.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (9.1.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->TTS) (3.0.9)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->TTS) (2021.11.10)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->TTS) (2022.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile->TTS) (2.21)\nRequirement already satisfied: tzlocal in /opt/conda/lib/python3.7/site-packages (from dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (4.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->flask->TTS) (3.8.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->flask->TTS) (2.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from jsonlines~=1.2.0->gruut[de]==2.2.3->TTS) (1.15.0)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.7/site-packages (from num2words<1.0.0,>=0.5.10->gruut[de]==2.2.3->TTS) (0.6.2)\nRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa==0.8.0->TTS) (1.4.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from pooch>=1.0->librosa==0.8.0->TTS) (2.28.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0->TTS) (3.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0->TTS) (1.26.14)\nRequirement already satisfied: backports.zoneinfo in /opt/conda/lib/python3.7/site-packages (from tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (0.2.1)\nRequirement already satisfied: pytz-deprecation-shim in /opt/conda/lib/python3.7/site-packages (from tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (0.1.0.post0)\nRequirement already satisfied: tzdata in /opt/conda/lib/python3.7/site-packages (from pytz-deprecation-shim->tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (2022.7)\nBuilding wheels for collected packages: librosa, umap-learn, unidic-lite, gruut-ipa, gruut_lang_de, gruut_lang_en, gruut\n  Building wheel for librosa (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201396 sha256=4c23ec7c9fbfd16be8d384da637bbda3ba765679a127e67a2f616a6b73ffcf3d\n  Stored in directory: /root/.cache/pip/wheels/de/1e/aa/d91797ae7e1ce11853ee100bee9d1781ae9d750e7458c95afb\n  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76565 sha256=f8ff8a71d17c422120f83ffa32497d9102c5696bcf902f390f86bc2f4d103387\n  Stored in directory: /root/.cache/pip/wheels/01/e7/bb/347dc0e510803d7116a13d592b10cc68262da56a8eec4dd72f\n  Building wheel for unidic-lite (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658836 sha256=9aec7bd65e0b092ea49ae71c857dabaa2989e993e120ac5fae50aa2a66bddf4c\n  Stored in directory: /root/.cache/pip/wheels/de/69/b1/112140b599f2b13f609d485a99e357ba68df194d2079c5b1a2\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104895 sha256=d111dfda536ea34d9f570c6ed91b2ca7f1419eaa2861311106fe710c87f19316\n  Stored in directory: /root/.cache/pip/wheels/22/71/a1/5c393a68f798dfaea6c4f09e9a13b52cfedd4765db6a18400d\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498200 sha256=91d749bf0791f4abf98c392a40e8a414343c0b78b11c297586fd46c54b73805b\n  Stored in directory: /root/.cache/pip/wheels/96/e2/2f/fda506cfd22e4bd7435fb38ecf7760f9be1f2d10ab69933b4c\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297197 sha256=ef39619d6085bc1f7d24cafc80cc9e48fcac7550badae2beedd373ca85d3b836\n  Stored in directory: /root/.cache/pip/wheels/b1/82/a1/88bb90a6ee9896acfa37d9c2bc3dd56dee0b6c91ae13b0f0bf\n  Building wheel for gruut (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75820 sha256=4d8502f71a00051323f0005217d6ee9efbc4b1d5abfdce1bdde02713c2ee6673\n  Stored in directory: /root/.cache/pip/wheels/67/6a/ef/c07b4563108f071b917e6f9cda1052da696906a0ed2357dcfb\nSuccessfully built librosa umap-learn unidic-lite gruut-ipa gruut_lang_de gruut_lang_en gruut\nInstalling collected packages: unidic-lite, python-crfsuite, mecab-python3, jamo, gruut_lang_en, gruut_lang_de, pysbd, pypinyin, numba, num2words, inflect, gruut-ipa, cython, coqpit, anyascii, trainer, librosa, dateparser, umap-learn, gruut, g2pkk, TTS\n  Attempting uninstall: numba\n    Found existing installation: numba 0.55.2\n    Uninstalling numba-0.55.2:\n      Successfully uninstalled numba-0.55.2\n  Attempting uninstall: cython\n    Found existing installation: Cython 0.29.33\n    Uninstalling Cython-0.29.33:\n      Successfully uninstalled Cython-0.29.33\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.9.2\n    Uninstalling librosa-0.9.2:\n      Successfully uninstalled librosa-0.9.2\n  Attempting uninstall: umap-learn\n    Found existing installation: umap-learn 0.5.3\n    Uninstalling umap-learn-0.5.3:\n      Successfully uninstalled umap-learn-0.5.3\nSuccessfully installed TTS-0.11.1 anyascii-0.3.1 coqpit-0.0.17 cython-0.29.28 dateparser-1.1.7 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 inflect-5.6.0 jamo-0.4.1 librosa-0.8.0 mecab-python3-1.0.5 num2words-0.5.12 numba-0.55.1 pypinyin-0.48.0 pysbd-0.3.4 python-crfsuite-0.9.9 trainer-0.0.20 umap-learn-0.5.1 unidic-lite-1.0.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"dataset_path = '../input/the-lj-speech-dataset/LJSpeech-1.1/'","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:16.288866Z","iopub.execute_input":"2023-02-11T14:03:16.289252Z","iopub.status.idle":"2023-02-11T14:03:16.294591Z","shell.execute_reply.started":"2023-02-11T14:03:16.289216Z","shell.execute_reply":"2023-02-11T14:03:16.293196Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:56:02.489432Z","iopub.execute_input":"2023-02-11T13:56:02.489820Z","iopub.status.idle":"2023-02-11T13:56:03.480355Z","shell.execute_reply.started":"2023-02-11T13:56:02.489771Z","shell.execute_reply":"2023-02-11T13:56:03.479215Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ..\n%pwd","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:56:07.049800Z","iopub.execute_input":"2023-02-11T13:56:07.050223Z","iopub.status.idle":"2023-02-11T13:56:07.066094Z","shell.execute_reply.started":"2023-02-11T13:56:07.050177Z","shell.execute_reply":"2023-02-11T13:56:07.065079Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle'"},"metadata":{}}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:56:08.810745Z","iopub.execute_input":"2023-02-11T13:56:08.811129Z","iopub.status.idle":"2023-02-11T13:56:09.832451Z","shell.execute_reply.started":"2023-02-11T13:56:08.811098Z","shell.execute_reply":"2023-02-11T13:56:09.831223Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"input  lib  working\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd working","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:56:11.131223Z","iopub.execute_input":"2023-02-11T13:56:11.131662Z","iopub.status.idle":"2023-02-11T13:56:11.140999Z","shell.execute_reply.started":"2023-02-11T13:56:11.131625Z","shell.execute_reply":"2023-02-11T13:56:11.139650Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"%pwd","metadata":{"execution":{"iopub.status.busy":"2023-02-11T13:56:13.054369Z","iopub.execute_input":"2023-02-11T13:56:13.054774Z","iopub.status.idle":"2023-02-11T13:56:13.069222Z","shell.execute_reply.started":"2023-02-11T13:56:13.054735Z","shell.execute_reply":"2023-02-11T13:56:13.065406Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"output_path = './output/fakeljspeech/'","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:19.131572Z","iopub.execute_input":"2023-02-11T14:03:19.132551Z","iopub.status.idle":"2023-02-11T14:03:19.140789Z","shell.execute_reply.started":"2023-02-11T14:03:19.132514Z","shell.execute_reply":"2023-02-11T14:03:19.136159Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Trainer: Where the ✨️ happens.\n# TrainingArgs: Defines the set of arguments of the Trainer.\nfrom trainer import Trainer, TrainerArgs\n\n# GlowTTSConfig: all model related values for training, validating and testing.\nfrom TTS.tts.configs.glow_tts_config import GlowTTSConfig\n\n# BaseDatasetConfig: defines name, formatter and path of the dataset.\nfrom TTS.tts.configs.shared_configs import BaseDatasetConfig\nfrom TTS.tts.datasets import load_tts_samples\nfrom TTS.tts.models.glow_tts import GlowTTS\nfrom TTS.tts.utils.text.tokenizer import TTSTokenizer\nfrom TTS.utils.audio import AudioProcessor","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:20.987454Z","iopub.execute_input":"2023-02-11T14:03:20.987833Z","iopub.status.idle":"2023-02-11T14:03:25.330085Z","shell.execute_reply.started":"2023-02-11T14:03:20.987802Z","shell.execute_reply":"2023-02-11T14:03:25.329033Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=dataset_path\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:30.854048Z","iopub.execute_input":"2023-02-11T14:03:30.854696Z","iopub.status.idle":"2023-02-11T14:03:30.863863Z","shell.execute_reply.started":"2023-02-11T14:03:30.854660Z","shell.execute_reply":"2023-02-11T14:03:30.862668Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"config = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n    print_step=25,\n    print_eval=False,\n    mixed_precision=True,\n    output_path=output_path,\n    datasets=[dataset_config],\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:36.512594Z","iopub.execute_input":"2023-02-11T14:03:36.512976Z","iopub.status.idle":"2023-02-11T14:03:36.520085Z","shell.execute_reply.started":"2023-02-11T14:03:36.512944Z","shell.execute_reply":"2023-02-11T14:03:36.518980Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n\n# INITIALIZE THE MODEL\n# Models take a config object and a speaker manager as input\n# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n# Speaker manager is used by multi-speaker models.\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the 🐸TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n\n# AND... 3,2,1... 🚀\ntrainer.fit()","metadata":{"execution":{"iopub.status.busy":"2023-02-11T14:03:38.355131Z","iopub.execute_input":"2023-02-11T14:03:38.355499Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" > Setting up Audio Processor...\n | > sample_rate:22050\n | > resample:False\n | > num_mels:80\n | > log_func:np.log10\n | > min_level_db:-100\n | > frame_shift_ms:None\n | > frame_length_ms:None\n | > ref_level_db:20\n | > fft_size:1024\n | > power:1.5\n | > preemphasis:0.0\n | > griffin_lim_iters:60\n | > signal_norm:True\n | > symmetric_norm:True\n | > mel_fmin:0\n | > mel_fmax:None\n | > pitch_fmin:1.0\n | > pitch_fmax:640.0\n | > spec_gain:20.0\n | > stft_pad_mode:reflect\n | > max_norm:4.0\n | > clip_norm:True\n | > do_trim_silence:True\n | > trim_db:45\n | > do_sound_norm:False\n | > do_amp_to_db_linear:True\n | > do_amp_to_db_mel:True\n | > do_rms_norm:False\n | > db_level:None\n | > stats_path:None\n | > base:10\n | > hop_length:256\n | > win_length:1024\n | > Found 13100 files in /kaggle/input/the-lj-speech-dataset/LJSpeech-1.1\n","output_type":"stream"},{"name":"stderr","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n > Training Environment:\n | > Current device: 0\n | > Num. of GPUs: 1\n | > Num. of CPUs: 2\n | > Num. of Torch Threads: 1\n | > Torch seed: 54321\n | > Torch CUDNN: True\n | > Torch CUDNN deterministic: False\n | > Torch CUDNN benchmark: False\n > Start Tensorboard: tensorboard --logdir=./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n\n > Model has 28610257 parameters\n\n\u001b[4m\u001b[1m > EPOCH: 0/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"[*] Pre-computing phonemes...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 3/12969 [00:00<35:23,  6.10it/s]","output_type":"stream"},{"name":"stdout","text":"ɪnstɛd əv weɪtɪŋ ðɛɹ, ɔzwɔld əpɛɹəntli wɛnt æz fɑɹ əweɪ æz hi kʊd ænd bɔɹdɪd ðə fɚst oʊk klɪf bʌs wɪt͡ʃ keɪm əlɔŋ\n [!] Character '͡' not found in the vocabulary. Discarding it.\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 2058/12969 [01:33<06:49, 26.67it/s]","output_type":"stream"},{"name":"stdout","text":"ɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n [!] Character '“' not found in the vocabulary. Discarding it.\nɪntu ðə “kɹeɪtɚ” dʌɡ aʊt ɪn ðə mɪdəl, pɔɹ ðə spʌnd͡ʒ, wɔɹm wɔtɚ, ðə məlæsɪz, ænd soʊdə dɪzɑlvd ɪn hɑt wɔtɚ.\n [!] Character '”' not found in the vocabulary. Discarding it.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 12969/12969 [06:07<00:00, 35.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\n\u001b[1m > TRAINING (2023-02-11 14:10:51) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 0/406 -- GLOBAL_STEP: 0\u001b[0m\n     | > current_lr: 0.00000 \n     | > step_time: 9.21460  (9.21456)\n     | > loader_time: 1.59720  (1.59716)\n\n\n\u001b[1m   --> STEP: 25/406 -- GLOBAL_STEP: 25\u001b[0m\n     | > loss: 3.71746  (3.62867)\n     | > log_mle: 0.88299  (0.88335)\n     | > loss_dur: 2.83446  (2.74533)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.71358  (8.92499)\n     | > current_lr: 0.00000 \n     | > step_time: 0.47710  (0.55598)\n     | > loader_time: 0.01660  (0.00609)\n\n\n\u001b[1m   --> STEP: 50/406 -- GLOBAL_STEP: 50\u001b[0m\n     | > loss: 3.72260  (3.61799)\n     | > log_mle: 0.88153  (0.88476)\n     | > loss_dur: 2.84107  (2.73323)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.68308  (9.31956)\n     | > current_lr: 0.00000 \n     | > step_time: 0.58490  (0.58847)\n     | > loader_time: 0.00380  (0.00681)\n\n\n\u001b[1m   --> STEP: 75/406 -- GLOBAL_STEP: 75\u001b[0m\n     | > loss: 3.67644  (3.61954)\n     | > log_mle: 0.89286  (0.88523)\n     | > loss_dur: 2.78358  (2.73431)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.61416  (9.42401)\n     | > current_lr: 0.00000 \n     | > step_time: 0.89760  (0.63001)\n     | > loader_time: 0.00640  (0.00742)\n\n\n\u001b[1m   --> STEP: 100/406 -- GLOBAL_STEP: 100\u001b[0m\n     | > loss: 3.63057  (3.61380)\n     | > log_mle: 0.89006  (0.88541)\n     | > loss_dur: 2.74051  (2.72839)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.68847  (9.46150)\n     | > current_lr: 0.00000 \n     | > step_time: 0.99560  (0.65543)\n     | > loader_time: 0.00550  (0.00763)\n\n\n\u001b[1m   --> STEP: 125/406 -- GLOBAL_STEP: 125\u001b[0m\n     | > loss: 3.60275  (3.60899)\n     | > log_mle: 0.88892  (0.88541)\n     | > loss_dur: 2.71383  (2.72358)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.49680  (9.47852)\n     | > current_lr: 0.00000 \n     | > step_time: 0.53900  (0.68147)\n     | > loader_time: 0.00700  (0.00804)\n\n\n\u001b[1m   --> STEP: 150/406 -- GLOBAL_STEP: 150\u001b[0m\n     | > loss: 3.60579  (3.60922)\n     | > log_mle: 0.88969  (0.88545)\n     | > loss_dur: 2.71610  (2.72378)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.56777  (9.49250)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87050  (0.70873)\n     | > loader_time: 0.01760  (0.00824)\n\n\n\u001b[1m   --> STEP: 175/406 -- GLOBAL_STEP: 175\u001b[0m\n     | > loss: 3.59648  (3.60804)\n     | > log_mle: 0.88433  (0.88536)\n     | > loss_dur: 2.71215  (2.72268)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.48436  (9.49714)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78140  (0.72814)\n     | > loader_time: 0.00680  (0.00835)\n\n\n\u001b[1m   --> STEP: 200/406 -- GLOBAL_STEP: 200\u001b[0m\n     | > loss: 3.55511  (3.60575)\n     | > log_mle: 0.88853  (0.88534)\n     | > loss_dur: 2.66658  (2.72041)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.39497  (9.49439)\n     | > current_lr: 0.00000 \n     | > step_time: 1.05710  (0.75263)\n     | > loader_time: 0.01280  (0.00876)\n\n\n\u001b[1m   --> STEP: 225/406 -- GLOBAL_STEP: 225\u001b[0m\n     | > loss: 3.63080  (3.60371)\n     | > log_mle: 0.88316  (0.88544)\n     | > loss_dur: 2.74764  (2.71827)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.51486  (9.49125)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01640  (0.77388)\n     | > loader_time: 0.00900  (0.00919)\n\n\n\u001b[1m   --> STEP: 250/406 -- GLOBAL_STEP: 250\u001b[0m\n     | > loss: 3.68420  (3.60368)\n     | > log_mle: 0.88716  (0.88554)\n     | > loss_dur: 2.79703  (2.71813)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.58696  (9.48928)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92920  (0.79709)\n     | > loader_time: 0.01170  (0.00934)\n\n\n\u001b[1m   --> STEP: 275/406 -- GLOBAL_STEP: 275\u001b[0m\n     | > loss: 3.56377  (3.60318)\n     | > log_mle: 0.88709  (0.88534)\n     | > loss_dur: 2.67668  (2.71784)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.35812  (9.48681)\n     | > current_lr: 0.00000 \n     | > step_time: 1.33150  (0.81937)\n     | > loader_time: 0.03620  (0.01003)\n\n\n\u001b[1m   --> STEP: 300/406 -- GLOBAL_STEP: 300\u001b[0m\n     | > loss: 3.56995  (3.60153)\n     | > log_mle: 0.88212  (0.88524)\n     | > loss_dur: 2.68783  (2.71629)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.34167  (9.47765)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92750  (0.83863)\n     | > loader_time: 0.00890  (0.01082)\n\n\n\u001b[1m   --> STEP: 325/406 -- GLOBAL_STEP: 325\u001b[0m\n     | > loss: 3.55152  (3.59972)\n     | > log_mle: 0.88329  (0.88512)\n     | > loss_dur: 2.66823  (2.71460)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.28833  (9.46873)\n     | > current_lr: 0.00000 \n     | > step_time: 1.16740  (0.86401)\n     | > loader_time: 0.01430  (0.01163)\n\n\n\u001b[1m   --> STEP: 350/406 -- GLOBAL_STEP: 350\u001b[0m\n     | > loss: 3.54645  (3.60002)\n     | > log_mle: 0.88199  (0.88503)\n     | > loss_dur: 2.66445  (2.71498)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.32173  (9.46197)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14300  (0.88668)\n     | > loader_time: 0.02190  (0.01229)\n\n\n\u001b[1m   --> STEP: 375/406 -- GLOBAL_STEP: 375\u001b[0m\n     | > loss: 3.60835  (3.59774)\n     | > log_mle: 0.87955  (0.88485)\n     | > loss_dur: 2.72880  (2.71289)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.34577  (9.45037)\n     | > current_lr: 0.00000 \n     | > step_time: 1.20020  (0.91027)\n     | > loader_time: 0.01500  (0.01307)\n\n\n\u001b[1m   --> STEP: 400/406 -- GLOBAL_STEP: 400\u001b[0m\n     | > loss: 3.56493  (3.59494)\n     | > log_mle: 0.88125  (0.88480)\n     | > loss_dur: 2.68368  (2.71014)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.21729  (9.43586)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92960  (0.92868)\n     | > loader_time: 0.00610  (0.01353)\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time: 0.00590 \u001b[0m(+0.00000)\n     | > avg_loss: 3.60117 \u001b[0m(+0.00000)\n     | > avg_log_mle: 0.88140 \u001b[0m(+0.00000)\n     | > avg_loss_dur: 2.71977 \u001b[0m(+0.00000)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_406.pth\n\n\u001b[4m\u001b[1m > EPOCH: 1/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\n\u001b[1m > TRAINING (2023-02-11 14:17:51) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 19/406 -- GLOBAL_STEP: 425\u001b[0m\n     | > loss: 3.55607  (3.61080)\n     | > log_mle: 0.88083  (0.87667)\n     | > loss_dur: 2.67524  (2.73413)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.17775  (9.13866)\n     | > current_lr: 0.00000 \n     | > step_time: 0.53120  (0.56164)\n     | > loader_time: 0.00330  (0.00447)\n\n\n\u001b[1m   --> STEP: 44/406 -- GLOBAL_STEP: 450\u001b[0m\n     | > loss: 3.45968  (3.55876)\n     | > log_mle: 0.88254  (0.87911)\n     | > loss_dur: 2.57714  (2.67965)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.88986  (9.09037)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60410  (0.57332)\n     | > loader_time: 0.01280  (0.00493)\n\n\n\u001b[1m   --> STEP: 69/406 -- GLOBAL_STEP: 475\u001b[0m\n     | > loss: 3.62513  (3.55293)\n     | > log_mle: 0.88810  (0.87985)\n     | > loss_dur: 2.73703  (2.67308)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.24880  (9.08716)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69260  (0.60313)\n     | > loader_time: 0.01130  (0.00527)\n\n\n\u001b[1m   --> STEP: 94/406 -- GLOBAL_STEP: 500\u001b[0m\n     | > loss: 3.46167  (3.54243)\n     | > log_mle: 0.88143  (0.87998)\n     | > loss_dur: 2.58023  (2.66245)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.88176  (9.06354)\n     | > current_lr: 0.00000 \n     | > step_time: 0.62120  (0.61537)\n     | > loader_time: 0.00430  (0.00541)\n\n\n\u001b[1m   --> STEP: 119/406 -- GLOBAL_STEP: 525\u001b[0m\n     | > loss: 3.51628  (3.53605)\n     | > log_mle: 0.87788  (0.88010)\n     | > loss_dur: 2.63840  (2.65595)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.91859  (9.04865)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69700  (0.63798)\n     | > loader_time: 0.00430  (0.00578)\n\n\n\u001b[1m   --> STEP: 144/406 -- GLOBAL_STEP: 550\u001b[0m\n     | > loss: 3.53927  (3.53389)\n     | > log_mle: 0.87933  (0.88010)\n     | > loss_dur: 2.65995  (2.65379)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.03230  (9.03299)\n     | > current_lr: 0.00000 \n     | > step_time: 0.75420  (0.65428)\n     | > loader_time: 0.00440  (0.00601)\n\n\n\u001b[1m   --> STEP: 169/406 -- GLOBAL_STEP: 575\u001b[0m\n     | > loss: 3.48350  (3.52852)\n     | > log_mle: 0.88754  (0.88008)\n     | > loss_dur: 2.59596  (2.64844)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.77320  (9.01070)\n     | > current_lr: 0.00000 \n     | > step_time: 0.76920  (0.67365)\n     | > loader_time: 0.01310  (0.00620)\n\n\n\u001b[1m   --> STEP: 194/406 -- GLOBAL_STEP: 600\u001b[0m\n     | > loss: 3.53305  (3.52515)\n     | > log_mle: 0.88266  (0.87992)\n     | > loss_dur: 2.65039  (2.64523)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.94583  (8.99025)\n     | > current_lr: 0.00000 \n     | > step_time: 0.80950  (0.69482)\n     | > loader_time: 0.00750  (0.00647)\n\n\n\u001b[1m   --> STEP: 219/406 -- GLOBAL_STEP: 625\u001b[0m\n     | > loss: 3.39987  (3.52025)\n     | > log_mle: 0.88258  (0.87988)\n     | > loss_dur: 2.51729  (2.64037)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.59628  (8.96601)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87450  (0.71521)\n     | > loader_time: 0.00860  (0.00649)\n\n\n\u001b[1m   --> STEP: 244/406 -- GLOBAL_STEP: 650\u001b[0m\n     | > loss: 3.48627  (3.51797)\n     | > log_mle: 0.87776  (0.87988)\n     | > loss_dur: 2.60852  (2.63809)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.71755  (8.94522)\n     | > current_lr: 0.00000 \n     | > step_time: 0.96240  (0.73857)\n     | > loader_time: 0.00590  (0.00655)\n\n\n\u001b[1m   --> STEP: 269/406 -- GLOBAL_STEP: 675\u001b[0m\n     | > loss: 3.46505  (3.51458)\n     | > log_mle: 0.88235  (0.87961)\n     | > loss_dur: 2.58270  (2.63497)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.58660  (8.92203)\n     | > current_lr: 0.00000 \n     | > step_time: 0.98950  (0.76270)\n     | > loader_time: 0.00580  (0.00679)\n\n\n\u001b[1m   --> STEP: 294/406 -- GLOBAL_STEP: 700\u001b[0m\n     | > loss: 3.44823  (3.51175)\n     | > log_mle: 0.87672  (0.87941)\n     | > loss_dur: 2.57151  (2.63234)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.48121  (8.89830)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11430  (0.78718)\n     | > loader_time: 0.01100  (0.00694)\n\n\n\u001b[1m   --> STEP: 319/406 -- GLOBAL_STEP: 725\u001b[0m\n     | > loss: 3.45459  (3.50813)\n     | > log_mle: 0.87242  (0.87920)\n     | > loss_dur: 2.58217  (2.62892)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.52832  (8.87243)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11870  (0.81621)\n     | > loader_time: 0.01490  (0.00754)\n\n\n\u001b[1m   --> STEP: 344/406 -- GLOBAL_STEP: 750\u001b[0m\n     | > loss: 3.45724  (3.50683)\n     | > log_mle: 0.87382  (0.87901)\n     | > loss_dur: 2.58341  (2.62782)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.50492  (8.85079)\n     | > current_lr: 0.00000 \n     | > step_time: 1.47250  (0.84480)\n     | > loader_time: 0.02760  (0.00791)\n\n\n\u001b[1m   --> STEP: 369/406 -- GLOBAL_STEP: 775\u001b[0m\n     | > loss: 3.40340  (3.50352)\n     | > log_mle: 0.87167  (0.87874)\n     | > loss_dur: 2.53173  (2.62478)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.34149  (8.82465)\n     | > current_lr: 0.00000 \n     | > step_time: 1.51540  (0.87629)\n     | > loader_time: 0.00730  (0.00825)\n\n\n\u001b[1m   --> STEP: 394/406 -- GLOBAL_STEP: 800\u001b[0m\n     | > loss: 3.47112  (3.49938)\n     | > log_mle: 0.88028  (0.87854)\n     | > loss_dur: 2.59084  (2.62085)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.41741  (8.79582)\n     | > current_lr: 0.00000 \n     | > step_time: 1.65970  (0.90382)\n     | > loader_time: 0.00650  (0.00838)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00602 \u001b[0m(+0.00013)\n     | > avg_loss:\u001b[92m 3.45304 \u001b[0m(-0.14813)\n     | > avg_log_mle:\u001b[92m 0.87280 \u001b[0m(-0.00860)\n     | > avg_loss_dur:\u001b[92m 2.58024 \u001b[0m(-0.13953)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_812.pth\n\n\u001b[4m\u001b[1m > EPOCH: 2/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 14:24:25) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 13/406 -- GLOBAL_STEP: 825\u001b[0m\n     | > loss: 3.44090  (3.48478)\n     | > log_mle: 0.86266  (0.86834)\n     | > loss_dur: 2.57824  (2.61644)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.21180  (8.19039)\n     | > current_lr: 0.00000 \n     | > step_time: 0.49140  (0.50355)\n     | > loader_time: 0.00360  (0.00406)\n\n\n\u001b[1m   --> STEP: 38/406 -- GLOBAL_STEP: 850\u001b[0m\n     | > loss: 3.43919  (3.43471)\n     | > log_mle: 0.87234  (0.87089)\n     | > loss_dur: 2.56685  (2.56382)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.07025  (8.12421)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60970  (0.55521)\n     | > loader_time: 0.00510  (0.00476)\n\n\n\u001b[1m   --> STEP: 63/406 -- GLOBAL_STEP: 875\u001b[0m\n     | > loss: 3.38089  (3.42180)\n     | > log_mle: 0.86801  (0.87106)\n     | > loss_dur: 2.51289  (2.55074)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.88814  (8.07739)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59670  (0.58620)\n     | > loader_time: 0.00670  (0.00516)\n\n\n\u001b[1m   --> STEP: 88/406 -- GLOBAL_STEP: 900\u001b[0m\n     | > loss: 3.35836  (3.41194)\n     | > log_mle: 0.87170  (0.87098)\n     | > loss_dur: 2.48666  (2.54095)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.83592  (8.02127)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66790  (0.60892)\n     | > loader_time: 0.00670  (0.00522)\n\n\n\u001b[1m   --> STEP: 113/406 -- GLOBAL_STEP: 925\u001b[0m\n     | > loss: 3.38527  (3.39749)\n     | > log_mle: 0.86928  (0.87070)\n     | > loss_dur: 2.51599  (2.52679)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.69043  (7.94971)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72010  (0.63241)\n     | > loader_time: 0.00550  (0.00568)\n\n\n\u001b[1m   --> STEP: 138/406 -- GLOBAL_STEP: 950\u001b[0m\n     | > loss: 3.36707  (3.39011)\n     | > log_mle: 0.86541  (0.87004)\n     | > loss_dur: 2.50165  (2.52008)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.54678  (7.88587)\n     | > current_lr: 0.00000 \n     | > step_time: 0.76070  (0.65476)\n     | > loader_time: 0.00480  (0.00589)\n\n\n\u001b[1m   --> STEP: 163/406 -- GLOBAL_STEP: 975\u001b[0m\n     | > loss: 3.31426  (3.38415)\n     | > log_mle: 0.86417  (0.86936)\n     | > loss_dur: 2.45008  (2.51479)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.34499  (7.82555)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78310  (0.67583)\n     | > loader_time: 0.00530  (0.00607)\n\n\n\u001b[1m   --> STEP: 188/406 -- GLOBAL_STEP: 1000\u001b[0m\n     | > loss: 3.47164  (3.37702)\n     | > log_mle: 0.86263  (0.86859)\n     | > loss_dur: 2.60901  (2.50843)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.44850  (7.75824)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93820  (0.69560)\n     | > loader_time: 0.00560  (0.00613)\n\n\n\u001b[1m   --> STEP: 213/406 -- GLOBAL_STEP: 1025\u001b[0m\n     | > loss: 3.26249  (3.36983)\n     | > log_mle: 0.85941  (0.86784)\n     | > loss_dur: 2.40308  (2.50199)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.04777  (7.69179)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02590  (0.71732)\n     | > loader_time: 0.01400  (0.00621)\n\n\n\u001b[1m   --> STEP: 238/406 -- GLOBAL_STEP: 1050\u001b[0m\n     | > loss: 3.33248  (3.36285)\n     | > log_mle: 0.86137  (0.86722)\n     | > loss_dur: 2.47111  (2.49563)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.03432  (7.62339)\n     | > current_lr: 0.00000 \n     | > step_time: 0.96380  (0.73870)\n     | > loader_time: 0.00630  (0.00626)\n\n\n\u001b[1m   --> STEP: 263/406 -- GLOBAL_STEP: 1075\u001b[0m\n     | > loss: 3.25863  (3.35559)\n     | > log_mle: 0.85830  (0.86625)\n     | > loss_dur: 2.40033  (2.48934)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.75389  (7.55294)\n     | > current_lr: 0.00000 \n     | > step_time: 0.97300  (0.76224)\n     | > loader_time: 0.00560  (0.00644)\n\n\n\u001b[1m   --> STEP: 288/406 -- GLOBAL_STEP: 1100\u001b[0m\n     | > loss: 3.22497  (3.34878)\n     | > log_mle: 0.84960  (0.86523)\n     | > loss_dur: 2.37538  (2.48355)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.65331  (7.48159)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07620  (0.78425)\n     | > loader_time: 0.00680  (0.00656)\n\n\n\u001b[1m   --> STEP: 313/406 -- GLOBAL_STEP: 1125\u001b[0m\n     | > loss: 3.25348  (3.34257)\n     | > log_mle: 0.85085  (0.86426)\n     | > loss_dur: 2.40263  (2.47831)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.46575  (7.41136)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06690  (0.80752)\n     | > loader_time: 0.00960  (0.00671)\n\n\n\u001b[1m   --> STEP: 338/406 -- GLOBAL_STEP: 1150\u001b[0m\n     | > loss: 3.23481  (3.33632)\n     | > log_mle: 0.84957  (0.86315)\n     | > loss_dur: 2.38524  (2.47317)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.35001  (7.34180)\n     | > current_lr: 0.00000 \n     | > step_time: 1.13620  (0.83105)\n     | > loader_time: 0.00610  (0.00699)\n\n\n\u001b[1m   --> STEP: 363/406 -- GLOBAL_STEP: 1175\u001b[0m\n     | > loss: 3.20589  (3.33029)\n     | > log_mle: 0.84659  (0.86203)\n     | > loss_dur: 2.35931  (2.46826)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.24522  (7.27280)\n     | > current_lr: 0.00000 \n     | > step_time: 1.22490  (0.85659)\n     | > loader_time: 0.01250  (0.00751)\n\n\n\u001b[1m   --> STEP: 388/406 -- GLOBAL_STEP: 1200\u001b[0m\n     | > loss: 3.21057  (3.32307)\n     | > log_mle: 0.84817  (0.86091)\n     | > loss_dur: 2.36240  (2.46216)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.16219  (7.20355)\n     | > current_lr: 0.00000 \n     | > step_time: 1.18200  (0.88567)\n     | > loader_time: 0.00710  (0.00794)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00696 \u001b[0m(+0.00094)\n     | > avg_loss:\u001b[92m 3.18126 \u001b[0m(-0.27178)\n     | > avg_log_mle:\u001b[92m 0.83905 \u001b[0m(-0.03375)\n     | > avg_loss_dur:\u001b[92m 2.34221 \u001b[0m(-0.23803)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_1218.pth\n\n\u001b[4m\u001b[1m > EPOCH: 3/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 14:30:55) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 7/406 -- GLOBAL_STEP: 1225\u001b[0m\n     | > loss: 3.13585  (3.28215)\n     | > log_mle: 0.83946  (0.83792)\n     | > loss_dur: 2.29639  (2.44423)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.93590  (6.12038)\n     | > current_lr: 0.00000 \n     | > step_time: 0.51860  (0.49834)\n     | > loader_time: 0.00540  (0.00383)\n\n\n\u001b[1m   --> STEP: 32/406 -- GLOBAL_STEP: 1250\u001b[0m\n     | > loss: 3.25690  (3.24069)\n     | > log_mle: 0.83982  (0.84033)\n     | > loss_dur: 2.41708  (2.40036)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.88808  (6.00189)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59180  (0.56483)\n     | > loader_time: 0.00460  (0.00437)\n\n\n\u001b[1m   --> STEP: 57/406 -- GLOBAL_STEP: 1275\u001b[0m\n     | > loss: 3.26570  (3.22735)\n     | > log_mle: 0.83624  (0.83936)\n     | > loss_dur: 2.42946  (2.38799)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.90818  (5.92345)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60260  (0.58217)\n     | > loader_time: 0.00390  (0.00483)\n\n\n\u001b[1m   --> STEP: 82/406 -- GLOBAL_STEP: 1300\u001b[0m\n     | > loss: 3.15808  (3.22064)\n     | > log_mle: 0.83060  (0.83769)\n     | > loss_dur: 2.32748  (2.38295)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.62864  (5.86678)\n     | > current_lr: 0.00000 \n     | > step_time: 0.67190  (0.60801)\n     | > loader_time: 0.00520  (0.00528)\n\n\n\u001b[1m   --> STEP: 107/406 -- GLOBAL_STEP: 1325\u001b[0m\n     | > loss: 3.10938  (3.20853)\n     | > log_mle: 0.82779  (0.83563)\n     | > loss_dur: 2.28159  (2.37290)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.59457  (5.81577)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68590  (0.62622)\n     | > loader_time: 0.01870  (0.00560)\n\n\n\u001b[1m   --> STEP: 132/406 -- GLOBAL_STEP: 1350\u001b[0m\n     | > loss: 3.19742  (3.20178)\n     | > log_mle: 0.81654  (0.83325)\n     | > loss_dur: 2.38089  (2.36852)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.59882  (5.77331)\n     | > current_lr: 0.00000 \n     | > step_time: 0.76620  (0.64889)\n     | > loader_time: 0.00770  (0.00595)\n\n\n\u001b[1m   --> STEP: 157/406 -- GLOBAL_STEP: 1375\u001b[0m\n     | > loss: 3.23930  (3.19864)\n     | > log_mle: 0.81756  (0.83103)\n     | > loss_dur: 2.42174  (2.36760)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.62023  (5.73990)\n     | > current_lr: 0.00000 \n     | > step_time: 0.74010  (0.66541)\n     | > loader_time: 0.00790  (0.00613)\n\n\n\u001b[1m   --> STEP: 182/406 -- GLOBAL_STEP: 1400\u001b[0m\n     | > loss: 3.18070  (3.19354)\n     | > log_mle: 0.80580  (0.82861)\n     | > loss_dur: 2.37491  (2.36493)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.49226  (5.70359)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93970  (0.68884)\n     | > loader_time: 0.00620  (0.00637)\n\n\n\u001b[1m   --> STEP: 207/406 -- GLOBAL_STEP: 1425\u001b[0m\n     | > loss: 3.17025  (3.18872)\n     | > log_mle: 0.80439  (0.82630)\n     | > loss_dur: 2.36586  (2.36243)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.45337  (5.67052)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90570  (0.71297)\n     | > loader_time: 0.02250  (0.00679)\n\n\n\u001b[1m   --> STEP: 232/406 -- GLOBAL_STEP: 1450\u001b[0m\n     | > loss: 3.12458  (3.18350)\n     | > log_mle: 0.79704  (0.82399)\n     | > loss_dur: 2.32754  (2.35950)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.37141  (5.64016)\n     | > current_lr: 0.00000 \n     | > step_time: 0.84200  (0.73391)\n     | > loader_time: 0.00540  (0.00683)\n\n\n\u001b[1m   --> STEP: 257/406 -- GLOBAL_STEP: 1475\u001b[0m\n     | > loss: 3.11514  (3.17993)\n     | > log_mle: 0.79601  (0.82159)\n     | > loss_dur: 2.31913  (2.35834)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.32786  (5.61324)\n     | > current_lr: 0.00000 \n     | > step_time: 1.05490  (0.76116)\n     | > loader_time: 0.00590  (0.00707)\n\n\n\u001b[1m   --> STEP: 282/406 -- GLOBAL_STEP: 1500\u001b[0m\n     | > loss: 3.11198  (3.17587)\n     | > log_mle: 0.79323  (0.81909)\n     | > loss_dur: 2.31875  (2.35678)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.29540  (5.58829)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02520  (0.78591)\n     | > loader_time: 0.01170  (0.00736)\n\n\n\u001b[1m   --> STEP: 307/406 -- GLOBAL_STEP: 1525\u001b[0m\n     | > loss: 3.11468  (3.17241)\n     | > log_mle: 0.78613  (0.81657)\n     | > loss_dur: 2.32855  (2.35584)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.28363  (5.56623)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14710  (0.81230)\n     | > loader_time: 0.01420  (0.00778)\n\n\n\u001b[1m   --> STEP: 332/406 -- GLOBAL_STEP: 1550\u001b[0m\n     | > loss: 3.11068  (3.16788)\n     | > log_mle: 0.77700  (0.81402)\n     | > loss_dur: 2.33368  (2.35387)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.29405  (5.54470)\n     | > current_lr: 0.00000 \n     | > step_time: 1.12030  (0.83803)\n     | > loader_time: 0.01590  (0.00818)\n\n\n\u001b[1m   --> STEP: 357/406 -- GLOBAL_STEP: 1575\u001b[0m\n     | > loss: 3.10864  (3.16512)\n     | > log_mle: 0.77981  (0.81140)\n     | > loss_dur: 2.32883  (2.35372)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.30717  (5.52684)\n     | > current_lr: 0.00000 \n     | > step_time: 1.26220  (0.86415)\n     | > loader_time: 0.00850  (0.00842)\n\n\n\u001b[1m   --> STEP: 382/406 -- GLOBAL_STEP: 1600\u001b[0m\n     | > loss: 3.07849  (3.15972)\n     | > log_mle: 0.77179  (0.80882)\n     | > loss_dur: 2.30670  (2.35090)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.22534  (5.50743)\n     | > current_lr: 0.00000 \n     | > step_time: 1.23130  (0.89051)\n     | > loader_time: 0.00770  (0.00873)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00578 \u001b[0m(-0.00119)\n     | > avg_loss:\u001b[92m 3.04344 \u001b[0m(-0.13782)\n     | > avg_log_mle:\u001b[92m 0.76400 \u001b[0m(-0.07505)\n     | > avg_loss_dur:\u001b[92m 2.27944 \u001b[0m(-0.06277)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_1624.pth\n\n\u001b[4m\u001b[1m > EPOCH: 4/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 14:37:28) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 1/406 -- GLOBAL_STEP: 1625\u001b[0m\n     | > loss: 3.08736  (3.08736)\n     | > log_mle: 0.76759  (0.76759)\n     | > loss_dur: 2.31976  (2.31976)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.20024  (5.20024)\n     | > current_lr: 0.00000 \n     | > step_time: 0.49990  (0.49989)\n     | > loader_time: 0.00300  (0.00295)\n\n\n\u001b[1m   --> STEP: 26/406 -- GLOBAL_STEP: 1650\u001b[0m\n     | > loss: 3.02764  (3.06453)\n     | > log_mle: 0.76970  (0.77160)\n     | > loss_dur: 2.25795  (2.29293)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.15927  (5.19136)\n     | > current_lr: 0.00000 \n     | > step_time: 0.57780  (0.54802)\n     | > loader_time: 0.00350  (0.00440)\n\n\n\u001b[1m   --> STEP: 51/406 -- GLOBAL_STEP: 1675\u001b[0m\n     | > loss: 3.02389  (3.05236)\n     | > log_mle: 0.75537  (0.76908)\n     | > loss_dur: 2.26852  (2.28327)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.07495  (5.17775)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69250  (0.58297)\n     | > loader_time: 0.00520  (0.00497)\n\n\n\u001b[1m   --> STEP: 76/406 -- GLOBAL_STEP: 1700\u001b[0m\n     | > loss: 3.05793  (3.04570)\n     | > log_mle: 0.76198  (0.76544)\n     | > loss_dur: 2.29595  (2.28026)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.21123  (5.17192)\n     | > current_lr: 0.00000 \n     | > step_time: 0.67800  (0.61073)\n     | > loader_time: 0.00430  (0.00524)\n\n\n\u001b[1m   --> STEP: 101/406 -- GLOBAL_STEP: 1725\u001b[0m\n     | > loss: 3.03072  (3.03356)\n     | > log_mle: 0.74330  (0.76152)\n     | > loss_dur: 2.28742  (2.27203)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.18857  (5.15902)\n     | > current_lr: 0.00000 \n     | > step_time: 0.65740  (0.63305)\n     | > loader_time: 0.00430  (0.00561)\n\n\n\u001b[1m   --> STEP: 126/406 -- GLOBAL_STEP: 1750\u001b[0m\n     | > loss: 2.93020  (3.02232)\n     | > log_mle: 0.73832  (0.75751)\n     | > loss_dur: 2.19188  (2.26480)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.04755  (5.14845)\n     | > current_lr: 0.00000 \n     | > step_time: 0.73310  (0.64928)\n     | > loader_time: 0.01640  (0.00580)\n\n\n\u001b[1m   --> STEP: 151/406 -- GLOBAL_STEP: 1775\u001b[0m\n     | > loss: 2.94943  (3.01632)\n     | > log_mle: 0.73430  (0.75353)\n     | > loss_dur: 2.21513  (2.26279)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.04352  (5.14252)\n     | > current_lr: 0.00000 \n     | > step_time: 0.81140  (0.66912)\n     | > loader_time: 0.00790  (0.00597)\n\n\n\u001b[1m   --> STEP: 176/406 -- GLOBAL_STEP: 1800\u001b[0m\n     | > loss: 2.94491  (3.00687)\n     | > log_mle: 0.72371  (0.74963)\n     | > loss_dur: 2.22120  (2.25724)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.04993  (5.12970)\n     | > current_lr: 0.00000 \n     | > step_time: 0.83560  (0.68483)\n     | > loader_time: 0.00750  (0.00602)\n\n\n\u001b[1m   --> STEP: 201/406 -- GLOBAL_STEP: 1825\u001b[0m\n     | > loss: 2.90396  (2.99739)\n     | > log_mle: 0.72366  (0.74584)\n     | > loss_dur: 2.18030  (2.25155)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.01258  (5.11600)\n     | > current_lr: 0.00000 \n     | > step_time: 0.86450  (0.70435)\n     | > loader_time: 0.00530  (0.00635)\n\n\n\u001b[1m   --> STEP: 226/406 -- GLOBAL_STEP: 1850\u001b[0m\n     | > loss: 2.89810  (2.98762)\n     | > log_mle: 0.70524  (0.74209)\n     | > loss_dur: 2.19286  (2.24553)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.97737  (5.10381)\n     | > current_lr: 0.00000 \n     | > step_time: 0.83870  (0.72757)\n     | > loader_time: 0.00890  (0.00642)\n\n\n\u001b[1m   --> STEP: 251/406 -- GLOBAL_STEP: 1875\u001b[0m\n     | > loss: 2.93018  (2.97922)\n     | > log_mle: 0.70092  (0.73846)\n     | > loss_dur: 2.22926  (2.24075)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.01355  (5.09204)\n     | > current_lr: 0.00000 \n     | > step_time: 0.88590  (0.74887)\n     | > loader_time: 0.00780  (0.00653)\n\n\n\u001b[1m   --> STEP: 276/406 -- GLOBAL_STEP: 1900\u001b[0m\n     | > loss: 2.82881  (2.97013)\n     | > log_mle: 0.69515  (0.73467)\n     | > loss_dur: 2.13366  (2.23546)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.87675  (5.07974)\n     | > current_lr: 0.00000 \n     | > step_time: 0.96840  (0.77099)\n     | > loader_time: 0.00580  (0.00667)\n\n\n\u001b[1m   --> STEP: 301/406 -- GLOBAL_STEP: 1925\u001b[0m\n     | > loss: 2.87884  (2.96128)\n     | > log_mle: 0.68206  (0.73098)\n     | > loss_dur: 2.19678  (2.23031)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.94757  (5.06834)\n     | > current_lr: 0.00000 \n     | > step_time: 1.03310  (0.79358)\n     | > loader_time: 0.00850  (0.00680)\n\n\n\u001b[1m   --> STEP: 326/406 -- GLOBAL_STEP: 1950\u001b[0m\n     | > loss: 2.81113  (2.95177)\n     | > log_mle: 0.67779  (0.72728)\n     | > loss_dur: 2.13335  (2.22449)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.88491  (5.05732)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07100  (0.81622)\n     | > loader_time: 0.00830  (0.00688)\n\n\n\u001b[1m   --> STEP: 351/406 -- GLOBAL_STEP: 1975\u001b[0m\n     | > loss: 2.80862  (2.94427)\n     | > log_mle: 0.67248  (0.72350)\n     | > loss_dur: 2.13614  (2.22076)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.87995  (5.04833)\n     | > current_lr: 0.00000 \n     | > step_time: 1.17530  (0.83933)\n     | > loader_time: 0.00770  (0.00699)\n\n\n\u001b[1m   --> STEP: 376/406 -- GLOBAL_STEP: 2000\u001b[0m\n     | > loss: 2.74741  (2.93380)\n     | > log_mle: 0.66726  (0.71975)\n     | > loss_dur: 2.08014  (2.21405)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.77131  (5.03550)\n     | > current_lr: 0.00000 \n     | > step_time: 1.16760  (0.86259)\n     | > loader_time: 0.00880  (0.00710)\n\n\n\u001b[1m   --> STEP: 401/406 -- GLOBAL_STEP: 2025\u001b[0m\n     | > loss: 2.74701  (2.92370)\n     | > log_mle: 0.65960  (0.71609)\n     | > loss_dur: 2.08741  (2.20761)\n     | > amp_scaler: 16384.00000  (16424.85786)\n     | > grad_norm: 4.78684  (5.01123)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90310  (0.88272)\n     | > loader_time: 0.00630  (0.00719)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00560 \u001b[0m(-0.00018)\n     | > avg_loss:\u001b[92m 2.69338 \u001b[0m(-0.35005)\n     | > avg_log_mle:\u001b[92m 0.65556 \u001b[0m(-0.10843)\n     | > avg_loss_dur:\u001b[92m 2.03782 \u001b[0m(-0.24162)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_2030.pth\n\n\u001b[4m\u001b[1m > EPOCH: 5/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 14:43:51) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 20/406 -- GLOBAL_STEP: 2050\u001b[0m\n     | > loss: 2.63864  (2.75653)\n     | > log_mle: 0.67840  (0.67057)\n     | > loss_dur: 1.96025  (2.08596)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.60955  (4.78666)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66800  (0.56198)\n     | > loader_time: 0.00350  (0.00467)\n\n\n\u001b[1m   --> STEP: 45/406 -- GLOBAL_STEP: 2075\u001b[0m\n     | > loss: 2.72012  (2.72200)\n     | > log_mle: 0.66328  (0.66685)\n     | > loss_dur: 2.05684  (2.05515)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.80090  (4.75533)\n     | > current_lr: 0.00000 \n     | > step_time: 0.67340  (0.57292)\n     | > loader_time: 0.00380  (0.00489)\n\n\n\u001b[1m   --> STEP: 70/406 -- GLOBAL_STEP: 2100\u001b[0m\n     | > loss: 2.64966  (2.71073)\n     | > log_mle: 0.64063  (0.66086)\n     | > loss_dur: 2.00903  (2.04987)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.68104  (4.74752)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59430  (0.59438)\n     | > loader_time: 0.00460  (0.00553)\n\n\n\u001b[1m   --> STEP: 95/406 -- GLOBAL_STEP: 2125\u001b[0m\n     | > loss: 2.57987  (2.69449)\n     | > log_mle: 0.63742  (0.65502)\n     | > loss_dur: 1.94245  (2.03947)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.62830  (4.73419)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66460  (0.61464)\n     | > loader_time: 0.00710  (0.00608)\n\n\n\u001b[1m   --> STEP: 120/406 -- GLOBAL_STEP: 2150\u001b[0m\n     | > loss: 2.60297  (2.67738)\n     | > log_mle: 0.61986  (0.64967)\n     | > loss_dur: 1.98311  (2.02771)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.63984  (4.71665)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72530  (0.63877)\n     | > loader_time: 0.00580  (0.00650)\n\n\n\u001b[1m   --> STEP: 145/406 -- GLOBAL_STEP: 2175\u001b[0m\n     | > loss: 2.53888  (2.66538)\n     | > log_mle: 0.61577  (0.64424)\n     | > loss_dur: 1.92311  (2.02114)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.60079  (4.70686)\n     | > current_lr: 0.00000 \n     | > step_time: 0.75300  (0.65511)\n     | > loader_time: 0.00490  (0.00655)\n\n\n\u001b[1m   --> STEP: 170/406 -- GLOBAL_STEP: 2200\u001b[0m\n     | > loss: 2.54763  (2.65239)\n     | > log_mle: 0.60404  (0.63935)\n     | > loss_dur: 1.94359  (2.01304)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.56112  (4.69225)\n     | > current_lr: 0.00000 \n     | > step_time: 0.75970  (0.67874)\n     | > loader_time: 0.00510  (0.00655)\n\n\n\u001b[1m   --> STEP: 195/406 -- GLOBAL_STEP: 2225\u001b[0m\n     | > loss: 2.54165  (2.63923)\n     | > log_mle: 0.60933  (0.63453)\n     | > loss_dur: 1.93232  (2.00470)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.57827  (4.67652)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87700  (0.70258)\n     | > loader_time: 0.00550  (0.00658)\n\n\n\u001b[1m   --> STEP: 220/406 -- GLOBAL_STEP: 2250\u001b[0m\n     | > loss: 2.53258  (2.62438)\n     | > log_mle: 0.58161  (0.62996)\n     | > loss_dur: 1.95097  (1.99442)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.55048  (4.65796)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87110  (0.72304)\n     | > loader_time: 0.00630  (0.00668)\n\n\n\u001b[1m   --> STEP: 245/406 -- GLOBAL_STEP: 2275\u001b[0m\n     | > loss: 2.50170  (2.61252)\n     | > log_mle: 0.58993  (0.62552)\n     | > loss_dur: 1.91176  (1.98700)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.53440  (4.64415)\n     | > current_lr: 0.00000 \n     | > step_time: 1.00620  (0.74700)\n     | > loader_time: 0.00600  (0.00671)\n\n\n\u001b[1m   --> STEP: 270/406 -- GLOBAL_STEP: 2300\u001b[0m\n     | > loss: 2.46149  (2.60011)\n     | > log_mle: 0.57685  (0.62118)\n     | > loss_dur: 1.88464  (1.97893)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.39474  (4.62920)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90960  (0.77144)\n     | > loader_time: 0.00820  (0.00681)\n\n\n\u001b[1m   --> STEP: 295/406 -- GLOBAL_STEP: 2325\u001b[0m\n     | > loss: 2.44737  (2.58810)\n     | > log_mle: 0.57095  (0.61696)\n     | > loss_dur: 1.87641  (1.97114)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.45205  (4.61427)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06120  (0.79406)\n     | > loader_time: 0.00590  (0.00696)\n\n\n\u001b[1m   --> STEP: 320/406 -- GLOBAL_STEP: 2350\u001b[0m\n     | > loss: 2.41127  (2.57575)\n     | > log_mle: 0.56808  (0.61291)\n     | > loss_dur: 1.84320  (1.96284)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.40956  (4.59905)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07890  (0.81778)\n     | > loader_time: 0.01180  (0.00714)\n\n\n\u001b[1m   --> STEP: 345/406 -- GLOBAL_STEP: 2375\u001b[0m\n     | > loss: 2.41355  (2.56474)\n     | > log_mle: 0.54688  (0.60876)\n     | > loss_dur: 1.86667  (1.95597)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.41530  (4.58552)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14940  (0.84219)\n     | > loader_time: 0.01240  (0.00739)\n\n\n\u001b[1m   --> STEP: 370/406 -- GLOBAL_STEP: 2400\u001b[0m\n     | > loss: 2.33975  (2.55238)\n     | > log_mle: 0.54110  (0.60477)\n     | > loss_dur: 1.79866  (1.94761)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.26143  (4.56937)\n     | > current_lr: 0.00000 \n     | > step_time: 1.26420  (0.86990)\n     | > loader_time: 0.00690  (0.00783)\n\n\n\u001b[1m   --> STEP: 395/406 -- GLOBAL_STEP: 2425\u001b[0m\n     | > loss: 2.35480  (2.53992)\n     | > log_mle: 0.53677  (0.60097)\n     | > loss_dur: 1.81804  (1.93895)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.34142  (4.55247)\n     | > current_lr: 0.00000 \n     | > step_time: 1.28280  (0.89731)\n     | > loader_time: 0.00690  (0.00811)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00568 \u001b[0m(+0.00008)\n     | > avg_loss:\u001b[92m 2.26223 \u001b[0m(-0.43115)\n     | > avg_log_mle:\u001b[92m 0.53908 \u001b[0m(-0.11648)\n     | > avg_loss_dur:\u001b[92m 1.72315 \u001b[0m(-0.31467)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_2436.pth\n\n\u001b[4m\u001b[1m > EPOCH: 6/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 14:50:21) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 14/406 -- GLOBAL_STEP: 2450\u001b[0m\n     | > loss: 2.37491  (2.36616)\n     | > log_mle: 0.54550  (0.55969)\n     | > loss_dur: 1.82941  (1.80647)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.41494  (4.29135)\n     | > current_lr: 0.00000 \n     | > step_time: 0.52180  (0.53344)\n     | > loader_time: 0.00580  (0.00378)\n\n\n\u001b[1m   --> STEP: 39/406 -- GLOBAL_STEP: 2475\u001b[0m\n     | > loss: 2.32024  (2.32686)\n     | > log_mle: 0.55542  (0.55711)\n     | > loss_dur: 1.76481  (1.76975)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.27664  (4.25455)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60040  (0.55178)\n     | > loader_time: 0.00480  (0.00465)\n\n\n\u001b[1m   --> STEP: 64/406 -- GLOBAL_STEP: 2500\u001b[0m\n     | > loss: 2.16076  (2.30876)\n     | > log_mle: 0.55087  (0.55238)\n     | > loss_dur: 1.60990  (1.75638)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.00415  (4.23364)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59410  (0.58906)\n     | > loader_time: 0.00410  (0.00501)\n\n\n\u001b[1m   --> STEP: 89/406 -- GLOBAL_STEP: 2525\u001b[0m\n     | > loss: 2.27115  (2.29378)\n     | > log_mle: 0.53315  (0.54734)\n     | > loss_dur: 1.73800  (1.74644)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.14312  (4.20939)\n     | > current_lr: 0.00000 \n     | > step_time: 0.73450  (0.61996)\n     | > loader_time: 0.00670  (0.00547)\n\n\n\u001b[1m   --> STEP: 114/406 -- GLOBAL_STEP: 2550\u001b[0m\n     | > loss: 2.20375  (2.27568)\n     | > log_mle: 0.51488  (0.54248)\n     | > loss_dur: 1.68887  (1.73320)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.00419  (4.18248)\n     | > current_lr: 0.00000 \n     | > step_time: 0.76040  (0.65382)\n     | > loader_time: 0.00730  (0.00593)\n\n\n\u001b[1m   --> STEP: 139/406 -- GLOBAL_STEP: 2575\u001b[0m\n     | > loss: 2.20961  (2.26246)\n     | > log_mle: 0.51344  (0.53781)\n     | > loss_dur: 1.69617  (1.72465)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.02812  (4.16026)\n     | > current_lr: 0.00000 \n     | > step_time: 0.74650  (0.69043)\n     | > loader_time: 0.00660  (0.00623)\n\n\n\u001b[1m   --> STEP: 164/406 -- GLOBAL_STEP: 2600\u001b[0m\n     | > loss: 2.16960  (2.25054)\n     | > log_mle: 0.51606  (0.53373)\n     | > loss_dur: 1.65353  (1.71681)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.96005  (4.14067)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92310  (0.71127)\n     | > loader_time: 0.00790  (0.00628)\n\n\n\u001b[1m   --> STEP: 189/406 -- GLOBAL_STEP: 2625\u001b[0m\n     | > loss: 2.19834  (2.23921)\n     | > log_mle: 0.49779  (0.52985)\n     | > loss_dur: 1.70055  (1.70936)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.03723  (4.11908)\n     | > current_lr: 0.00000 \n     | > step_time: 0.82430  (0.73767)\n     | > loader_time: 0.00940  (0.00665)\n\n\n\u001b[1m   --> STEP: 214/406 -- GLOBAL_STEP: 2650\u001b[0m\n     | > loss: 2.10698  (2.22670)\n     | > log_mle: 0.49844  (0.52631)\n     | > loss_dur: 1.60855  (1.70038)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.89851  (4.09682)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90710  (0.76321)\n     | > loader_time: 0.00580  (0.00675)\n\n\n\u001b[1m   --> STEP: 239/406 -- GLOBAL_STEP: 2675\u001b[0m\n     | > loss: 2.15639  (2.21593)\n     | > log_mle: 0.48073  (0.52287)\n     | > loss_dur: 1.67566  (1.69306)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.97937  (4.07694)\n     | > current_lr: 0.00000 \n     | > step_time: 1.08880  (0.78943)\n     | > loader_time: 0.00940  (0.00690)\n\n\n\u001b[1m   --> STEP: 264/406 -- GLOBAL_STEP: 2700\u001b[0m\n     | > loss: 2.12314  (2.20553)\n     | > log_mle: 0.48700  (0.51960)\n     | > loss_dur: 1.63613  (1.68593)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.88984  (4.05790)\n     | > current_lr: 0.00000 \n     | > step_time: 1.20530  (0.81540)\n     | > loader_time: 0.00780  (0.00697)\n\n\n\u001b[1m   --> STEP: 289/406 -- GLOBAL_STEP: 2725\u001b[0m\n     | > loss: 2.02548  (2.19521)\n     | > log_mle: 0.48008  (0.51638)\n     | > loss_dur: 1.54540  (1.67883)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.73263  (4.03842)\n     | > current_lr: 0.00000 \n     | > step_time: 1.08350  (0.83990)\n     | > loader_time: 0.00830  (0.00717)\n\n\n\u001b[1m   --> STEP: 314/406 -- GLOBAL_STEP: 2750\u001b[0m\n     | > loss: 2.02678  (2.18554)\n     | > log_mle: 0.47141  (0.51345)\n     | > loss_dur: 1.55537  (1.67209)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.73063  (4.01981)\n     | > current_lr: 0.00000 \n     | > step_time: 1.08770  (0.86452)\n     | > loader_time: 0.00940  (0.00737)\n\n\n\u001b[1m   --> STEP: 339/406 -- GLOBAL_STEP: 2775\u001b[0m\n     | > loss: 2.04460  (2.17630)\n     | > log_mle: 0.47172  (0.51048)\n     | > loss_dur: 1.57288  (1.66581)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.74254  (4.00256)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14560  (0.88768)\n     | > loader_time: 0.00960  (0.00748)\n\n\n\u001b[1m   --> STEP: 364/406 -- GLOBAL_STEP: 2800\u001b[0m\n     | > loss: 2.03511  (2.16729)\n     | > log_mle: 0.47334  (0.50757)\n     | > loss_dur: 1.56177  (1.65972)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.71926  (3.98561)\n     | > current_lr: 0.00000 \n     | > step_time: 1.34250  (0.91374)\n     | > loader_time: 0.01170  (0.00766)\n\n\n\u001b[1m   --> STEP: 389/406 -- GLOBAL_STEP: 2825\u001b[0m\n     | > loss: 2.00218  (2.15725)\n     | > log_mle: 0.45724  (0.50487)\n     | > loss_dur: 1.54494  (1.65238)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.67228  (3.96634)\n     | > current_lr: 0.00000 \n     | > step_time: 1.34440  (0.94038)\n     | > loader_time: 0.00790  (0.00774)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00778 \u001b[0m(+0.00210)\n     | > avg_loss:\u001b[92m 1.92221 \u001b[0m(-0.34002)\n     | > avg_log_mle:\u001b[92m 0.46215 \u001b[0m(-0.07693)\n     | > avg_loss_dur:\u001b[92m 1.46006 \u001b[0m(-0.26309)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_2842.pth\n\n\u001b[4m\u001b[1m > EPOCH: 7/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n\n\u001b[1m > TRAINING (2023-02-11 14:57:13) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 8/406 -- GLOBAL_STEP: 2850\u001b[0m\n     | > loss: 2.04188  (2.04047)\n     | > log_mle: 0.48303  (0.48914)\n     | > loss_dur: 1.55885  (1.55134)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.74505  (3.70671)\n     | > current_lr: 0.00000 \n     | > step_time: 0.50300  (0.52712)\n     | > loader_time: 0.00350  (0.00556)\n\n\n\u001b[1m   --> STEP: 33/406 -- GLOBAL_STEP: 2875\u001b[0m\n     | > loss: 2.02763  (1.99961)\n     | > log_mle: 0.48123  (0.48397)\n     | > loss_dur: 1.54640  (1.51564)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.71368  (3.66570)\n     | > current_lr: 0.00000 \n     | > step_time: 0.55330  (0.56624)\n     | > loader_time: 0.00380  (0.00495)\n\n\n\u001b[1m   --> STEP: 58/406 -- GLOBAL_STEP: 2900\u001b[0m\n     | > loss: 2.01391  (1.98703)\n     | > log_mle: 0.46989  (0.48061)\n     | > loss_dur: 1.54401  (1.50642)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.65842  (3.64810)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59730  (0.60019)\n     | > loader_time: 0.00410  (0.00514)\n\n\n\u001b[1m   --> STEP: 83/406 -- GLOBAL_STEP: 2925\u001b[0m\n     | > loss: 1.96440  (1.97597)\n     | > log_mle: 0.45924  (0.47656)\n     | > loss_dur: 1.50516  (1.49942)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.59929  (3.62856)\n     | > current_lr: 0.00000 \n     | > step_time: 0.71940  (0.62366)\n     | > loader_time: 0.00460  (0.00544)\n\n\n\u001b[1m   --> STEP: 108/406 -- GLOBAL_STEP: 2950\u001b[0m\n     | > loss: 1.87871  (1.96061)\n     | > log_mle: 0.45478  (0.47269)\n     | > loss_dur: 1.42393  (1.48792)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.48248  (3.60200)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68840  (0.65464)\n     | > loader_time: 0.00770  (0.00578)\n\n\n\u001b[1m   --> STEP: 133/406 -- GLOBAL_STEP: 2975\u001b[0m\n     | > loss: 1.89345  (1.95021)\n     | > log_mle: 0.44807  (0.46874)\n     | > loss_dur: 1.44537  (1.48146)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.46657  (3.58262)\n     | > current_lr: 0.00000 \n     | > step_time: 0.94710  (0.67756)\n     | > loader_time: 0.01260  (0.00582)\n\n\n\u001b[1m   --> STEP: 158/406 -- GLOBAL_STEP: 3000\u001b[0m\n     | > loss: 1.89922  (1.94313)\n     | > log_mle: 0.44054  (0.46557)\n     | > loss_dur: 1.45869  (1.47756)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.47328  (3.56854)\n     | > current_lr: 0.00000 \n     | > step_time: 0.79670  (0.69469)\n     | > loader_time: 0.00500  (0.00582)\n\n\n\u001b[1m   --> STEP: 183/406 -- GLOBAL_STEP: 3025\u001b[0m\n     | > loss: 1.86396  (1.93550)\n     | > log_mle: 0.44102  (0.46263)\n     | > loss_dur: 1.42294  (1.47286)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.39333  (3.55261)\n     | > current_lr: 0.00000 \n     | > step_time: 0.85680  (0.71065)\n     | > loader_time: 0.00880  (0.00600)\n\n\n\u001b[1m   --> STEP: 208/406 -- GLOBAL_STEP: 3050\u001b[0m\n     | > loss: 1.83221  (1.92815)\n     | > log_mle: 0.42775  (0.45987)\n     | > loss_dur: 1.40446  (1.46827)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.39927  (3.53681)\n     | > current_lr: 0.00000 \n     | > step_time: 1.20160  (0.73187)\n     | > loader_time: 0.01760  (0.00614)\n\n\n\u001b[1m   --> STEP: 233/406 -- GLOBAL_STEP: 3075\u001b[0m\n     | > loss: 1.88673  (1.92072)\n     | > log_mle: 0.43530  (0.45723)\n     | > loss_dur: 1.45143  (1.46348)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.39641  (3.52200)\n     | > current_lr: 0.00000 \n     | > step_time: 0.98320  (0.75497)\n     | > loader_time: 0.00570  (0.00631)\n\n\n\u001b[1m   --> STEP: 258/406 -- GLOBAL_STEP: 3100\u001b[0m\n     | > loss: 1.88399  (1.91465)\n     | > log_mle: 0.43133  (0.45475)\n     | > loss_dur: 1.45266  (1.45990)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.43078  (3.50986)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11820  (0.77838)\n     | > loader_time: 0.00750  (0.00640)\n\n\n\u001b[1m   --> STEP: 283/406 -- GLOBAL_STEP: 3125\u001b[0m\n     | > loss: 1.82351  (1.90782)\n     | > log_mle: 0.42228  (0.45231)\n     | > loss_dur: 1.40123  (1.45551)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.32332  (3.49578)\n     | > current_lr: 0.00000 \n     | > step_time: 1.00220  (0.80183)\n     | > loader_time: 0.00820  (0.00649)\n\n\n\u001b[1m   --> STEP: 308/406 -- GLOBAL_STEP: 3150\u001b[0m\n     | > loss: 1.81723  (1.90194)\n     | > log_mle: 0.42164  (0.45015)\n     | > loss_dur: 1.39559  (1.45178)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.35000  (3.48349)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06100  (0.82549)\n     | > loader_time: 0.00720  (0.00660)\n\n\n\u001b[1m   --> STEP: 333/406 -- GLOBAL_STEP: 3175\u001b[0m\n     | > loss: 1.86308  (1.89572)\n     | > log_mle: 0.41570  (0.44801)\n     | > loss_dur: 1.44738  (1.44771)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.38230  (3.47122)\n     | > current_lr: 0.00000 \n     | > step_time: 1.03820  (0.84934)\n     | > loader_time: 0.00640  (0.00673)\n\n\n\u001b[1m   --> STEP: 358/406 -- GLOBAL_STEP: 3200\u001b[0m\n     | > loss: 1.74981  (1.89058)\n     | > log_mle: 0.39960  (0.44579)\n     | > loss_dur: 1.35021  (1.44479)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.24597  (3.46066)\n     | > current_lr: 0.00000 \n     | > step_time: 1.18520  (0.87267)\n     | > loader_time: 0.01050  (0.00685)\n\n\n\u001b[1m   --> STEP: 383/406 -- GLOBAL_STEP: 3225\u001b[0m\n     | > loss: 1.81204  (1.88415)\n     | > log_mle: 0.41577  (0.44381)\n     | > loss_dur: 1.39627  (1.44034)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.29564  (3.44721)\n     | > current_lr: 0.00000 \n     | > step_time: 1.24410  (0.89657)\n     | > loader_time: 0.00870  (0.00704)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00911 \u001b[0m(+0.00133)\n     | > avg_loss:\u001b[92m 1.71566 \u001b[0m(-0.20655)\n     | > avg_log_mle:\u001b[92m 0.41417 \u001b[0m(-0.04798)\n     | > avg_loss_dur:\u001b[92m 1.30149 \u001b[0m(-0.15857)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_3248.pth\n\n\u001b[4m\u001b[1m > EPOCH: 8/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:03:50) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 2/406 -- GLOBAL_STEP: 3250\u001b[0m\n     | > loss: 1.86305  (1.84949)\n     | > log_mle: 0.44558  (0.43828)\n     | > loss_dur: 1.41747  (1.41120)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.37224  (3.33963)\n     | > current_lr: 0.00000 \n     | > step_time: 0.45350  (0.44944)\n     | > loader_time: 0.00290  (0.00331)\n\n\n\u001b[1m   --> STEP: 27/406 -- GLOBAL_STEP: 3275\u001b[0m\n     | > loss: 1.73460  (1.79136)\n     | > log_mle: 0.44145  (0.43877)\n     | > loss_dur: 1.29315  (1.35259)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.16783  (3.25857)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66130  (0.56440)\n     | > loader_time: 0.00530  (0.00441)\n\n\n\u001b[1m   --> STEP: 52/406 -- GLOBAL_STEP: 3300\u001b[0m\n     | > loss: 1.74122  (1.77800)\n     | > log_mle: 0.42598  (0.43624)\n     | > loss_dur: 1.31524  (1.34176)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.13158  (3.23722)\n     | > current_lr: 0.00000 \n     | > step_time: 0.61840  (0.58723)\n     | > loader_time: 0.00450  (0.00516)\n\n\n\u001b[1m   --> STEP: 77/406 -- GLOBAL_STEP: 3325\u001b[0m\n     | > loss: 1.76418  (1.76908)\n     | > log_mle: 0.43012  (0.43244)\n     | > loss_dur: 1.33406  (1.33664)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.18162  (3.22316)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68020  (0.61366)\n     | > loader_time: 0.01450  (0.00588)\n\n\n\u001b[1m   --> STEP: 102/406 -- GLOBAL_STEP: 3350\u001b[0m\n     | > loss: 1.74873  (1.75842)\n     | > log_mle: 0.40236  (0.42868)\n     | > loss_dur: 1.34636  (1.32974)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.20969  (3.20101)\n     | > current_lr: 0.00000 \n     | > step_time: 0.65300  (0.63140)\n     | > loader_time: 0.00460  (0.00590)\n\n\n\u001b[1m   --> STEP: 127/406 -- GLOBAL_STEP: 3375\u001b[0m\n     | > loss: 1.71422  (1.74815)\n     | > log_mle: 0.40487  (0.42513)\n     | > loss_dur: 1.30935  (1.32303)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.09907  (3.18014)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78150  (0.65614)\n     | > loader_time: 0.00510  (0.00601)\n\n\n\u001b[1m   --> STEP: 152/406 -- GLOBAL_STEP: 3400\u001b[0m\n     | > loss: 1.70240  (1.74316)\n     | > log_mle: 0.40298  (0.42203)\n     | > loss_dur: 1.29942  (1.32113)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.10194  (3.16931)\n     | > current_lr: 0.00000 \n     | > step_time: 0.82000  (0.67468)\n     | > loader_time: 0.01670  (0.00632)\n\n\n\u001b[1m   --> STEP: 177/406 -- GLOBAL_STEP: 3425\u001b[0m\n     | > loss: 1.70368  (1.73751)\n     | > log_mle: 0.39407  (0.41938)\n     | > loss_dur: 1.30962  (1.31813)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.08362  (3.15644)\n     | > current_lr: 0.00000 \n     | > step_time: 0.95000  (0.70021)\n     | > loader_time: 0.01040  (0.00647)\n\n\n\u001b[1m   --> STEP: 202/406 -- GLOBAL_STEP: 3450\u001b[0m\n     | > loss: 1.68679  (1.73161)\n     | > log_mle: 0.39909  (0.41688)\n     | > loss_dur: 1.28770  (1.31473)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.04917  (3.14313)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92080  (0.72491)\n     | > loader_time: 0.00580  (0.00647)\n\n\n\u001b[1m   --> STEP: 227/406 -- GLOBAL_STEP: 3475\u001b[0m\n     | > loss: 1.67039  (1.72558)\n     | > log_mle: 0.40613  (0.41452)\n     | > loss_dur: 1.26426  (1.31106)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.99143  (3.13032)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01830  (0.75069)\n     | > loader_time: 0.00570  (0.00659)\n\n\n\u001b[1m   --> STEP: 252/406 -- GLOBAL_STEP: 3500\u001b[0m\n     | > loss: 1.70125  (1.72110)\n     | > log_mle: 0.38789  (0.41228)\n     | > loss_dur: 1.31336  (1.30882)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.07548  (3.12160)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92810  (0.77294)\n     | > loader_time: 0.00620  (0.00665)\n\n\n\u001b[1m   --> STEP: 277/406 -- GLOBAL_STEP: 3525\u001b[0m\n     | > loss: 1.65569  (1.71608)\n     | > log_mle: 0.38955  (0.41008)\n     | > loss_dur: 1.26614  (1.30600)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.97982  (3.11047)\n     | > current_lr: 0.00000 \n     | > step_time: 1.08420  (0.79796)\n     | > loader_time: 0.00710  (0.00675)\n\n\n\u001b[1m   --> STEP: 302/406 -- GLOBAL_STEP: 3550\u001b[0m\n     | > loss: 1.65459  (1.71140)\n     | > log_mle: 0.38302  (0.40812)\n     | > loss_dur: 1.27157  (1.30327)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.97961  (3.09949)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02400  (0.82015)\n     | > loader_time: 0.00770  (0.00693)\n\n\n\u001b[1m   --> STEP: 327/406 -- GLOBAL_STEP: 3575\u001b[0m\n     | > loss: 1.64589  (1.70619)\n     | > log_mle: 0.38313  (0.40626)\n     | > loss_dur: 1.26276  (1.29993)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.94997  (3.08931)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07960  (0.84537)\n     | > loader_time: 0.01350  (0.00708)\n\n\n\u001b[1m   --> STEP: 352/406 -- GLOBAL_STEP: 3600\u001b[0m\n     | > loss: 1.63953  (1.70272)\n     | > log_mle: 0.38002  (0.40431)\n     | > loss_dur: 1.25952  (1.29841)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.91082  (3.08073)\n     | > current_lr: 0.00000 \n     | > step_time: 1.21860  (0.87123)\n     | > loader_time: 0.01400  (0.00742)\n\n\n\u001b[1m   --> STEP: 377/406 -- GLOBAL_STEP: 3625\u001b[0m\n     | > loss: 1.64923  (1.69762)\n     | > log_mle: 0.37658  (0.40241)\n     | > loss_dur: 1.27265  (1.29521)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.94937  (3.06958)\n     | > current_lr: 0.00000 \n     | > step_time: 1.32800  (0.90039)\n     | > loader_time: 0.01590  (0.00788)\n\n\n\u001b[1m   --> STEP: 402/406 -- GLOBAL_STEP: 3650\u001b[0m\n     | > loss: 1.61110  (1.69256)\n     | > log_mle: 0.36849  (0.40065)\n     | > loss_dur: 1.24261  (1.29191)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.88874  (3.05796)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90620  (0.92151)\n     | > loader_time: 0.00620  (0.00802)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00631 \u001b[0m(-0.00280)\n     | > avg_loss:\u001b[92m 1.55799 \u001b[0m(-0.15766)\n     | > avg_log_mle:\u001b[92m 0.37725 \u001b[0m(-0.03691)\n     | > avg_loss_dur:\u001b[92m 1.18074 \u001b[0m(-0.12075)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_3654.pth\n\n\u001b[4m\u001b[1m > EPOCH: 9/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:10:30) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 21/406 -- GLOBAL_STEP: 3675\u001b[0m\n     | > loss: 1.55829  (1.63193)\n     | > log_mle: 0.39899  (0.40283)\n     | > loss_dur: 1.15930  (1.22910)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.75055  (2.92088)\n     | > current_lr: 0.00000 \n     | > step_time: 0.53360  (0.54617)\n     | > loader_time: 0.00360  (0.00514)\n\n\n\u001b[1m   --> STEP: 46/406 -- GLOBAL_STEP: 3700\u001b[0m\n     | > loss: 1.59898  (1.61305)\n     | > log_mle: 0.39568  (0.40128)\n     | > loss_dur: 1.20330  (1.21176)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.88666  (2.88637)\n     | > current_lr: 0.00000 \n     | > step_time: 0.61700  (0.57889)\n     | > loader_time: 0.00380  (0.00516)\n\n\n\u001b[1m   --> STEP: 71/406 -- GLOBAL_STEP: 3725\u001b[0m\n     | > loss: 1.58964  (1.60668)\n     | > log_mle: 0.38860  (0.39679)\n     | > loss_dur: 1.20104  (1.20988)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.85417  (2.87655)\n     | > current_lr: 0.00000 \n     | > step_time: 0.71510  (0.60338)\n     | > loader_time: 0.00610  (0.00525)\n\n\n\u001b[1m   --> STEP: 96/406 -- GLOBAL_STEP: 3750\u001b[0m\n     | > loss: 1.58742  (1.59717)\n     | > log_mle: 0.38015  (0.39271)\n     | > loss_dur: 1.20727  (1.20446)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.82954  (2.85606)\n     | > current_lr: 0.00000 \n     | > step_time: 0.63810  (0.62914)\n     | > loader_time: 0.00510  (0.00556)\n\n\n\u001b[1m   --> STEP: 121/406 -- GLOBAL_STEP: 3775\u001b[0m\n     | > loss: 1.57936  (1.58876)\n     | > log_mle: 0.37721  (0.38928)\n     | > loss_dur: 1.20215  (1.19948)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.80564  (2.83739)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72220  (0.64687)\n     | > loader_time: 0.00460  (0.00610)\n\n\n\u001b[1m   --> STEP: 146/406 -- GLOBAL_STEP: 3800\u001b[0m\n     | > loss: 1.54932  (1.58346)\n     | > log_mle: 0.37770  (0.38600)\n     | > loss_dur: 1.17162  (1.19746)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.76346  (2.82911)\n     | > current_lr: 0.00000 \n     | > step_time: 0.94890  (0.66688)\n     | > loader_time: 0.00760  (0.00627)\n\n\n\u001b[1m   --> STEP: 171/406 -- GLOBAL_STEP: 3825\u001b[0m\n     | > loss: 1.51463  (1.57856)\n     | > log_mle: 0.36525  (0.38336)\n     | > loss_dur: 1.14938  (1.19520)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.69660  (2.81966)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06650  (0.68527)\n     | > loader_time: 0.01780  (0.00660)\n\n\n\u001b[1m   --> STEP: 196/406 -- GLOBAL_STEP: 3850\u001b[0m\n     | > loss: 1.52354  (1.57406)\n     | > log_mle: 0.35815  (0.38081)\n     | > loss_dur: 1.16540  (1.19325)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.72310  (2.80840)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87080  (0.70843)\n     | > loader_time: 0.00780  (0.00669)\n\n\n\u001b[1m   --> STEP: 221/406 -- GLOBAL_STEP: 3875\u001b[0m\n     | > loss: 1.54431  (1.56888)\n     | > log_mle: 0.36584  (0.37850)\n     | > loss_dur: 1.17848  (1.19039)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.69276  (2.79658)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11910  (0.73532)\n     | > loader_time: 0.01980  (0.00684)\n\n\n\u001b[1m   --> STEP: 246/406 -- GLOBAL_STEP: 3900\u001b[0m\n     | > loss: 1.50130  (1.56463)\n     | > log_mle: 0.35451  (0.37621)\n     | > loss_dur: 1.14679  (1.18841)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.68440  (2.78702)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92870  (0.76110)\n     | > loader_time: 0.00580  (0.00695)\n\n\n\u001b[1m   --> STEP: 271/406 -- GLOBAL_STEP: 3925\u001b[0m\n     | > loss: 1.51952  (1.56054)\n     | > log_mle: 0.34012  (0.37408)\n     | > loss_dur: 1.17940  (1.18646)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.71365  (2.77896)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01770  (0.78629)\n     | > loader_time: 0.00660  (0.00704)\n\n\n\u001b[1m   --> STEP: 296/406 -- GLOBAL_STEP: 3950\u001b[0m\n     | > loss: 1.53879  (1.55660)\n     | > log_mle: 0.35244  (0.37209)\n     | > loss_dur: 1.18634  (1.18452)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.69317  (2.77139)\n     | > current_lr: 0.00000 \n     | > step_time: 1.04410  (0.80983)\n     | > loader_time: 0.01000  (0.00721)\n\n\n\u001b[1m   --> STEP: 321/406 -- GLOBAL_STEP: 3975\u001b[0m\n     | > loss: 1.49450  (1.55235)\n     | > log_mle: 0.34432  (0.37023)\n     | > loss_dur: 1.15018  (1.18212)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.61643  (2.76330)\n     | > current_lr: 0.00000 \n     | > step_time: 1.15450  (0.83527)\n     | > loader_time: 0.00860  (0.00743)\n\n\n\u001b[1m   --> STEP: 346/406 -- GLOBAL_STEP: 4000\u001b[0m\n     | > loss: 1.48917  (1.54921)\n     | > log_mle: 0.34796  (0.36825)\n     | > loss_dur: 1.14122  (1.18096)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.62243  (2.75574)\n     | > current_lr: 0.00000 \n     | > step_time: 1.19550  (0.86427)\n     | > loader_time: 0.01250  (0.00781)\n\n\n\u001b[1m   --> STEP: 371/406 -- GLOBAL_STEP: 4025\u001b[0m\n     | > loss: 1.48304  (1.54507)\n     | > log_mle: 0.33843  (0.36633)\n     | > loss_dur: 1.14462  (1.17874)\n     | > amp_scaler: 32768.00000  (17002.26415)\n     | > grad_norm: 2.61250  (2.74710)\n     | > current_lr: 0.00000 \n     | > step_time: 1.32080  (0.89106)\n     | > loader_time: 0.01340  (0.00825)\n\n\n\u001b[1m   --> STEP: 396/406 -- GLOBAL_STEP: 4050\u001b[0m\n     | > loss: 1.48568  (1.54085)\n     | > log_mle: 0.32695  (0.36456)\n     | > loss_dur: 1.15873  (1.17629)\n     | > amp_scaler: 32768.00000  (17997.57576)\n     | > grad_norm: 2.62525  (2.73804)\n     | > current_lr: 0.00000 \n     | > step_time: 1.30800  (0.91892)\n     | > loader_time: 0.00810  (0.00853)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00596 \u001b[0m(-0.00035)\n     | > avg_loss:\u001b[92m 1.43505 \u001b[0m(-0.12294)\n     | > avg_log_mle:\u001b[92m 0.34253 \u001b[0m(-0.03472)\n     | > avg_loss_dur:\u001b[92m 1.09252 \u001b[0m(-0.08822)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_4060.pth\n\n\u001b[4m\u001b[1m > EPOCH: 10/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:17:10) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 15/406 -- GLOBAL_STEP: 4075\u001b[0m\n     | > loss: 1.48033  (1.50417)\n     | > log_mle: 0.35689  (0.36691)\n     | > loss_dur: 1.12344  (1.13726)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.64380  (2.64179)\n     | > current_lr: 0.00000 \n     | > step_time: 0.54410  (0.56474)\n     | > loader_time: 0.00330  (0.00589)\n\n\n\u001b[1m   --> STEP: 40/406 -- GLOBAL_STEP: 4100\u001b[0m\n     | > loss: 1.43747  (1.48016)\n     | > log_mle: 0.37455  (0.36659)\n     | > loss_dur: 1.06292  (1.11357)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.52129  (2.60314)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60080  (0.57393)\n     | > loader_time: 0.00640  (0.00552)\n\n\n\u001b[1m   --> STEP: 65/406 -- GLOBAL_STEP: 4125\u001b[0m\n     | > loss: 1.48025  (1.47026)\n     | > log_mle: 0.35588  (0.36274)\n     | > loss_dur: 1.12437  (1.10752)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.61771  (2.58645)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68840  (0.60226)\n     | > loader_time: 0.00410  (0.00536)\n\n\n\u001b[1m   --> STEP: 90/406 -- GLOBAL_STEP: 4150\u001b[0m\n     | > loss: 1.43738  (1.46331)\n     | > log_mle: 0.34555  (0.35857)\n     | > loss_dur: 1.09184  (1.10473)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.51537  (2.58119)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70440  (0.62012)\n     | > loader_time: 0.00440  (0.00538)\n\n\n\u001b[1m   --> STEP: 115/406 -- GLOBAL_STEP: 4175\u001b[0m\n     | > loss: 1.43916  (1.45480)\n     | > log_mle: 0.34988  (0.35459)\n     | > loss_dur: 1.08929  (1.10020)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.49407  (2.57501)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69700  (0.64281)\n     | > loader_time: 0.00470  (0.00567)\n\n\n\u001b[1m   --> STEP: 140/406 -- GLOBAL_STEP: 4200\u001b[0m\n     | > loss: 1.41258  (1.44988)\n     | > log_mle: 0.32811  (0.35101)\n     | > loss_dur: 1.08448  (1.09888)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.50599  (2.56938)\n     | > current_lr: 0.00000 \n     | > step_time: 0.79100  (0.66102)\n     | > loader_time: 0.00660  (0.00573)\n\n\n\u001b[1m   --> STEP: 165/406 -- GLOBAL_STEP: 4225\u001b[0m\n     | > loss: 1.40666  (1.44565)\n     | > log_mle: 0.32326  (0.34813)\n     | > loss_dur: 1.08340  (1.09752)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.48194  (2.55998)\n     | > current_lr: 0.00000 \n     | > step_time: 0.76330  (0.68493)\n     | > loader_time: 0.00550  (0.00596)\n\n\n\u001b[1m   --> STEP: 190/406 -- GLOBAL_STEP: 4250\u001b[0m\n     | > loss: 1.42588  (1.44191)\n     | > log_mle: 0.32114  (0.34540)\n     | > loss_dur: 1.10473  (1.09651)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.51550  (2.55095)\n     | > current_lr: 0.00000 \n     | > step_time: 0.80670  (0.70679)\n     | > loader_time: 0.00530  (0.00623)\n\n\n\u001b[1m   --> STEP: 215/406 -- GLOBAL_STEP: 4275\u001b[0m\n     | > loss: 1.39369  (1.43707)\n     | > log_mle: 0.32226  (0.34307)\n     | > loss_dur: 1.07143  (1.09399)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.50402  (2.54155)\n     | > current_lr: 0.00000 \n     | > step_time: 0.89510  (0.72708)\n     | > loader_time: 0.00780  (0.00638)\n\n\n\u001b[1m   --> STEP: 240/406 -- GLOBAL_STEP: 4300\u001b[0m\n     | > loss: 1.37674  (1.43331)\n     | > log_mle: 0.31346  (0.34070)\n     | > loss_dur: 1.06328  (1.09262)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.46868  (2.53276)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01360  (0.75076)\n     | > loader_time: 0.00570  (0.00656)\n\n\n\u001b[1m   --> STEP: 265/406 -- GLOBAL_STEP: 4325\u001b[0m\n     | > loss: 1.39666  (1.42969)\n     | > log_mle: 0.31169  (0.33851)\n     | > loss_dur: 1.08497  (1.09118)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.49602  (2.52855)\n     | > current_lr: 0.00000 \n     | > step_time: 0.98090  (0.77533)\n     | > loader_time: 0.00710  (0.00684)\n\n\n\u001b[1m   --> STEP: 290/406 -- GLOBAL_STEP: 4350\u001b[0m\n     | > loss: 1.37445  (1.42623)\n     | > log_mle: 0.30297  (0.33637)\n     | > loss_dur: 1.07148  (1.08986)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.39865  (2.52978)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14420  (0.80142)\n     | > loader_time: 0.01210  (0.00730)\n\n\n\u001b[1m   --> STEP: 315/406 -- GLOBAL_STEP: 4375\u001b[0m\n     | > loss: 1.35636  (1.42306)\n     | > log_mle: 0.29522  (0.33447)\n     | > loss_dur: 1.06114  (1.08859)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.42652  (2.52801)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11730  (0.82773)\n     | > loader_time: 0.00680  (0.00770)\n\n\n\u001b[1m   --> STEP: 340/406 -- GLOBAL_STEP: 4400\u001b[0m\n     | > loss: 1.41008  (1.42031)\n     | > log_mle: 0.30917  (0.33257)\n     | > loss_dur: 1.10091  (1.08773)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.47095  (2.52156)\n     | > current_lr: 0.00000 \n     | > step_time: 1.15010  (0.85630)\n     | > loader_time: 0.01800  (0.00818)\n\n\n\u001b[1m   --> STEP: 365/406 -- GLOBAL_STEP: 4425\u001b[0m\n     | > loss: 1.31617  (1.41704)\n     | > log_mle: 0.30606  (0.33057)\n     | > loss_dur: 1.01011  (1.08647)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.25529  (2.51376)\n     | > current_lr: 0.00000 \n     | > step_time: 1.38960  (0.88664)\n     | > loader_time: 0.01390  (0.00873)\n\n\n\u001b[1m   --> STEP: 390/406 -- GLOBAL_STEP: 4450\u001b[0m\n     | > loss: 1.35602  (1.41337)\n     | > log_mle: 0.29525  (0.32877)\n     | > loss_dur: 1.06077  (1.08459)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.35071  (2.50421)\n     | > current_lr: 0.00000 \n     | > step_time: 1.25700  (0.91882)\n     | > loader_time: 0.01030  (0.00913)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00630 \u001b[0m(+0.00034)\n     | > avg_loss:\u001b[92m 1.32824 \u001b[0m(-0.10681)\n     | > avg_log_mle:\u001b[92m 0.30797 \u001b[0m(-0.03456)\n     | > avg_loss_dur:\u001b[92m 1.02027 \u001b[0m(-0.07225)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_4466.pth\n\n\u001b[4m\u001b[1m > EPOCH: 11/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:23:55) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 9/406 -- GLOBAL_STEP: 4475\u001b[0m\n     | > loss: 1.38537  (1.40330)\n     | > log_mle: 0.33846  (0.33703)\n     | > loss_dur: 1.04691  (1.06627)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.37731  (2.43775)\n     | > current_lr: 0.00000 \n     | > step_time: 0.57080  (0.54813)\n     | > loader_time: 0.00600  (0.00372)\n\n\n\u001b[1m   --> STEP: 34/406 -- GLOBAL_STEP: 4500\u001b[0m\n     | > loss: 1.36029  (1.36735)\n     | > log_mle: 0.34221  (0.33191)\n     | > loss_dur: 1.01809  (1.03544)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.30351  (2.41300)\n     | > current_lr: 0.00000 \n     | > step_time: 0.57360  (0.61955)\n     | > loader_time: 0.00670  (0.00523)\n\n\n\u001b[1m   --> STEP: 59/406 -- GLOBAL_STEP: 4525\u001b[0m\n     | > loss: 1.32758  (1.35610)\n     | > log_mle: 0.30831  (0.32807)\n     | > loss_dur: 1.01927  (1.02804)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.36902  (2.38981)\n     | > current_lr: 0.00000 \n     | > step_time: 0.64750  (0.63409)\n     | > loader_time: 0.00400  (0.00550)\n\n\n\u001b[1m   --> STEP: 84/406 -- GLOBAL_STEP: 4550\u001b[0m\n     | > loss: 1.32258  (1.34982)\n     | > log_mle: 0.31493  (0.32417)\n     | > loss_dur: 1.00765  (1.02565)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.36197  (2.38903)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69950  (0.66393)\n     | > loader_time: 0.00460  (0.00602)\n\n\n\u001b[1m   --> STEP: 109/406 -- GLOBAL_STEP: 4575\u001b[0m\n     | > loss: 1.30278  (1.34088)\n     | > log_mle: 0.29567  (0.32004)\n     | > loss_dur: 1.00711  (1.02085)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.31833  (2.37392)\n     | > current_lr: 0.00000 \n     | > step_time: 0.77280  (0.68791)\n     | > loader_time: 0.00500  (0.00635)\n\n\n\u001b[1m   --> STEP: 134/406 -- GLOBAL_STEP: 4600\u001b[0m\n     | > loss: 1.33403  (1.33570)\n     | > log_mle: 0.29753  (0.31613)\n     | > loss_dur: 1.03650  (1.01957)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.42526  (2.36812)\n     | > current_lr: 0.00000 \n     | > step_time: 0.87670  (0.71321)\n     | > loader_time: 0.00510  (0.00665)\n\n\n\u001b[1m   --> STEP: 159/406 -- GLOBAL_STEP: 4625\u001b[0m\n     | > loss: 1.30690  (1.33225)\n     | > log_mle: 0.29550  (0.31308)\n     | > loss_dur: 1.01140  (1.01917)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.26274  (2.36071)\n     | > current_lr: 0.00000 \n     | > step_time: 0.80770  (0.73770)\n     | > loader_time: 0.00620  (0.00703)\n\n\n\u001b[1m   --> STEP: 184/406 -- GLOBAL_STEP: 4650\u001b[0m\n     | > loss: 1.34584  (1.32854)\n     | > log_mle: 0.27721  (0.31022)\n     | > loss_dur: 1.06863  (1.01832)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.68176  (2.35862)\n     | > current_lr: 0.00000 \n     | > step_time: 0.91260  (0.75169)\n     | > loader_time: 0.00570  (0.00701)\n\n\n\u001b[1m   --> STEP: 209/406 -- GLOBAL_STEP: 4675\u001b[0m\n     | > loss: 1.31712  (1.32478)\n     | > log_mle: 0.27895  (0.30764)\n     | > loss_dur: 1.03817  (1.01714)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.36474  (2.36901)\n     | > current_lr: 0.00000 \n     | > step_time: 0.88610  (0.77398)\n     | > loader_time: 0.00510  (0.00701)\n\n\n\u001b[1m   --> STEP: 234/406 -- GLOBAL_STEP: 4700\u001b[0m\n     | > loss: 1.30090  (1.32072)\n     | > log_mle: 0.27648  (0.30512)\n     | > loss_dur: 1.02441  (1.01560)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.46835  (2.36420)\n     | > current_lr: 0.00000 \n     | > step_time: 0.88480  (0.79365)\n     | > loader_time: 0.01010  (0.00710)\n\n\n\u001b[1m   --> STEP: 259/406 -- GLOBAL_STEP: 4725\u001b[0m\n     | > loss: 1.28470  (1.31794)\n     | > log_mle: 0.26855  (0.30277)\n     | > loss_dur: 1.01614  (1.01517)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.47144  (2.36179)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93750  (0.81415)\n     | > loader_time: 0.00660  (0.00718)\n\n\n\u001b[1m   --> STEP: 284/406 -- GLOBAL_STEP: 4750\u001b[0m\n     | > loss: 1.29306  (1.31482)\n     | > log_mle: 0.27186  (0.30051)\n     | > loss_dur: 1.02120  (1.01431)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.66759  (2.36269)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06370  (0.83610)\n     | > loader_time: 0.00890  (0.00725)\n\n\n\u001b[1m   --> STEP: 309/406 -- GLOBAL_STEP: 4775\u001b[0m\n     | > loss: 1.25565  (1.31205)\n     | > log_mle: 0.27622  (0.29854)\n     | > loss_dur: 0.97943  (1.01350)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.20289  (2.35860)\n     | > current_lr: 0.00000 \n     | > step_time: 1.37780  (0.85791)\n     | > loader_time: 0.00680  (0.00724)\n\n\n\u001b[1m   --> STEP: 334/406 -- GLOBAL_STEP: 4800\u001b[0m\n     | > loss: 1.30661  (1.30919)\n     | > log_mle: 0.26698  (0.29658)\n     | > loss_dur: 1.03963  (1.01261)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.26523  (2.35645)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14250  (0.88133)\n     | > loader_time: 0.00820  (0.00747)\n\n\n\u001b[1m   --> STEP: 359/406 -- GLOBAL_STEP: 4825\u001b[0m\n     | > loss: 1.24294  (1.30662)\n     | > log_mle: 0.27225  (0.29453)\n     | > loss_dur: 0.97069  (1.01209)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.16033  (2.34982)\n     | > current_lr: 0.00000 \n     | > step_time: 1.22090  (0.90908)\n     | > loader_time: 0.01610  (0.00798)\n\n\n\u001b[1m   --> STEP: 384/406 -- GLOBAL_STEP: 4850\u001b[0m\n     | > loss: 1.23141  (1.30327)\n     | > log_mle: 0.27290  (0.29267)\n     | > loss_dur: 0.95851  (1.01061)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.08714  (2.34115)\n     | > current_lr: 0.00000 \n     | > step_time: 1.53650  (0.93780)\n     | > loader_time: 0.03000  (0.00848)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00733 \u001b[0m(+0.00103)\n     | > avg_loss:\u001b[92m 1.23344 \u001b[0m(-0.09480)\n     | > avg_log_mle:\u001b[92m 0.27262 \u001b[0m(-0.03534)\n     | > avg_loss_dur:\u001b[92m 0.96081 \u001b[0m(-0.05946)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_4872.pth\n\n\u001b[4m\u001b[1m > EPOCH: 12/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:30:51) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 3/406 -- GLOBAL_STEP: 4875\u001b[0m\n     | > loss: 1.33553  (1.32094)\n     | > log_mle: 0.29697  (0.29564)\n     | > loss_dur: 1.03856  (1.02530)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.27310  (2.34030)\n     | > current_lr: 0.00000 \n     | > step_time: 0.56010  (0.51626)\n     | > loader_time: 0.00220  (0.00292)\n\n\n\u001b[1m   --> STEP: 28/406 -- GLOBAL_STEP: 4900\u001b[0m\n     | > loss: 1.23628  (1.26264)\n     | > log_mle: 0.29184  (0.29563)\n     | > loss_dur: 0.94445  (0.96701)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.06928  (2.30148)\n     | > current_lr: 0.00000 \n     | > step_time: 0.57770  (0.57782)\n     | > loader_time: 0.00390  (0.00521)\n\n\n\u001b[1m   --> STEP: 53/406 -- GLOBAL_STEP: 4925\u001b[0m\n     | > loss: 1.21891  (1.25096)\n     | > log_mle: 0.28995  (0.29273)\n     | > loss_dur: 0.92896  (0.95823)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.20070  (2.27436)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60320  (0.60311)\n     | > loader_time: 0.01160  (0.00577)\n\n\n\u001b[1m   --> STEP: 78/406 -- GLOBAL_STEP: 4950\u001b[0m\n     | > loss: 1.20607  (1.24392)\n     | > log_mle: 0.28198  (0.28842)\n     | > loss_dur: 0.92409  (0.95549)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.35134  (2.31537)\n     | > current_lr: 0.00000 \n     | > step_time: 0.67590  (0.63447)\n     | > loader_time: 0.00700  (0.00604)\n\n\n\u001b[1m   --> STEP: 103/406 -- GLOBAL_STEP: 4975\u001b[0m\n     | > loss: 1.19960  (1.23638)\n     | > log_mle: 0.24455  (0.28381)\n     | > loss_dur: 0.95505  (0.95257)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.36665  (2.30085)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70100  (0.65512)\n     | > loader_time: 0.00500  (0.00629)\n\n\n\u001b[1m   --> STEP: 128/406 -- GLOBAL_STEP: 5000\u001b[0m\n     | > loss: 1.24199  (1.23011)\n     | > log_mle: 0.25591  (0.28001)\n     | > loss_dur: 0.98608  (0.95010)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.32341  (2.26925)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69990  (0.67857)\n     | > loader_time: 0.00520  (0.00638)\n\n\n\u001b[1m   --> STEP: 153/406 -- GLOBAL_STEP: 5025\u001b[0m\n     | > loss: 1.21294  (1.22685)\n     | > log_mle: 0.26288  (0.27676)\n     | > loss_dur: 0.95006  (0.95009)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.11892  (2.25943)\n     | > current_lr: 0.00000 \n     | > step_time: 0.97840  (0.70010)\n     | > loader_time: 0.01030  (0.00645)\n\n\n\u001b[1m   --> STEP: 178/406 -- GLOBAL_STEP: 5050\u001b[0m\n     | > loss: 1.19105  (1.22320)\n     | > log_mle: 0.26649  (0.27389)\n     | > loss_dur: 0.92456  (0.94930)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.11422  (2.24067)\n     | > current_lr: 0.00000 \n     | > step_time: 1.03090  (0.72051)\n     | > loader_time: 0.00540  (0.00641)\n\n\n\u001b[1m   --> STEP: 203/406 -- GLOBAL_STEP: 5075\u001b[0m\n     | > loss: 1.20805  (1.21998)\n     | > log_mle: 0.25994  (0.27120)\n     | > loss_dur: 0.94811  (0.94878)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.08054  (2.23593)\n     | > current_lr: 0.00000 \n     | > step_time: 0.83750  (0.74533)\n     | > loader_time: 0.00700  (0.00661)\n\n\n\u001b[1m   --> STEP: 228/406 -- GLOBAL_STEP: 5100\u001b[0m\n     | > loss: 1.21197  (1.21608)\n     | > log_mle: 0.23918  (0.26854)\n     | > loss_dur: 0.97279  (0.94755)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.98973  (2.24180)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02160  (0.76983)\n     | > loader_time: 0.00580  (0.00670)\n\n\n\u001b[1m   --> STEP: 253/406 -- GLOBAL_STEP: 5125\u001b[0m\n     | > loss: 1.19964  (1.21333)\n     | > log_mle: 0.23036  (0.26607)\n     | > loss_dur: 0.96928  (0.94726)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.88086  (2.26577)\n     | > current_lr: 0.00000 \n     | > step_time: 1.04310  (0.79422)\n     | > loader_time: 0.00590  (0.00680)\n\n\n\u001b[1m   --> STEP: 278/406 -- GLOBAL_STEP: 5150\u001b[0m\n     | > loss: 1.15560  (1.21015)\n     | > log_mle: 0.25420  (0.26382)\n     | > loss_dur: 0.90140  (0.94632)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.12743  (2.27217)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06150  (0.81681)\n     | > loader_time: 0.01610  (0.00692)\n\n\n\u001b[1m   --> STEP: 303/406 -- GLOBAL_STEP: 5175\u001b[0m\n     | > loss: 1.19273  (1.20784)\n     | > log_mle: 0.24353  (0.26174)\n     | > loss_dur: 0.94920  (0.94610)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.41159  (2.26888)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06880  (0.83844)\n     | > loader_time: 0.00760  (0.00708)\n\n\n\u001b[1m   --> STEP: 328/406 -- GLOBAL_STEP: 5200\u001b[0m\n     | > loss: 1.21149  (1.20477)\n     | > log_mle: 0.23430  (0.25980)\n     | > loss_dur: 0.97720  (0.94497)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.17501  (2.26356)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11680  (0.85966)\n     | > loader_time: 0.00920  (0.00723)\n\n\n\u001b[1m   --> STEP: 353/406 -- GLOBAL_STEP: 5225\u001b[0m\n     | > loss: 1.16067  (1.20287)\n     | > log_mle: 0.22101  (0.25773)\n     | > loss_dur: 0.93966  (0.94514)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.11958  (2.25659)\n     | > current_lr: 0.00000 \n     | > step_time: 1.42180  (0.88395)\n     | > loader_time: 0.00970  (0.00738)\n\n\n\u001b[1m   --> STEP: 378/406 -- GLOBAL_STEP: 5250\u001b[0m\n     | > loss: 1.16265  (1.19955)\n     | > log_mle: 0.23422  (0.25571)\n     | > loss_dur: 0.92843  (0.94384)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.12127  (2.24875)\n     | > current_lr: 0.00000 \n     | > step_time: 1.30550  (0.91131)\n     | > loader_time: 0.00970  (0.00754)\n\n\n\u001b[1m   --> STEP: 403/406 -- GLOBAL_STEP: 5275\u001b[0m\n     | > loss: 1.12531  (1.19633)\n     | > log_mle: 0.22509  (0.25376)\n     | > loss_dur: 0.90023  (0.94257)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.25111  (2.24797)\n     | > current_lr: 0.00000 \n     | > step_time: 0.90170  (0.93074)\n     | > loader_time: 0.00700  (0.00762)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00533 \u001b[0m(-0.00200)\n     | > avg_loss:\u001b[92m 1.14292 \u001b[0m(-0.09052)\n     | > avg_log_mle:\u001b[92m 0.23616 \u001b[0m(-0.03646)\n     | > avg_loss_dur:\u001b[92m 0.90676 \u001b[0m(-0.05406)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_5278.pth\n\n\u001b[4m\u001b[1m > EPOCH: 13/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:37:40) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 22/406 -- GLOBAL_STEP: 5300\u001b[0m\n     | > loss: 1.09689  (1.15821)\n     | > log_mle: 0.25699  (0.25809)\n     | > loss_dur: 0.83991  (0.90012)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.94097  (2.17873)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59830  (0.55616)\n     | > loader_time: 0.00390  (0.00469)\n\n\n\u001b[1m   --> STEP: 47/406 -- GLOBAL_STEP: 5325\u001b[0m\n     | > loss: 1.17550  (1.14713)\n     | > log_mle: 0.23910  (0.25599)\n     | > loss_dur: 0.93640  (0.89115)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.48945  (2.13508)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60980  (0.58729)\n     | > loader_time: 0.00390  (0.00538)\n\n\n\u001b[1m   --> STEP: 72/406 -- GLOBAL_STEP: 5350\u001b[0m\n     | > loss: 1.11444  (1.14099)\n     | > log_mle: 0.25115  (0.25141)\n     | > loss_dur: 0.86329  (0.88957)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.06955  (2.16552)\n     | > current_lr: 0.00000 \n     | > step_time: 0.60560  (0.60890)\n     | > loader_time: 0.00440  (0.00527)\n\n\n\u001b[1m   --> STEP: 97/406 -- GLOBAL_STEP: 5375\u001b[0m\n     | > loss: 1.10652  (1.13413)\n     | > log_mle: 0.23485  (0.24671)\n     | > loss_dur: 0.87166  (0.88742)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.29271  (2.17414)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69680  (0.63422)\n     | > loader_time: 0.00680  (0.00559)\n\n\n\u001b[1m   --> STEP: 122/406 -- GLOBAL_STEP: 5400\u001b[0m\n     | > loss: 1.08792  (1.12857)\n     | > log_mle: 0.23611  (0.24295)\n     | > loss_dur: 0.85180  (0.88562)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.82277  (2.14888)\n     | > current_lr: 0.00000 \n     | > step_time: 0.99320  (0.65212)\n     | > loader_time: 0.01380  (0.00571)\n\n\n\u001b[1m   --> STEP: 147/406 -- GLOBAL_STEP: 5425\u001b[0m\n     | > loss: 1.11445  (1.12517)\n     | > log_mle: 0.22282  (0.23936)\n     | > loss_dur: 0.89163  (0.88582)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.04391  (2.13792)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72760  (0.67243)\n     | > loader_time: 0.00560  (0.00598)\n\n\n\u001b[1m   --> STEP: 172/406 -- GLOBAL_STEP: 5450\u001b[0m\n     | > loss: 1.09667  (1.12179)\n     | > log_mle: 0.21993  (0.23642)\n     | > loss_dur: 0.87674  (0.88537)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.60231  (2.16977)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78120  (0.69710)\n     | > loader_time: 0.00870  (0.00639)\n\n\n\u001b[1m   --> STEP: 197/406 -- GLOBAL_STEP: 5475\u001b[0m\n     | > loss: 1.09348  (1.11904)\n     | > log_mle: 0.21800  (0.23365)\n     | > loss_dur: 0.87548  (0.88539)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.93696  (2.18896)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11640  (0.71920)\n     | > loader_time: 0.00530  (0.00655)\n\n\n\u001b[1m   --> STEP: 222/406 -- GLOBAL_STEP: 5500\u001b[0m\n     | > loss: 1.07502  (1.11536)\n     | > log_mle: 0.20426  (0.23108)\n     | > loss_dur: 0.87076  (0.88428)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.15600  (2.19542)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93610  (0.74167)\n     | > loader_time: 0.00550  (0.00659)\n\n\n\u001b[1m   --> STEP: 247/406 -- GLOBAL_STEP: 5525\u001b[0m\n     | > loss: 1.05571  (1.11252)\n     | > log_mle: 0.22566  (0.22871)\n     | > loss_dur: 0.83006  (0.88381)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.91818  (2.20209)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01900  (0.76767)\n     | > loader_time: 0.00590  (0.00678)\n\n\n\u001b[1m   --> STEP: 272/406 -- GLOBAL_STEP: 5550\u001b[0m\n     | > loss: 1.06628  (1.10991)\n     | > log_mle: 0.19620  (0.22630)\n     | > loss_dur: 0.87009  (0.88361)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.30166  (2.19787)\n     | > current_lr: 0.00000 \n     | > step_time: 1.09800  (0.79404)\n     | > loader_time: 0.00650  (0.00697)\n\n\n\u001b[1m   --> STEP: 297/406 -- GLOBAL_STEP: 5575\u001b[0m\n     | > loss: 1.07182  (1.10747)\n     | > log_mle: 0.20267  (0.22424)\n     | > loss_dur: 0.86915  (0.88324)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.19019  (2.18829)\n     | > current_lr: 0.00000 \n     | > step_time: 1.17830  (0.82041)\n     | > loader_time: 0.01200  (0.00734)\n\n\n\u001b[1m   --> STEP: 322/406 -- GLOBAL_STEP: 5600\u001b[0m\n     | > loss: 1.10186  (1.10468)\n     | > log_mle: 0.19290  (0.22224)\n     | > loss_dur: 0.90896  (0.88244)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.17017  (2.19166)\n     | > current_lr: 0.00000 \n     | > step_time: 1.16500  (0.84779)\n     | > loader_time: 0.01190  (0.00782)\n\n\n\u001b[1m   --> STEP: 347/406 -- GLOBAL_STEP: 5625\u001b[0m\n     | > loss: 1.07927  (1.10260)\n     | > log_mle: 0.18579  (0.22019)\n     | > loss_dur: 0.89348  (0.88240)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.52275  (2.20727)\n     | > current_lr: 0.00000 \n     | > step_time: 1.29510  (0.87536)\n     | > loader_time: 0.01610  (0.00828)\n\n\n\u001b[1m   --> STEP: 372/406 -- GLOBAL_STEP: 5650\u001b[0m\n     | > loss: 1.04805  (1.09972)\n     | > log_mle: 0.19847  (0.21822)\n     | > loss_dur: 0.84958  (0.88150)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.30890  (2.22409)\n     | > current_lr: 0.00000 \n     | > step_time: 1.39580  (0.90321)\n     | > loader_time: 0.01520  (0.00873)\n\n\n\u001b[1m   --> STEP: 397/406 -- GLOBAL_STEP: 5675\u001b[0m\n     | > loss: 1.05457  (1.09676)\n     | > log_mle: 0.19021  (0.21632)\n     | > loss_dur: 0.86436  (0.88044)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.94690  (2.21684)\n     | > current_lr: 0.00000 \n     | > step_time: 1.11990  (0.93115)\n     | > loader_time: 0.00950  (0.00900)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00744 \u001b[0m(+0.00212)\n     | > avg_loss:\u001b[92m 1.04966 \u001b[0m(-0.09326)\n     | > avg_log_mle:\u001b[92m 0.20226 \u001b[0m(-0.03391)\n     | > avg_loss_dur:\u001b[92m 0.84740 \u001b[0m(-0.05936)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_5684.pth\n\n\u001b[4m\u001b[1m > EPOCH: 14/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:44:28) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 16/406 -- GLOBAL_STEP: 5700\u001b[0m\n     | > loss: 1.07681  (1.07049)\n     | > log_mle: 0.21430  (0.21913)\n     | > loss_dur: 0.86251  (0.85136)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.25947  (2.06856)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66240  (0.54583)\n     | > loader_time: 0.00870  (0.00432)\n\n\n\u001b[1m   --> STEP: 41/406 -- GLOBAL_STEP: 5725\u001b[0m\n     | > loss: 1.03264  (1.05037)\n     | > log_mle: 0.22570  (0.21936)\n     | > loss_dur: 0.80693  (0.83101)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.75441  (2.01196)\n     | > current_lr: 0.00000 \n     | > step_time: 0.62300  (0.58375)\n     | > loader_time: 0.00410  (0.00554)\n\n\n\u001b[1m   --> STEP: 66/406 -- GLOBAL_STEP: 5750\u001b[0m\n     | > loss: 1.05494  (1.04404)\n     | > log_mle: 0.20417  (0.21517)\n     | > loss_dur: 0.85078  (0.82887)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.18438  (2.12645)\n     | > current_lr: 0.00000 \n     | > step_time: 0.73010  (0.59986)\n     | > loader_time: 0.00420  (0.00562)\n\n\n\u001b[1m   --> STEP: 91/406 -- GLOBAL_STEP: 5775\u001b[0m\n     | > loss: 1.00394  (1.03892)\n     | > log_mle: 0.18109  (0.21067)\n     | > loss_dur: 0.82285  (0.82825)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.06586  (2.10602)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69880  (0.62905)\n     | > loader_time: 0.01210  (0.00607)\n\n\n\u001b[1m   --> STEP: 116/406 -- GLOBAL_STEP: 5800\u001b[0m\n     | > loss: 1.01068  (1.03265)\n     | > log_mle: 0.19642  (0.20655)\n     | > loss_dur: 0.81426  (0.82610)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.80060  (2.08049)\n     | > current_lr: 0.00000 \n     | > step_time: 0.71440  (0.64770)\n     | > loader_time: 0.00850  (0.00615)\n\n\n\u001b[1m   --> STEP: 141/406 -- GLOBAL_STEP: 5825\u001b[0m\n     | > loss: 1.02473  (1.02926)\n     | > log_mle: 0.18284  (0.20276)\n     | > loss_dur: 0.84189  (0.82650)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.95756  (2.09523)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78570  (0.66826)\n     | > loader_time: 0.00470  (0.00625)\n\n\n\u001b[1m   --> STEP: 166/406 -- GLOBAL_STEP: 5850\u001b[0m\n     | > loss: 1.00092  (1.02583)\n     | > log_mle: 0.16947  (0.19969)\n     | > loss_dur: 0.83145  (0.82613)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.84673  (2.09949)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92460  (0.69075)\n     | > loader_time: 0.00650  (0.00635)\n\n\n\u001b[1m   --> STEP: 191/406 -- GLOBAL_STEP: 5875\u001b[0m\n     | > loss: 0.98024  (1.02311)\n     | > log_mle: 0.17475  (0.19685)\n     | > loss_dur: 0.80548  (0.82626)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.96219  (2.07821)\n     | > current_lr: 0.00000 \n     | > step_time: 0.84290  (0.71479)\n     | > loader_time: 0.00820  (0.00651)\n\n\n\u001b[1m   --> STEP: 216/406 -- GLOBAL_STEP: 5900\u001b[0m\n     | > loss: 0.99547  (1.01965)\n     | > log_mle: 0.16573  (0.19434)\n     | > loss_dur: 0.82974  (0.82531)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.72603  (2.06009)\n     | > current_lr: 0.00000 \n     | > step_time: 0.95500  (0.73858)\n     | > loader_time: 0.00550  (0.00659)\n\n\n\u001b[1m   --> STEP: 241/406 -- GLOBAL_STEP: 5925\u001b[0m\n     | > loss: 0.98091  (1.01673)\n     | > log_mle: 0.17966  (0.19190)\n     | > loss_dur: 0.80125  (0.82483)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.79820  (2.05019)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92590  (0.76399)\n     | > loader_time: 0.00870  (0.00673)\n\n\n\u001b[1m   --> STEP: 266/406 -- GLOBAL_STEP: 5950\u001b[0m\n     | > loss: 1.00202  (1.01406)\n     | > log_mle: 0.17455  (0.18956)\n     | > loss_dur: 0.82747  (0.82450)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.86781  (2.06175)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06560  (0.78415)\n     | > loader_time: 0.00860  (0.00693)\n\n\n\u001b[1m   --> STEP: 291/406 -- GLOBAL_STEP: 5975\u001b[0m\n     | > loss: 1.00130  (1.01126)\n     | > log_mle: 0.17808  (0.18734)\n     | > loss_dur: 0.82321  (0.82393)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.92856  (2.09063)\n     | > current_lr: 0.00000 \n     | > step_time: 1.01680  (0.81084)\n     | > loader_time: 0.01280  (0.00728)\n\n\n\u001b[1m   --> STEP: 316/406 -- GLOBAL_STEP: 6000\u001b[0m\n     | > loss: 0.97172  (1.00860)\n     | > log_mle: 0.16218  (0.18531)\n     | > loss_dur: 0.80954  (0.82329)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.73721  (2.08860)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14260  (0.84128)\n     | > loader_time: 0.01550  (0.00787)\n\n\n\u001b[1m   --> STEP: 341/406 -- GLOBAL_STEP: 6025\u001b[0m\n     | > loss: 0.96410  (1.00639)\n     | > log_mle: 0.14313  (0.18339)\n     | > loss_dur: 0.82097  (0.82300)\n     | > amp_scaler: 65536.00000  (34113.31378)\n     | > grad_norm: 1.86888  (2.07909)\n     | > current_lr: 0.00000 \n     | > step_time: 1.20890  (0.87002)\n     | > loader_time: 0.01410  (0.00838)\n\n\n\u001b[1m   --> STEP: 366/406 -- GLOBAL_STEP: 6050\u001b[0m\n     | > loss: 0.95862  (1.00380)\n     | > log_mle: 0.14968  (0.18138)\n     | > loss_dur: 0.80895  (0.82242)\n     | > amp_scaler: 65536.00000  (36259.67213)\n     | > grad_norm: 3.26988  (2.08683)\n     | > current_lr: 0.00000 \n     | > step_time: 1.18770  (0.89866)\n     | > loader_time: 0.00770  (0.00898)\n\n\n\u001b[1m   --> STEP: 391/406 -- GLOBAL_STEP: 6075\u001b[0m\n     | > loss: 0.94991  (1.00084)\n     | > log_mle: 0.14017  (0.17948)\n     | > loss_dur: 0.80974  (0.82136)\n     | > amp_scaler: 65536.00000  (38131.56010)\n     | > grad_norm: 1.99707  (2.08725)\n     | > current_lr: 0.00000 \n     | > step_time: 1.21870  (0.92778)\n     | > loader_time: 0.01010  (0.00943)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00654 \u001b[0m(-0.00090)\n     | > avg_loss:\u001b[92m 0.95327 \u001b[0m(-0.09639)\n     | > avg_log_mle:\u001b[92m 0.16611 \u001b[0m(-0.03615)\n     | > avg_loss_dur:\u001b[92m 0.78716 \u001b[0m(-0.06024)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_6090.pth\n\n\u001b[4m\u001b[1m > EPOCH: 15/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:51:16) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 10/406 -- GLOBAL_STEP: 6100\u001b[0m\n     | > loss: 0.95100  (0.98137)\n     | > log_mle: 0.18175  (0.18640)\n     | > loss_dur: 0.76925  (0.79497)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.85578  (1.85413)\n     | > current_lr: 0.00000 \n     | > step_time: 0.55970  (0.58554)\n     | > loader_time: 0.00590  (0.00383)\n\n\n\u001b[1m   --> STEP: 35/406 -- GLOBAL_STEP: 6125\u001b[0m\n     | > loss: 0.94069  (0.95460)\n     | > log_mle: 0.16592  (0.18172)\n     | > loss_dur: 0.77476  (0.77288)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.75092  (1.81505)\n     | > current_lr: 0.00000 \n     | > step_time: 0.56750  (0.57249)\n     | > loader_time: 0.00380  (0.00463)\n\n\n\u001b[1m   --> STEP: 60/406 -- GLOBAL_STEP: 6150\u001b[0m\n     | > loss: 0.90466  (0.94808)\n     | > log_mle: 0.16872  (0.17838)\n     | > loss_dur: 0.73594  (0.76969)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.64726  (1.85248)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59840  (0.60759)\n     | > loader_time: 0.00740  (0.00518)\n\n\n\u001b[1m   --> STEP: 85/406 -- GLOBAL_STEP: 6175\u001b[0m\n     | > loss: 0.89735  (0.94363)\n     | > log_mle: 0.14593  (0.17434)\n     | > loss_dur: 0.75142  (0.76929)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.75947  (1.83882)\n     | > current_lr: 0.00000 \n     | > step_time: 0.63440  (0.62360)\n     | > loader_time: 0.00580  (0.00529)\n\n\n\u001b[1m   --> STEP: 110/406 -- GLOBAL_STEP: 6200\u001b[0m\n     | > loss: 0.91683  (0.93696)\n     | > log_mle: 0.14399  (0.17028)\n     | > loss_dur: 0.77284  (0.76667)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.05611  (1.88634)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70750  (0.64903)\n     | > loader_time: 0.00470  (0.00580)\n\n\n\u001b[1m   --> STEP: 135/406 -- GLOBAL_STEP: 6225\u001b[0m\n     | > loss: 0.93146  (0.93335)\n     | > log_mle: 0.15159  (0.16658)\n     | > loss_dur: 0.77987  (0.76678)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.80508  (1.87770)\n     | > current_lr: 0.00000 \n     | > step_time: 0.97730  (0.66958)\n     | > loader_time: 0.02260  (0.00597)\n\n\n\u001b[1m   --> STEP: 160/406 -- GLOBAL_STEP: 6250\u001b[0m\n     | > loss: 0.89442  (0.93023)\n     | > log_mle: 0.14701  (0.16358)\n     | > loss_dur: 0.74741  (0.76665)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.65381  (1.85330)\n     | > current_lr: 0.00000 \n     | > step_time: 0.80780  (0.69151)\n     | > loader_time: 0.00550  (0.00629)\n\n\n\u001b[1m   --> STEP: 185/406 -- GLOBAL_STEP: 6275\u001b[0m\n     | > loss: 0.89222  (0.92740)\n     | > log_mle: 0.14575  (0.16075)\n     | > loss_dur: 0.74647  (0.76665)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.70570  (1.85686)\n     | > current_lr: 0.00000 \n     | > step_time: 0.80660  (0.71551)\n     | > loader_time: 0.00510  (0.00642)\n\n\n\u001b[1m   --> STEP: 210/406 -- GLOBAL_STEP: 6300\u001b[0m\n     | > loss: 0.87835  (0.92453)\n     | > log_mle: 0.14463  (0.15825)\n     | > loss_dur: 0.73372  (0.76628)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.72934  (1.85524)\n     | > current_lr: 0.00000 \n     | > step_time: 0.85560  (0.74002)\n     | > loader_time: 0.00580  (0.00645)\n\n\n\u001b[1m   --> STEP: 235/406 -- GLOBAL_STEP: 6325\u001b[0m\n     | > loss: 0.90901  (0.92139)\n     | > log_mle: 0.13365  (0.15567)\n     | > loss_dur: 0.77536  (0.76572)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.04386  (1.86017)\n     | > current_lr: 0.00000 \n     | > step_time: 1.08060  (0.76186)\n     | > loader_time: 0.00580  (0.00656)\n\n\n\u001b[1m   --> STEP: 260/406 -- GLOBAL_STEP: 6350\u001b[0m\n     | > loss: 0.86141  (0.91878)\n     | > log_mle: 0.12370  (0.15337)\n     | > loss_dur: 0.73772  (0.76541)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.28753  (1.91974)\n     | > current_lr: 0.00000 \n     | > step_time: 0.99130  (0.78633)\n     | > loader_time: 0.00680  (0.00664)\n\n\n\u001b[1m   --> STEP: 285/406 -- GLOBAL_STEP: 6375\u001b[0m\n     | > loss: 0.88297  (0.91626)\n     | > log_mle: 0.13117  (0.15125)\n     | > loss_dur: 0.75180  (0.76501)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.68919  (1.91827)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02710  (0.81178)\n     | > loader_time: 0.00870  (0.00677)\n\n\n\u001b[1m   --> STEP: 310/406 -- GLOBAL_STEP: 6400\u001b[0m\n     | > loss: 0.85309  (0.91382)\n     | > log_mle: 0.13369  (0.14940)\n     | > loss_dur: 0.71941  (0.76442)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.55547  (1.89975)\n     | > current_lr: 0.00000 \n     | > step_time: 1.12960  (0.83615)\n     | > loader_time: 0.00750  (0.00694)\n\n\n\u001b[1m   --> STEP: 335/406 -- GLOBAL_STEP: 6425\u001b[0m\n     | > loss: 0.89487  (0.91150)\n     | > log_mle: 0.11476  (0.14750)\n     | > loss_dur: 0.78011  (0.76400)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.92811  (1.89880)\n     | > current_lr: 0.00000 \n     | > step_time: 1.10610  (0.86044)\n     | > loader_time: 0.00970  (0.00707)\n\n\n\u001b[1m   --> STEP: 360/406 -- GLOBAL_STEP: 6450\u001b[0m\n     | > loss: 0.86930  (0.90921)\n     | > log_mle: 0.11631  (0.14559)\n     | > loss_dur: 0.75299  (0.76362)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.00151  (1.89761)\n     | > current_lr: 0.00000 \n     | > step_time: 1.18840  (0.88543)\n     | > loader_time: 0.00760  (0.00716)\n\n\n\u001b[1m   --> STEP: 385/406 -- GLOBAL_STEP: 6475\u001b[0m\n     | > loss: 0.86362  (0.90647)\n     | > log_mle: 0.12133  (0.14381)\n     | > loss_dur: 0.74229  (0.76266)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.60753  (1.89298)\n     | > current_lr: 0.00000 \n     | > step_time: 1.37810  (0.91310)\n     | > loader_time: 0.00960  (0.00729)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00756 \u001b[0m(+0.00102)\n     | > avg_loss:\u001b[92m 0.86200 \u001b[0m(-0.09127)\n     | > avg_log_mle:\u001b[92m 0.13545 \u001b[0m(-0.03065)\n     | > avg_loss_dur:\u001b[92m 0.72654 \u001b[0m(-0.06061)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_6496.pth\n\n\u001b[4m\u001b[1m > EPOCH: 16/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 15:58:00) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 4/406 -- GLOBAL_STEP: 6500\u001b[0m\n     | > loss: 0.93002  (0.90807)\n     | > log_mle: 0.17129  (0.15335)\n     | > loss_dur: 0.75873  (0.75472)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.74280  (1.82333)\n     | > current_lr: 0.00000 \n     | > step_time: 0.53890  (0.51990)\n     | > loader_time: 0.00330  (0.00549)\n\n\n\u001b[1m   --> STEP: 29/406 -- GLOBAL_STEP: 6525\u001b[0m\n     | > loss: 0.84827  (0.86282)\n     | > log_mle: 0.14830  (0.14836)\n     | > loss_dur: 0.69997  (0.71447)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.72358  (1.69720)\n     | > current_lr: 0.00000 \n     | > step_time: 0.58680  (0.57937)\n     | > loader_time: 0.00640  (0.00599)\n\n\n\u001b[1m   --> STEP: 54/406 -- GLOBAL_STEP: 6550\u001b[0m\n     | > loss: 0.82887  (0.85688)\n     | > log_mle: 0.13752  (0.14557)\n     | > loss_dur: 0.69134  (0.71131)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.75443  (1.73618)\n     | > current_lr: 0.00000 \n     | > step_time: 0.65670  (0.59620)\n     | > loader_time: 0.00550  (0.00561)\n\n\n\u001b[1m   --> STEP: 79/406 -- GLOBAL_STEP: 6575\u001b[0m\n     | > loss: 0.84382  (0.85215)\n     | > log_mle: 0.12419  (0.14137)\n     | > loss_dur: 0.71963  (0.71078)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.59318  (1.75187)\n     | > current_lr: 0.00000 \n     | > step_time: 0.63090  (0.62277)\n     | > loader_time: 0.00450  (0.00586)\n\n\n\u001b[1m   --> STEP: 104/406 -- GLOBAL_STEP: 6600\u001b[0m\n     | > loss: 0.82957  (0.84699)\n     | > log_mle: 0.13242  (0.13704)\n     | > loss_dur: 0.69715  (0.70995)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.55960  (1.76115)\n     | > current_lr: 0.00000 \n     | > step_time: 0.74780  (0.63797)\n     | > loader_time: 0.00470  (0.00598)\n\n\n\u001b[1m   --> STEP: 129/406 -- GLOBAL_STEP: 6625\u001b[0m\n     | > loss: 0.84506  (0.84262)\n     | > log_mle: 0.12306  (0.13343)\n     | > loss_dur: 0.72200  (0.70919)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.67158  (1.75187)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70830  (0.66144)\n     | > loader_time: 0.00490  (0.00604)\n\n\n\u001b[1m   --> STEP: 154/406 -- GLOBAL_STEP: 6650\u001b[0m\n     | > loss: 0.81243  (0.83993)\n     | > log_mle: 0.11881  (0.13045)\n     | > loss_dur: 0.69362  (0.70948)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.91381  (1.75855)\n     | > current_lr: 0.00000 \n     | > step_time: 0.71520  (0.68355)\n     | > loader_time: 0.00880  (0.00629)\n\n\n\u001b[1m   --> STEP: 179/406 -- GLOBAL_STEP: 6675\u001b[0m\n     | > loss: 0.82244  (0.83761)\n     | > log_mle: 0.10930  (0.12783)\n     | > loss_dur: 0.71314  (0.70979)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.55137  (1.78139)\n     | > current_lr: 0.00000 \n     | > step_time: 0.86190  (0.70479)\n     | > loader_time: 0.00960  (0.00649)\n\n\n\u001b[1m   --> STEP: 204/406 -- GLOBAL_STEP: 6700\u001b[0m\n     | > loss: 0.81076  (0.83493)\n     | > log_mle: 0.10971  (0.12543)\n     | > loss_dur: 0.70105  (0.70950)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.17259  (1.83742)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92120  (0.72844)\n     | > loader_time: 0.00530  (0.00670)\n\n\n\u001b[1m   --> STEP: 229/406 -- GLOBAL_STEP: 6725\u001b[0m\n     | > loss: 0.79567  (0.83192)\n     | > log_mle: 0.11038  (0.12302)\n     | > loss_dur: 0.68529  (0.70889)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.79934  (1.87453)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93190  (0.75299)\n     | > loader_time: 0.00600  (0.00674)\n\n\n\u001b[1m   --> STEP: 254/406 -- GLOBAL_STEP: 6750\u001b[0m\n     | > loss: 0.81158  (0.82968)\n     | > log_mle: 0.10943  (0.12081)\n     | > loss_dur: 0.70215  (0.70886)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.26611  (1.93875)\n     | > current_lr: 0.00000 \n     | > step_time: 0.91780  (0.77523)\n     | > loader_time: 0.00600  (0.00690)\n\n\n\u001b[1m   --> STEP: 279/406 -- GLOBAL_STEP: 6775\u001b[0m\n     | > loss: 0.81898  (0.82731)\n     | > log_mle: 0.10286  (0.11883)\n     | > loss_dur: 0.71612  (0.70848)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.24141  (1.99613)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93770  (0.79565)\n     | > loader_time: 0.00900  (0.00698)\n\n\n\u001b[1m   --> STEP: 304/406 -- GLOBAL_STEP: 6800\u001b[0m\n     | > loss: 0.77836  (0.82531)\n     | > log_mle: 0.09340  (0.11698)\n     | > loss_dur: 0.68496  (0.70833)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.82966  (2.00103)\n     | > current_lr: 0.00000 \n     | > step_time: 1.05560  (0.81742)\n     | > loader_time: 0.00620  (0.00715)\n\n\n\u001b[1m   --> STEP: 329/406 -- GLOBAL_STEP: 6825\u001b[0m\n     | > loss: 0.80711  (0.82298)\n     | > log_mle: 0.10488  (0.11533)\n     | > loss_dur: 0.70222  (0.70765)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.42790  (1.98379)\n     | > current_lr: 0.00000 \n     | > step_time: 1.06660  (0.84132)\n     | > loader_time: 0.00720  (0.00728)\n\n\n\u001b[1m   --> STEP: 354/406 -- GLOBAL_STEP: 6850\u001b[0m\n     | > loss: 0.80184  (0.82131)\n     | > log_mle: 0.09372  (0.11354)\n     | > loss_dur: 0.70812  (0.70776)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.00168  (1.96658)\n     | > current_lr: 0.00000 \n     | > step_time: 1.37060  (0.86804)\n     | > loader_time: 0.00980  (0.00743)\n\n\n\u001b[1m   --> STEP: 379/406 -- GLOBAL_STEP: 6875\u001b[0m\n     | > loss: 0.77447  (0.81886)\n     | > log_mle: 0.08641  (0.11180)\n     | > loss_dur: 0.68806  (0.70707)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.85755  (1.98492)\n     | > current_lr: 0.00000 \n     | > step_time: 1.23560  (0.89421)\n     | > loader_time: 0.00860  (0.00755)\n\n\n\u001b[1m   --> STEP: 404/406 -- GLOBAL_STEP: 6900\u001b[0m\n     | > loss: 0.80395  (0.81637)\n     | > log_mle: 0.08685  (0.11015)\n     | > loss_dur: 0.71710  (0.70623)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.30051  (1.96168)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93280  (0.91530)\n     | > loader_time: 0.00650  (0.00760)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00701 \u001b[0m(-0.00055)\n     | > avg_loss:\u001b[92m 0.78113 \u001b[0m(-0.08086)\n     | > avg_log_mle:\u001b[92m 0.10719 \u001b[0m(-0.02826)\n     | > avg_loss_dur:\u001b[92m 0.67394 \u001b[0m(-0.05260)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_6902.pth\n\n\u001b[4m\u001b[1m > EPOCH: 17/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:04:42) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 23/406 -- GLOBAL_STEP: 6925\u001b[0m\n     | > loss: 0.76965  (0.78112)\n     | > log_mle: 0.11830  (0.11911)\n     | > loss_dur: 0.65136  (0.66200)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.45087  (1.53376)\n     | > current_lr: 0.00000 \n     | > step_time: 0.61510  (0.55299)\n     | > loader_time: 0.00630  (0.00470)\n\n\n\u001b[1m   --> STEP: 48/406 -- GLOBAL_STEP: 6950\u001b[0m\n     | > loss: 0.78388  (0.77705)\n     | > log_mle: 0.10998  (0.11716)\n     | > loss_dur: 0.67390  (0.65989)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.07387  (1.57100)\n     | > current_lr: 0.00000 \n     | > step_time: 0.57560  (0.59112)\n     | > loader_time: 0.00420  (0.00555)\n\n\n\u001b[1m   --> STEP: 73/406 -- GLOBAL_STEP: 6975\u001b[0m\n     | > loss: 0.72994  (0.77220)\n     | > log_mle: 0.08631  (0.11273)\n     | > loss_dur: 0.64364  (0.65947)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.57144  (1.58850)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69460  (0.61304)\n     | > loader_time: 0.00480  (0.00566)\n\n\n\u001b[1m   --> STEP: 98/406 -- GLOBAL_STEP: 7000\u001b[0m\n     | > loss: 0.77710  (0.76824)\n     | > log_mle: 0.10270  (0.10867)\n     | > loss_dur: 0.67440  (0.65957)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.37249  (1.67226)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78700  (0.63565)\n     | > loader_time: 0.00710  (0.00585)\n\n\n\u001b[1m   --> STEP: 123/406 -- GLOBAL_STEP: 7025\u001b[0m\n     | > loss: 0.75867  (0.76419)\n     | > log_mle: 0.08240  (0.10517)\n     | > loss_dur: 0.67627  (0.65902)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.06432  (1.77529)\n     | > current_lr: 0.00000 \n     | > step_time: 0.86980  (0.65175)\n     | > loader_time: 0.00580  (0.00595)\n\n\n\u001b[1m   --> STEP: 148/406 -- GLOBAL_STEP: 7050\u001b[0m\n     | > loss: 0.75337  (0.76192)\n     | > log_mle: 0.08096  (0.10207)\n     | > loss_dur: 0.67241  (0.65985)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.52906  (1.76433)\n     | > current_lr: 0.00000 \n     | > step_time: 0.75980  (0.67017)\n     | > loader_time: 0.00800  (0.00618)\n\n\n\u001b[1m   --> STEP: 173/406 -- GLOBAL_STEP: 7075\u001b[0m\n     | > loss: 0.77129  (0.76003)\n     | > log_mle: 0.08642  (0.09957)\n     | > loss_dur: 0.68486  (0.66046)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.95077  (1.75869)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78420  (0.69077)\n     | > loader_time: 0.00900  (0.00627)\n\n\n\u001b[1m   --> STEP: 198/406 -- GLOBAL_STEP: 7100\u001b[0m\n     | > loss: 0.72957  (0.75812)\n     | > log_mle: 0.07820  (0.09713)\n     | > loss_dur: 0.65136  (0.66099)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.08994  (1.74580)\n     | > current_lr: 0.00000 \n     | > step_time: 0.84870  (0.70725)\n     | > loader_time: 0.00790  (0.00638)\n\n\n\u001b[1m   --> STEP: 223/406 -- GLOBAL_STEP: 7125\u001b[0m\n     | > loss: 0.72933  (0.75557)\n     | > log_mle: 0.07876  (0.09495)\n     | > loss_dur: 0.65057  (0.66063)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.89115  (1.76484)\n     | > current_lr: 0.00000 \n     | > step_time: 0.91990  (0.72992)\n     | > loader_time: 0.00560  (0.00657)\n\n\n\u001b[1m   --> STEP: 248/406 -- GLOBAL_STEP: 7150\u001b[0m\n     | > loss: 0.72454  (0.75347)\n     | > log_mle: 0.07788  (0.09292)\n     | > loss_dur: 0.64667  (0.66055)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.68126  (1.77728)\n     | > current_lr: 0.00000 \n     | > step_time: 0.95320  (0.75474)\n     | > loader_time: 0.00600  (0.00666)\n\n\n\u001b[1m   --> STEP: 273/406 -- GLOBAL_STEP: 7175\u001b[0m\n     | > loss: 0.71279  (0.75156)\n     | > log_mle: 0.06487  (0.09085)\n     | > loss_dur: 0.64792  (0.66071)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.16740  (1.80335)\n     | > current_lr: 0.00000 \n     | > step_time: 1.02170  (0.78084)\n     | > loader_time: 0.00830  (0.00682)\n\n\n\u001b[1m   --> STEP: 298/406 -- GLOBAL_STEP: 7200\u001b[0m\n     | > loss: 0.72634  (0.74964)\n     | > log_mle: 0.07958  (0.08921)\n     | > loss_dur: 0.64676  (0.66042)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.44301  (1.80793)\n     | > current_lr: 0.00000 \n     | > step_time: 1.16990  (0.80581)\n     | > loader_time: 0.00810  (0.00707)\n\n\n\u001b[1m   --> STEP: 323/406 -- GLOBAL_STEP: 7225\u001b[0m\n     | > loss: 0.72122  (0.74771)\n     | > log_mle: 0.06488  (0.08753)\n     | > loss_dur: 0.65635  (0.66018)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.37939  (1.79649)\n     | > current_lr: 0.00000 \n     | > step_time: 1.51570  (0.83048)\n     | > loader_time: 0.00900  (0.00715)\n\n\n\u001b[1m   --> STEP: 348/406 -- GLOBAL_STEP: 7250\u001b[0m\n     | > loss: 0.75142  (0.74636)\n     | > log_mle: 0.07415  (0.08590)\n     | > loss_dur: 0.67727  (0.66046)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.23904  (1.78252)\n     | > current_lr: 0.00000 \n     | > step_time: 1.32580  (0.85777)\n     | > loader_time: 0.00960  (0.00732)\n\n\n\u001b[1m   --> STEP: 373/406 -- GLOBAL_STEP: 7275\u001b[0m\n     | > loss: 0.72431  (0.74441)\n     | > log_mle: 0.05672  (0.08423)\n     | > loss_dur: 0.66759  (0.66018)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.32744  (1.76462)\n     | > current_lr: 0.00000 \n     | > step_time: 1.74300  (0.88919)\n     | > loader_time: 0.00980  (0.00746)\n\n\n\u001b[1m   --> STEP: 398/406 -- GLOBAL_STEP: 7300\u001b[0m\n     | > loss: 0.70069  (0.74226)\n     | > log_mle: 0.05396  (0.08268)\n     | > loss_dur: 0.64673  (0.65958)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.28194  (1.77665)\n     | > current_lr: 0.00000 \n     | > step_time: 0.91420  (0.91554)\n     | > loader_time: 0.00670  (0.00756)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00503 \u001b[0m(-0.00198)\n     | > avg_loss:\u001b[92m 0.72243 \u001b[0m(-0.05870)\n     | > avg_log_mle:\u001b[92m 0.08658 \u001b[0m(-0.02061)\n     | > avg_loss_dur:\u001b[92m 0.63586 \u001b[0m(-0.03809)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_7308.pth\n\n\u001b[4m\u001b[1m > EPOCH: 18/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:11:24) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 17/406 -- GLOBAL_STEP: 7325\u001b[0m\n     | > loss: 0.69568  (0.71607)\n     | > log_mle: 0.09285  (0.09124)\n     | > loss_dur: 0.60282  (0.62483)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.09895  (1.42593)\n     | > current_lr: 0.00000 \n     | > step_time: 0.63930  (0.56605)\n     | > loader_time: 0.00430  (0.00495)\n\n\n\u001b[1m   --> STEP: 42/406 -- GLOBAL_STEP: 7350\u001b[0m\n     | > loss: 0.69092  (0.70973)\n     | > log_mle: 0.08018  (0.09151)\n     | > loss_dur: 0.61074  (0.61823)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.88892  (1.54882)\n     | > current_lr: 0.00000 \n     | > step_time: 0.63080  (0.59969)\n     | > loader_time: 0.00600  (0.00530)\n\n\n\u001b[1m   --> STEP: 67/406 -- GLOBAL_STEP: 7375\u001b[0m\n     | > loss: 0.67631  (0.70618)\n     | > log_mle: 0.07527  (0.08788)\n     | > loss_dur: 0.60104  (0.61830)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.40841  (1.62296)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68410  (0.62170)\n     | > loader_time: 0.00460  (0.00562)\n\n\n\u001b[1m   --> STEP: 92/406 -- GLOBAL_STEP: 7400\u001b[0m\n     | > loss: 0.68206  (0.70258)\n     | > log_mle: 0.06073  (0.08373)\n     | > loss_dur: 0.62134  (0.61885)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.94736  (1.64538)\n     | > current_lr: 0.00000 \n     | > step_time: 0.78130  (0.64298)\n     | > loader_time: 0.00460  (0.00562)\n\n\n\u001b[1m   --> STEP: 117/406 -- GLOBAL_STEP: 7425\u001b[0m\n     | > loss: 0.69581  (0.69932)\n     | > log_mle: 0.06391  (0.08013)\n     | > loss_dur: 0.63190  (0.61919)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.19952  (1.77341)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70670  (0.65876)\n     | > loader_time: 0.00790  (0.00581)\n\n\n\u001b[1m   --> STEP: 142/406 -- GLOBAL_STEP: 7450\u001b[0m\n     | > loss: 0.67487  (0.69719)\n     | > log_mle: 0.05661  (0.07676)\n     | > loss_dur: 0.61827  (0.62042)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.39345  (1.77331)\n     | > current_lr: 0.00000 \n     | > step_time: 0.70380  (0.67958)\n     | > loader_time: 0.00710  (0.00619)\n\n\n\u001b[1m   --> STEP: 167/406 -- GLOBAL_STEP: 7475\u001b[0m\n     | > loss: 0.67120  (0.69505)\n     | > log_mle: 0.05465  (0.07409)\n     | > loss_dur: 0.61655  (0.62095)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.26778  (1.77536)\n     | > current_lr: 0.00000 \n     | > step_time: 0.82380  (0.69812)\n     | > loader_time: 0.00550  (0.00625)\n\n\n\u001b[1m   --> STEP: 192/406 -- GLOBAL_STEP: 7500\u001b[0m\n     | > loss: 0.64718  (0.69327)\n     | > log_mle: 0.05424  (0.07167)\n     | > loss_dur: 0.59294  (0.62161)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.47999  (1.77063)\n     | > current_lr: 0.00000 \n     | > step_time: 0.83550  (0.72006)\n     | > loader_time: 0.00820  (0.00645)\n\n\n\u001b[1m   --> STEP: 217/406 -- GLOBAL_STEP: 7525\u001b[0m\n     | > loss: 0.66231  (0.69105)\n     | > log_mle: 0.05201  (0.06961)\n     | > loss_dur: 0.61031  (0.62145)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.11677  (1.75195)\n     | > current_lr: 0.00000 \n     | > step_time: 1.00460  (0.74341)\n     | > loader_time: 0.00840  (0.00655)\n\n\n\u001b[1m   --> STEP: 242/406 -- GLOBAL_STEP: 7550\u001b[0m\n     | > loss: 0.66812  (0.68921)\n     | > log_mle: 0.03356  (0.06752)\n     | > loss_dur: 0.63456  (0.62169)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.53796  (1.76695)\n     | > current_lr: 0.00000 \n     | > step_time: 1.32110  (0.76731)\n     | > loader_time: 0.00760  (0.00659)\n\n\n\u001b[1m   --> STEP: 267/406 -- GLOBAL_STEP: 7575\u001b[0m\n     | > loss: 0.66135  (0.68763)\n     | > log_mle: 0.04493  (0.06577)\n     | > loss_dur: 0.61642  (0.62186)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.61560  (1.82883)\n     | > current_lr: 0.00000 \n     | > step_time: 1.00780  (0.79072)\n     | > loader_time: 0.00970  (0.00677)\n\n\n\u001b[1m   --> STEP: 292/406 -- GLOBAL_STEP: 7600\u001b[0m\n     | > loss: 0.68159  (0.68598)\n     | > log_mle: 0.05205  (0.06408)\n     | > loss_dur: 0.62954  (0.62190)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.18563  (1.85155)\n     | > current_lr: 0.00000 \n     | > step_time: 1.04050  (0.81331)\n     | > loader_time: 0.00880  (0.00686)\n\n\n\u001b[1m   --> STEP: 317/406 -- GLOBAL_STEP: 7625\u001b[0m\n     | > loss: 0.66821  (0.68447)\n     | > log_mle: 0.04939  (0.06250)\n     | > loss_dur: 0.61882  (0.62197)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.57502  (1.83681)\n     | > current_lr: 0.00000 \n     | > step_time: 1.03690  (0.83712)\n     | > loader_time: 0.00660  (0.00701)\n\n\n\u001b[1m   --> STEP: 342/406 -- GLOBAL_STEP: 7650\u001b[0m\n     | > loss: 0.65248  (0.68325)\n     | > log_mle: 0.03426  (0.06100)\n     | > loss_dur: 0.61822  (0.62225)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.77452  (1.86249)\n     | > current_lr: 0.00000 \n     | > step_time: 1.30170  (0.86322)\n     | > loader_time: 0.00950  (0.00712)\n\n\n\u001b[1m   --> STEP: 367/406 -- GLOBAL_STEP: 7675\u001b[0m\n     | > loss: 0.65862  (0.68152)\n     | > log_mle: 0.03966  (0.05944)\n     | > loss_dur: 0.61896  (0.62208)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.67194  (1.91312)\n     | > current_lr: 0.00000 \n     | > step_time: 1.17940  (0.88939)\n     | > loader_time: 0.01040  (0.00727)\n\n\n\u001b[1m   --> STEP: 392/406 -- GLOBAL_STEP: 7700\u001b[0m\n     | > loss: 0.65284  (0.67995)\n     | > log_mle: 0.03452  (0.05796)\n     | > loss_dur: 0.61832  (0.62200)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.24460  (1.91298)\n     | > current_lr: 0.00000 \n     | > step_time: 1.27810  (0.91623)\n     | > loader_time: 0.01010  (0.00739)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00575 \u001b[0m(+0.00072)\n     | > avg_loss:\u001b[92m 0.68064 \u001b[0m(-0.04179)\n     | > avg_log_mle:\u001b[92m 0.07431 \u001b[0m(-0.01226)\n     | > avg_loss_dur:\u001b[92m 0.60633 \u001b[0m(-0.02953)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_7714.pth\n\n\u001b[4m\u001b[1m > EPOCH: 19/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:18:08) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 11/406 -- GLOBAL_STEP: 7725\u001b[0m\n     | > loss: 0.65819  (0.66839)\n     | > log_mle: 0.06608  (0.07098)\n     | > loss_dur: 0.59211  (0.59741)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.02630  (1.39690)\n     | > current_lr: 0.00000 \n     | > step_time: 0.62100  (0.55839)\n     | > loader_time: 0.00430  (0.00362)\n\n\n\u001b[1m   --> STEP: 36/406 -- GLOBAL_STEP: 7750\u001b[0m\n     | > loss: 0.65277  (0.65775)\n     | > log_mle: 0.07795  (0.06816)\n     | > loss_dur: 0.57481  (0.58960)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.04787  (1.44149)\n     | > current_lr: 0.00000 \n     | > step_time: 0.56290  (0.58362)\n     | > loader_time: 0.00380  (0.00491)\n\n\n\u001b[1m   --> STEP: 61/406 -- GLOBAL_STEP: 7775\u001b[0m\n     | > loss: 0.66525  (0.65319)\n     | > log_mle: 0.05688  (0.06522)\n     | > loss_dur: 0.60837  (0.58797)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.66889  (1.44324)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59560  (0.59806)\n     | > loader_time: 0.00660  (0.00509)\n\n\n\u001b[1m   --> STEP: 86/406 -- GLOBAL_STEP: 7800\u001b[0m\n     | > loss: 0.64882  (0.65020)\n     | > log_mle: 0.05440  (0.06159)\n     | > loss_dur: 0.59441  (0.58861)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.01396  (1.59320)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66490  (0.62663)\n     | > loader_time: 0.00720  (0.00563)\n\n\n\u001b[1m   --> STEP: 111/406 -- GLOBAL_STEP: 7825\u001b[0m\n     | > loss: 0.61816  (0.64657)\n     | > log_mle: 0.03543  (0.05783)\n     | > loss_dur: 0.58274  (0.58874)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.74880  (1.62996)\n     | > current_lr: 0.00000 \n     | > step_time: 0.68170  (0.64863)\n     | > loader_time: 0.01650  (0.00597)\n\n\n\u001b[1m   --> STEP: 136/406 -- GLOBAL_STEP: 7850\u001b[0m\n     | > loss: 0.65603  (0.64446)\n     | > log_mle: 0.03582  (0.05461)\n     | > loss_dur: 0.62022  (0.58985)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.10738  (1.64091)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72570  (0.66452)\n     | > loader_time: 0.00700  (0.00602)\n\n\n\u001b[1m   --> STEP: 161/406 -- GLOBAL_STEP: 7875\u001b[0m\n     | > loss: 0.64023  (0.64274)\n     | > log_mle: 0.04315  (0.05214)\n     | > loss_dur: 0.59708  (0.59059)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.75141  (1.61000)\n     | > current_lr: 0.00000 \n     | > step_time: 0.72060  (0.68274)\n     | > loader_time: 0.00500  (0.00632)\n\n\n\u001b[1m   --> STEP: 186/406 -- GLOBAL_STEP: 7900\u001b[0m\n     | > loss: 0.63071  (0.64141)\n     | > log_mle: 0.03043  (0.04975)\n     | > loss_dur: 0.60028  (0.59166)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.99755  (1.64518)\n     | > current_lr: 0.00000 \n     | > step_time: 0.79700  (0.69940)\n     | > loader_time: 0.00530  (0.00634)\n\n\n\u001b[1m   --> STEP: 211/406 -- GLOBAL_STEP: 7925\u001b[0m\n     | > loss: 0.61019  (0.63978)\n     | > log_mle: 0.01857  (0.04770)\n     | > loss_dur: 0.59162  (0.59208)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.88151  (1.68220)\n     | > current_lr: 0.00000 \n     | > step_time: 0.82270  (0.72220)\n     | > loader_time: 0.00600  (0.00650)\n\n\n\u001b[1m   --> STEP: 236/406 -- GLOBAL_STEP: 7950\u001b[0m\n     | > loss: 0.62790  (0.63808)\n     | > log_mle: 0.04323  (0.04566)\n     | > loss_dur: 0.58468  (0.59242)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.13564  (1.73489)\n     | > current_lr: 0.00000 \n     | > step_time: 0.98400  (0.74446)\n     | > loader_time: 0.00840  (0.00661)\n\n\n\u001b[1m   --> STEP: 261/406 -- GLOBAL_STEP: 7975\u001b[0m\n     | > loss: 0.61734  (0.63683)\n     | > log_mle: 0.02791  (0.04370)\n     | > loss_dur: 0.58943  (0.59313)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.74634  (1.83267)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07920  (0.77210)\n     | > loader_time: 0.01560  (0.00681)\n\n\n\u001b[1m   --> STEP: 286/406 -- GLOBAL_STEP: 8000\u001b[0m\n     | > loss: 0.60244  (0.63540)\n     | > log_mle: 0.02186  (0.04197)\n     | > loss_dur: 0.58058  (0.59344)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.65123  (1.86188)\n     | > current_lr: 0.00000 \n     | > step_time: 1.05370  (0.79824)\n     | > loader_time: 0.00890  (0.00708)\n\n\n\u001b[1m   --> STEP: 311/406 -- GLOBAL_STEP: 8025\u001b[0m\n     | > loss: 0.61750  (0.63419)\n     | > log_mle: 0.01734  (0.04051)\n     | > loss_dur: 0.60016  (0.59368)\n     | > amp_scaler: 131072.00000  (68486.17363)\n     | > grad_norm: 1.45600  (1.82461)\n     | > current_lr: 0.00000 \n     | > step_time: 1.14030  (0.82479)\n     | > loader_time: 0.01180  (0.00755)\n\n\n\u001b[1m   --> STEP: 336/406 -- GLOBAL_STEP: 8050\u001b[0m\n     | > loss: 0.63115  (0.63308)\n     | > log_mle: 0.02937  (0.03907)\n     | > loss_dur: 0.60178  (0.59401)\n     | > amp_scaler: 131072.00000  (73142.85714)\n     | > grad_norm: 2.47739  (1.83007)\n     | > current_lr: 0.00000 \n     | > step_time: 1.32240  (0.85271)\n     | > loader_time: 0.01230  (0.00792)\n\n\n\u001b[1m   --> STEP: 361/406 -- GLOBAL_STEP: 8075\u001b[0m\n     | > loss: 0.62477  (0.63180)\n     | > log_mle: 0.02455  (0.03758)\n     | > loss_dur: 0.60022  (0.59422)\n     | > amp_scaler: 65536.00000  (73342.22715)\n     | > grad_norm: 3.11756  (1.88732)\n     | > current_lr: 0.00000 \n     | > step_time: 1.20440  (0.88218)\n     | > loader_time: 0.01310  (0.00828)\n\n\n\u001b[1m   --> STEP: 386/406 -- GLOBAL_STEP: 8100\u001b[0m\n     | > loss: 0.61723  (0.63045)\n     | > log_mle: 0.01973  (0.03620)\n     | > loss_dur: 0.59750  (0.59425)\n     | > amp_scaler: 65536.00000  (72836.64249)\n     | > grad_norm: 2.49132  (1.92433)\n     | > current_lr: 0.00000 \n     | > step_time: 1.31380  (0.90955)\n     | > loader_time: 0.01560  (0.00845)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00647 \u001b[0m(+0.00071)\n     | > avg_loss:\u001b[92m 0.63237 \u001b[0m(-0.04828)\n     | > avg_log_mle:\u001b[92m 0.05417 \u001b[0m(-0.02014)\n     | > avg_loss_dur:\u001b[92m 0.57819 \u001b[0m(-0.02813)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_8120.pth\n\n\u001b[4m\u001b[1m > EPOCH: 20/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:24:54) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 5/406 -- GLOBAL_STEP: 8125\u001b[0m\n     | > loss: 0.61664  (0.62611)\n     | > log_mle: 0.05923  (0.05236)\n     | > loss_dur: 0.55741  (0.57374)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.16749  (1.40597)\n     | > current_lr: 0.00000 \n     | > step_time: 0.49540  (0.47905)\n     | > loader_time: 0.00460  (0.00341)\n\n\n\u001b[1m   --> STEP: 30/406 -- GLOBAL_STEP: 8150\u001b[0m\n     | > loss: 0.60444  (0.60563)\n     | > log_mle: 0.04015  (0.04665)\n     | > loss_dur: 0.56429  (0.55897)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.64356  (1.39800)\n     | > current_lr: 0.00000 \n     | > step_time: 0.61160  (0.56969)\n     | > loader_time: 0.00390  (0.00533)\n\n\n\u001b[1m   --> STEP: 55/406 -- GLOBAL_STEP: 8175\u001b[0m\n     | > loss: 0.56961  (0.60330)\n     | > log_mle: 0.02893  (0.04448)\n     | > loss_dur: 0.54068  (0.55882)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.01181  (1.40317)\n     | > current_lr: 0.00000 \n     | > step_time: 0.59850  (0.58887)\n     | > loader_time: 0.00410  (0.00542)\n\n\n\u001b[1m   --> STEP: 80/406 -- GLOBAL_STEP: 8200\u001b[0m\n     | > loss: 0.58490  (0.60071)\n     | > log_mle: 0.04139  (0.04106)\n     | > loss_dur: 0.54351  (0.55965)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.34787  (1.42856)\n     | > current_lr: 0.00000 \n     | > step_time: 0.66040  (0.61272)\n     | > loader_time: 0.00700  (0.00544)\n\n\n\u001b[1m   --> STEP: 105/406 -- GLOBAL_STEP: 8225\u001b[0m\n     | > loss: 0.57088  (0.59715)\n     | > log_mle: 0.03423  (0.03717)\n     | > loss_dur: 0.53665  (0.55999)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.21744  (1.67399)\n     | > current_lr: 0.00000 \n     | > step_time: 0.74490  (0.63587)\n     | > loader_time: 0.00430  (0.00568)\n\n\n\u001b[1m   --> STEP: 130/406 -- GLOBAL_STEP: 8250\u001b[0m\n     | > loss: 0.57388  (0.59397)\n     | > log_mle: 0.01923  (0.03394)\n     | > loss_dur: 0.55464  (0.56003)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.65364  (1.92183)\n     | > current_lr: 0.00000 \n     | > step_time: 0.69180  (0.66197)\n     | > loader_time: 0.00470  (0.00616)\n\n\n\u001b[1m   --> STEP: 155/406 -- GLOBAL_STEP: 8275\u001b[0m\n     | > loss: 0.57909  (0.59265)\n     | > log_mle: 0.02259  (0.03144)\n     | > loss_dur: 0.55650  (0.56121)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.16673  (2.02238)\n     | > current_lr: 0.00000 \n     | > step_time: 0.77830  (0.68016)\n     | > loader_time: 0.00730  (0.00624)\n\n\n\u001b[1m   --> STEP: 180/406 -- GLOBAL_STEP: 8300\u001b[0m\n     | > loss: 0.56442  (0.59129)\n     | > log_mle: 0.01230  (0.02919)\n     | > loss_dur: 0.55212  (0.56210)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.58747  (2.00266)\n     | > current_lr: 0.00000 \n     | > step_time: 0.93510  (0.70320)\n     | > loader_time: 0.00640  (0.00634)\n\n\n\u001b[1m   --> STEP: 205/406 -- GLOBAL_STEP: 8325\u001b[0m\n     | > loss: 0.57346  (0.58949)\n     | > log_mle: 0.00409  (0.02712)\n     | > loss_dur: 0.56937  (0.56237)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.93832  (1.97509)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92790  (0.72787)\n     | > loader_time: 0.00860  (0.00651)\n\n\n\u001b[1m   --> STEP: 230/406 -- GLOBAL_STEP: 8350\u001b[0m\n     | > loss: 0.57816  (0.58705)\n     | > log_mle: 0.01122  (0.02510)\n     | > loss_dur: 0.56694  (0.56195)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.58346  (1.99963)\n     | > current_lr: 0.00000 \n     | > step_time: 0.92140  (0.75290)\n     | > loader_time: 0.00560  (0.00663)\n\n\n\u001b[1m   --> STEP: 255/406 -- GLOBAL_STEP: 8375\u001b[0m\n     | > loss: 0.57074  (0.58529)\n     | > log_mle: 0.00815  (0.02324)\n     | > loss_dur: 0.56259  (0.56205)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.85856  (2.03319)\n     | > current_lr: 0.00000 \n     | > step_time: 0.96210  (0.77605)\n     | > loader_time: 0.00680  (0.00673)\n\n\n\u001b[1m   --> STEP: 280/406 -- GLOBAL_STEP: 8400\u001b[0m\n     | > loss: 0.56779  (0.58330)\n     | > log_mle: -0.00350  (0.02157)\n     | > loss_dur: 0.57129  (0.56173)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.95073  (2.02101)\n     | > current_lr: 0.00000 \n     | > step_time: 1.00650  (0.80190)\n     | > loader_time: 0.00730  (0.00685)\n\n\n\u001b[1m   --> STEP: 305/406 -- GLOBAL_STEP: 8425\u001b[0m\n     | > loss: 0.58090  (0.58168)\n     | > log_mle: 0.00472  (0.02008)\n     | > loss_dur: 0.57618  (0.56160)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.18267  (2.03454)\n     | > current_lr: 0.00000 \n     | > step_time: 1.07500  (0.82578)\n     | > loader_time: 0.00910  (0.00696)\n\n\n\u001b[1m   --> STEP: 330/406 -- GLOBAL_STEP: 8450\u001b[0m\n     | > loss: 0.57097  (0.57995)\n     | > log_mle: -0.00965  (0.01876)\n     | > loss_dur: 0.58062  (0.56120)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.46234  (2.05226)\n     | > current_lr: 0.00000 \n     | > step_time: 1.15610  (0.85196)\n     | > loader_time: 0.00610  (0.00706)\n\n\n\u001b[1m   --> STEP: 355/406 -- GLOBAL_STEP: 8475\u001b[0m\n     | > loss: 0.55094  (0.57840)\n     | > log_mle: -0.00997  (0.01731)\n     | > loss_dur: 0.56091  (0.56109)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.35824  (2.07918)\n     | > current_lr: 0.00000 \n     | > step_time: 1.35920  (0.87980)\n     | > loader_time: 0.00930  (0.00709)\n\n\n\u001b[1m   --> STEP: 380/406 -- GLOBAL_STEP: 8500\u001b[0m\n     | > loss: 0.55001  (0.57638)\n     | > log_mle: -0.00346  (0.01591)\n     | > loss_dur: 0.55347  (0.56047)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.39031  (2.06454)\n     | > current_lr: 0.00000 \n     | > step_time: 1.27330  (0.90893)\n     | > loader_time: 0.00780  (0.00720)\n\n\n\u001b[1m   --> STEP: 405/406 -- GLOBAL_STEP: 8525\u001b[0m\n     | > loss: 0.49716  (0.57432)\n     | > log_mle: -0.00328  (0.01459)\n     | > loss_dur: 0.50044  (0.55973)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.31882  (2.06299)\n     | > current_lr: 0.00000 \n     | > step_time: 0.44270  (0.92650)\n     | > loader_time: 0.00430  (0.00727)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00622 \u001b[0m(-0.00025)\n     | > avg_loss:\u001b[92m 0.59145 \u001b[0m(-0.04091)\n     | > avg_log_mle:\u001b[92m 0.05053 \u001b[0m(-0.00364)\n     | > avg_loss_dur:\u001b[92m 0.54092 \u001b[0m(-0.03727)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_8526.pth\n\n\u001b[4m\u001b[1m > EPOCH: 21/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:31:41) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 24/406 -- GLOBAL_STEP: 8550\u001b[0m\n     | > loss: 0.55398  (0.55018)\n     | > log_mle: 0.02224  (0.02787)\n     | > loss_dur: 0.53174  (0.52231)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.28011  (1.54792)\n     | > current_lr: 0.00001 \n     | > step_time: 0.57580  (0.53021)\n     | > loader_time: 0.00560  (0.00412)\n\n\n\u001b[1m   --> STEP: 49/406 -- GLOBAL_STEP: 8575\u001b[0m\n     | > loss: 0.54690  (0.54762)\n     | > log_mle: 0.02328  (0.02667)\n     | > loss_dur: 0.52362  (0.52095)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.43547  (1.55728)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65260  (0.57700)\n     | > loader_time: 0.00420  (0.00497)\n\n\n\u001b[1m   --> STEP: 74/406 -- GLOBAL_STEP: 8600\u001b[0m\n     | > loss: 0.53575  (0.54174)\n     | > log_mle: -0.00707  (0.02233)\n     | > loss_dur: 0.54282  (0.51941)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 5.51744  (2.00247)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67270  (0.61503)\n     | > loader_time: 0.00450  (0.00516)\n\n\n\u001b[1m   --> STEP: 99/406 -- GLOBAL_STEP: 8625\u001b[0m\n     | > loss: 0.54227  (0.53878)\n     | > log_mle: 0.00950  (0.01891)\n     | > loss_dur: 0.53277  (0.51986)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.73750  (2.06557)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66080  (0.64591)\n     | > loader_time: 0.00450  (0.00554)\n\n\n\u001b[1m   --> STEP: 124/406 -- GLOBAL_STEP: 8650\u001b[0m\n     | > loss: 0.51227  (0.53554)\n     | > log_mle: 0.00036  (0.01563)\n     | > loss_dur: 0.51191  (0.51991)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.19846  (2.20167)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01510  (0.66996)\n     | > loader_time: 0.00490  (0.00580)\n\n\n\u001b[1m   --> STEP: 149/406 -- GLOBAL_STEP: 8675\u001b[0m\n     | > loss: 0.51509  (0.53348)\n     | > log_mle: -0.00535  (0.01278)\n     | > loss_dur: 0.52043  (0.52070)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.08456  (2.08178)\n     | > current_lr: 0.00001 \n     | > step_time: 0.78460  (0.69225)\n     | > loader_time: 0.00490  (0.00592)\n\n\n\u001b[1m   --> STEP: 174/406 -- GLOBAL_STEP: 8700\u001b[0m\n     | > loss: 0.50393  (0.53206)\n     | > log_mle: -0.00328  (0.01063)\n     | > loss_dur: 0.50721  (0.52144)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.87414  (2.05259)\n     | > current_lr: 0.00001 \n     | > step_time: 0.83360  (0.71438)\n     | > loader_time: 0.00840  (0.00620)\n\n\n\u001b[1m   --> STEP: 199/406 -- GLOBAL_STEP: 8725\u001b[0m\n     | > loss: 0.53816  (0.53034)\n     | > log_mle: 0.00187  (0.00848)\n     | > loss_dur: 0.53629  (0.52186)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.82473  (2.16174)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18890  (0.73769)\n     | > loader_time: 0.00900  (0.00630)\n\n\n\u001b[1m   --> STEP: 224/406 -- GLOBAL_STEP: 8750\u001b[0m\n     | > loss: 0.50695  (0.52811)\n     | > log_mle: -0.00506  (0.00652)\n     | > loss_dur: 0.51201  (0.52159)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.01028  (2.23505)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15350  (0.76014)\n     | > loader_time: 0.00860  (0.00641)\n\n\n\u001b[1m   --> STEP: 249/406 -- GLOBAL_STEP: 8775\u001b[0m\n     | > loss: 0.53065  (0.52646)\n     | > log_mle: -0.00380  (0.00477)\n     | > loss_dur: 0.53445  (0.52170)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.30970  (2.21679)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05940  (0.78217)\n     | > loader_time: 0.00830  (0.00646)\n\n\n\u001b[1m   --> STEP: 274/406 -- GLOBAL_STEP: 8800\u001b[0m\n     | > loss: 0.51037  (0.52480)\n     | > log_mle: -0.01613  (0.00301)\n     | > loss_dur: 0.52650  (0.52179)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 5.81292  (2.38091)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99970  (0.80393)\n     | > loader_time: 0.00890  (0.00652)\n\n\n\u001b[1m   --> STEP: 299/406 -- GLOBAL_STEP: 8825\u001b[0m\n     | > loss: 0.49482  (0.52318)\n     | > log_mle: -0.03163  (0.00164)\n     | > loss_dur: 0.52645  (0.52154)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.81823  (2.43171)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18530  (0.82782)\n     | > loader_time: 0.00950  (0.00662)\n\n\n\u001b[1m   --> STEP: 324/406 -- GLOBAL_STEP: 8850\u001b[0m\n     | > loss: 0.49202  (0.52175)\n     | > log_mle: -0.01706  (0.00033)\n     | > loss_dur: 0.50908  (0.52142)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.20940  (2.48985)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12880  (0.85146)\n     | > loader_time: 0.00640  (0.00678)\n\n\n\u001b[1m   --> STEP: 349/406 -- GLOBAL_STEP: 8875\u001b[0m\n     | > loss: 0.51659  (0.52050)\n     | > log_mle: -0.01651  (-0.00099)\n     | > loss_dur: 0.53310  (0.52148)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.76162  (2.51875)\n     | > current_lr: 0.00001 \n     | > step_time: 1.13910  (0.87637)\n     | > loader_time: 0.01550  (0.00718)\n\n\n\u001b[1m   --> STEP: 374/406 -- GLOBAL_STEP: 8900\u001b[0m\n     | > loss: 0.48118  (0.51856)\n     | > log_mle: -0.01875  (-0.00237)\n     | > loss_dur: 0.49992  (0.52094)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.46497  (2.46149)\n     | > current_lr: 0.00001 \n     | > step_time: 1.36800  (0.90530)\n     | > loader_time: 0.01610  (0.00771)\n\n\n\u001b[1m   --> STEP: 399/406 -- GLOBAL_STEP: 8925\u001b[0m\n     | > loss: 0.49954  (0.51694)\n     | > log_mle: -0.02398  (-0.00366)\n     | > loss_dur: 0.52352  (0.52060)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.62760  (2.45157)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93290  (0.93041)\n     | > loader_time: 0.00680  (0.00799)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00715 \u001b[0m(+0.00093)\n     | > avg_loss:\u001b[92m 0.53521 \u001b[0m(-0.05624)\n     | > avg_log_mle:\u001b[92m 0.03589 \u001b[0m(-0.01464)\n     | > avg_loss_dur:\u001b[92m 0.49931 \u001b[0m(-0.04161)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_8932.pth\n\n\u001b[4m\u001b[1m > EPOCH: 22/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:38:29) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 18/406 -- GLOBAL_STEP: 8950\u001b[0m\n     | > loss: 0.51599  (0.49585)\n     | > log_mle: 0.01443  (0.00895)\n     | > loss_dur: 0.50156  (0.48690)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.40928  (1.65462)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60020  (0.54823)\n     | > loader_time: 0.00380  (0.00451)\n\n\n\u001b[1m   --> STEP: 43/406 -- GLOBAL_STEP: 8975\u001b[0m\n     | > loss: 0.46994  (0.49199)\n     | > log_mle: 0.01237  (0.00936)\n     | > loss_dur: 0.45757  (0.48263)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.16769  (1.58768)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61100  (0.58892)\n     | > loader_time: 0.00530  (0.00477)\n\n\n\u001b[1m   --> STEP: 68/406 -- GLOBAL_STEP: 9000\u001b[0m\n     | > loss: 0.47995  (0.48877)\n     | > log_mle: -0.00651  (0.00580)\n     | > loss_dur: 0.48646  (0.48297)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.54230  (1.82834)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66600  (0.62247)\n     | > loader_time: 0.00470  (0.00531)\n\n\n\u001b[1m   --> STEP: 93/406 -- GLOBAL_STEP: 9025\u001b[0m\n     | > loss: 0.47654  (0.48548)\n     | > log_mle: -0.02455  (0.00183)\n     | > loss_dur: 0.50109  (0.48365)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.25740  (2.20508)\n     | > current_lr: 0.00001 \n     | > step_time: 0.85960  (0.64673)\n     | > loader_time: 0.00460  (0.00606)\n\n\n\u001b[1m   --> STEP: 118/406 -- GLOBAL_STEP: 9050\u001b[0m\n     | > loss: 0.47503  (0.48329)\n     | > log_mle: -0.01390  (-0.00124)\n     | > loss_dur: 0.48893  (0.48453)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.63912  (2.24063)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69540  (0.66293)\n     | > loader_time: 0.00470  (0.00624)\n\n\n\u001b[1m   --> STEP: 143/406 -- GLOBAL_STEP: 9075\u001b[0m\n     | > loss: 0.45997  (0.48121)\n     | > log_mle: -0.02727  (-0.00428)\n     | > loss_dur: 0.48724  (0.48548)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.54169  (2.29065)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80030  (0.68406)\n     | > loader_time: 0.00690  (0.00635)\n\n\n\u001b[1m   --> STEP: 168/406 -- GLOBAL_STEP: 9100\u001b[0m\n     | > loss: 0.49153  (0.47953)\n     | > log_mle: -0.00707  (-0.00636)\n     | > loss_dur: 0.49860  (0.48589)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.08617  (2.34728)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84160  (0.70308)\n     | > loader_time: 0.00580  (0.00649)\n\n\n\u001b[1m   --> STEP: 193/406 -- GLOBAL_STEP: 9125\u001b[0m\n     | > loss: 0.47100  (0.47759)\n     | > log_mle: -0.02473  (-0.00858)\n     | > loss_dur: 0.49573  (0.48617)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.69711  (2.36397)\n     | > current_lr: 0.00001 \n     | > step_time: 0.83520  (0.72364)\n     | > loader_time: 0.00620  (0.00654)\n\n\n\u001b[1m   --> STEP: 218/406 -- GLOBAL_STEP: 9150\u001b[0m\n     | > loss: 0.47079  (0.47573)\n     | > log_mle: -0.02170  (-0.01032)\n     | > loss_dur: 0.49249  (0.48605)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.02274  (2.38466)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92900  (0.74644)\n     | > loader_time: 0.00550  (0.00672)\n\n\n\u001b[1m   --> STEP: 243/406 -- GLOBAL_STEP: 9175\u001b[0m\n     | > loss: 0.46695  (0.47412)\n     | > log_mle: -0.04099  (-0.01224)\n     | > loss_dur: 0.50793  (0.48636)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.78030  (2.35761)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98000  (0.76986)\n     | > loader_time: 0.00600  (0.00683)\n\n\n\u001b[1m   --> STEP: 268/406 -- GLOBAL_STEP: 9200\u001b[0m\n     | > loss: 0.44439  (0.47246)\n     | > log_mle: -0.03470  (-0.01373)\n     | > loss_dur: 0.47910  (0.48619)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.87759  (2.40151)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15050  (0.79234)\n     | > loader_time: 0.00730  (0.00686)\n\n\n\u001b[1m   --> STEP: 293/406 -- GLOBAL_STEP: 9225\u001b[0m\n     | > loss: 0.46702  (0.47078)\n     | > log_mle: -0.03682  (-0.01523)\n     | > loss_dur: 0.50384  (0.48601)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.90354  (2.40071)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09940  (0.81549)\n     | > loader_time: 0.00870  (0.00703)\n\n\n\u001b[1m   --> STEP: 318/406 -- GLOBAL_STEP: 9250\u001b[0m\n     | > loss: 0.44993  (0.46945)\n     | > log_mle: -0.03033  (-0.01658)\n     | > loss_dur: 0.48026  (0.48604)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.69970  (2.39301)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17570  (0.83995)\n     | > loader_time: 0.02470  (0.00720)\n\n\n\u001b[1m   --> STEP: 343/406 -- GLOBAL_STEP: 9275\u001b[0m\n     | > loss: 0.45415  (0.46832)\n     | > log_mle: -0.04423  (-0.01789)\n     | > loss_dur: 0.49838  (0.48621)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.84038  (2.40780)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17070  (0.86602)\n     | > loader_time: 0.01410  (0.00749)\n\n\n\u001b[1m   --> STEP: 368/406 -- GLOBAL_STEP: 9300\u001b[0m\n     | > loss: 0.44037  (0.46659)\n     | > log_mle: -0.04628  (-0.01923)\n     | > loss_dur: 0.48665  (0.48582)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.89856  (2.48356)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23350  (0.89292)\n     | > loader_time: 0.00770  (0.00787)\n\n\n\u001b[1m   --> STEP: 393/406 -- GLOBAL_STEP: 9325\u001b[0m\n     | > loss: 0.42897  (0.46510)\n     | > log_mle: -0.03713  (-0.02043)\n     | > loss_dur: 0.46610  (0.48553)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.85742  (2.49622)\n     | > current_lr: 0.00001 \n     | > step_time: 1.41410  (0.92010)\n     | > loader_time: 0.01150  (0.00802)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00624 \u001b[0m(-0.00092)\n     | > avg_loss:\u001b[92m 0.49343 \u001b[0m(-0.04178)\n     | > avg_log_mle:\u001b[92m 0.02774 \u001b[0m(-0.00816)\n     | > avg_loss_dur:\u001b[92m 0.46569 \u001b[0m(-0.03362)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_9338.pth\n\n\u001b[4m\u001b[1m > EPOCH: 23/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:45:16) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 12/406 -- GLOBAL_STEP: 9350\u001b[0m\n     | > loss: 0.45081  (0.44647)\n     | > log_mle: -0.01426  (-0.00528)\n     | > loss_dur: 0.46507  (0.45175)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.35728  (1.51348)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56990  (0.57293)\n     | > loader_time: 0.00330  (0.00476)\n\n\n\u001b[1m   --> STEP: 37/406 -- GLOBAL_STEP: 9375\u001b[0m\n     | > loss: 0.42556  (0.44022)\n     | > log_mle: -0.00267  (-0.00666)\n     | > loss_dur: 0.42823  (0.44689)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.01696  (1.97130)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61600  (0.57256)\n     | > loader_time: 0.00700  (0.00528)\n\n\n\u001b[1m   --> STEP: 62/406 -- GLOBAL_STEP: 9400\u001b[0m\n     | > loss: 0.42472  (0.43914)\n     | > log_mle: -0.00316  (-0.00944)\n     | > loss_dur: 0.42788  (0.44858)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.91212  (2.08100)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79600  (0.61272)\n     | > loader_time: 0.00460  (0.00586)\n\n\n\u001b[1m   --> STEP: 87/406 -- GLOBAL_STEP: 9425\u001b[0m\n     | > loss: 0.41112  (0.43595)\n     | > log_mle: -0.03897  (-0.01334)\n     | > loss_dur: 0.45008  (0.44929)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.50382  (2.20769)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64790  (0.62652)\n     | > loader_time: 0.00440  (0.00590)\n\n\n\u001b[1m   --> STEP: 112/406 -- GLOBAL_STEP: 9450\u001b[0m\n     | > loss: 0.42010  (0.43285)\n     | > log_mle: -0.03635  (-0.01683)\n     | > loss_dur: 0.45645  (0.44968)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.83376  (2.30956)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70700  (0.65037)\n     | > loader_time: 0.00670  (0.00596)\n\n\n\u001b[1m   --> STEP: 137/406 -- GLOBAL_STEP: 9475\u001b[0m\n     | > loss: 0.43548  (0.43048)\n     | > log_mle: -0.03110  (-0.01975)\n     | > loss_dur: 0.46659  (0.45023)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 4.61723  (2.54806)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66630  (0.66433)\n     | > loader_time: 0.03920  (0.00637)\n\n\n\u001b[1m   --> STEP: 162/406 -- GLOBAL_STEP: 9500\u001b[0m\n     | > loss: 0.43471  (0.42904)\n     | > log_mle: -0.05294  (-0.02214)\n     | > loss_dur: 0.48766  (0.45118)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 4.19455  (2.48318)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81810  (0.68461)\n     | > loader_time: 0.01810  (0.00642)\n\n\n\u001b[1m   --> STEP: 187/406 -- GLOBAL_STEP: 9525\u001b[0m\n     | > loss: 0.41166  (0.42766)\n     | > log_mle: -0.03889  (-0.02428)\n     | > loss_dur: 0.45056  (0.45194)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.77986  (2.53701)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82150  (0.70898)\n     | > loader_time: 0.00650  (0.00655)\n\n\n\u001b[1m   --> STEP: 212/406 -- GLOBAL_STEP: 9550\u001b[0m\n     | > loss: 0.43044  (0.42590)\n     | > log_mle: -0.04565  (-0.02620)\n     | > loss_dur: 0.47609  (0.45211)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 4.07996  (2.57033)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90750  (0.73024)\n     | > loader_time: 0.00870  (0.00666)\n\n\n\u001b[1m   --> STEP: 237/406 -- GLOBAL_STEP: 9575\u001b[0m\n     | > loss: 0.40911  (0.42421)\n     | > log_mle: -0.04961  (-0.02806)\n     | > loss_dur: 0.45872  (0.45226)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 4.44985  (2.58460)\n     | > current_lr: 0.00001 \n     | > step_time: 1.22570  (0.75824)\n     | > loader_time: 0.00610  (0.00683)\n\n\n\u001b[1m   --> STEP: 262/406 -- GLOBAL_STEP: 9600\u001b[0m\n     | > loss: 0.40572  (0.42259)\n     | > log_mle: -0.03332  (-0.02972)\n     | > loss_dur: 0.43904  (0.45232)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.34414  (2.64003)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94180  (0.78581)\n     | > loader_time: 0.00630  (0.00698)\n\n\n\u001b[1m   --> STEP: 287/406 -- GLOBAL_STEP: 9625\u001b[0m\n     | > loss: 0.43506  (0.42093)\n     | > log_mle: -0.04015  (-0.03124)\n     | > loss_dur: 0.47521  (0.45218)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.96732  (2.62879)\n     | > current_lr: 0.00001 \n     | > step_time: 1.29590  (0.81378)\n     | > loader_time: 0.01420  (0.00734)\n\n\n\u001b[1m   --> STEP: 312/406 -- GLOBAL_STEP: 9650\u001b[0m\n     | > loss: 0.41609  (0.41973)\n     | > log_mle: -0.04319  (-0.03250)\n     | > loss_dur: 0.45929  (0.45222)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.53233  (2.57368)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12130  (0.84195)\n     | > loader_time: 0.01470  (0.00800)\n\n\n\u001b[1m   --> STEP: 337/406 -- GLOBAL_STEP: 9675\u001b[0m\n     | > loss: 0.40496  (0.41844)\n     | > log_mle: -0.05211  (-0.03374)\n     | > loss_dur: 0.45707  (0.45218)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.62513  (2.54471)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32250  (0.87083)\n     | > loader_time: 0.01420  (0.00842)\n\n\n\u001b[1m   --> STEP: 362/406 -- GLOBAL_STEP: 9700\u001b[0m\n     | > loss: 0.39199  (0.41689)\n     | > log_mle: -0.05849  (-0.03505)\n     | > loss_dur: 0.45049  (0.45194)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 7.86422  (2.65249)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23700  (0.89740)\n     | > loader_time: 0.01860  (0.00896)\n\n\n\u001b[1m   --> STEP: 387/406 -- GLOBAL_STEP: 9725\u001b[0m\n     | > loss: 0.38213  (0.41545)\n     | > log_mle: -0.05377  (-0.03623)\n     | > loss_dur: 0.43590  (0.45168)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.71864  (2.62255)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31760  (0.92657)\n     | > loader_time: 0.01740  (0.00931)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00618 \u001b[0m(-0.00006)\n     | > avg_loss:\u001b[92m 0.44396 \u001b[0m(-0.04947)\n     | > avg_log_mle:\u001b[92m 0.01184 \u001b[0m(-0.01590)\n     | > avg_loss_dur:\u001b[92m 0.43213 \u001b[0m(-0.03357)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_9744.pth\n\n\u001b[4m\u001b[1m > EPOCH: 24/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:52:08) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 6/406 -- GLOBAL_STEP: 9750\u001b[0m\n     | > loss: 0.38787  (0.40139)\n     | > log_mle: -0.02238  (-0.01879)\n     | > loss_dur: 0.41025  (0.42017)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.42065  (1.44823)\n     | > current_lr: 0.00001 \n     | > step_time: 0.51270  (0.58572)\n     | > loader_time: 0.00330  (0.00388)\n\n\n\u001b[1m   --> STEP: 31/406 -- GLOBAL_STEP: 9775\u001b[0m\n     | > loss: 0.38297  (0.39197)\n     | > log_mle: -0.02109  (-0.02269)\n     | > loss_dur: 0.40406  (0.41466)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.84363  (1.89391)\n     | > current_lr: 0.00001 \n     | > step_time: 0.59240  (0.58489)\n     | > loader_time: 0.00700  (0.00494)\n\n\n\u001b[1m   --> STEP: 56/406 -- GLOBAL_STEP: 9800\u001b[0m\n     | > loss: 0.39225  (0.39049)\n     | > log_mle: -0.03202  (-0.02471)\n     | > loss_dur: 0.42427  (0.41521)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.98197  (2.25528)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62760  (0.61513)\n     | > loader_time: 0.00500  (0.00538)\n\n\n\u001b[1m   --> STEP: 81/406 -- GLOBAL_STEP: 9825\u001b[0m\n     | > loss: 0.38630  (0.38823)\n     | > log_mle: -0.04213  (-0.02785)\n     | > loss_dur: 0.42842  (0.41608)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.04801  (2.44794)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62740  (0.63601)\n     | > loader_time: 0.00630  (0.00559)\n\n\n\u001b[1m   --> STEP: 106/406 -- GLOBAL_STEP: 9850\u001b[0m\n     | > loss: 0.36778  (0.38562)\n     | > log_mle: -0.03545  (-0.03139)\n     | > loss_dur: 0.40323  (0.41701)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.76175  (2.75329)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65670  (0.66095)\n     | > loader_time: 0.00550  (0.00609)\n\n\n\u001b[1m   --> STEP: 131/406 -- GLOBAL_STEP: 9875\u001b[0m\n     | > loss: 0.35289  (0.38329)\n     | > log_mle: -0.04476  (-0.03444)\n     | > loss_dur: 0.39765  (0.41772)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.59146  (2.91369)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71670  (0.67650)\n     | > loader_time: 0.00770  (0.00625)\n\n\n\u001b[1m   --> STEP: 156/406 -- GLOBAL_STEP: 9900\u001b[0m\n     | > loss: 0.36125  (0.38212)\n     | > log_mle: -0.05310  (-0.03672)\n     | > loss_dur: 0.41435  (0.41885)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.07857  (2.96562)\n     | > current_lr: 0.00001 \n     | > step_time: 0.86470  (0.69705)\n     | > loader_time: 0.00660  (0.00643)\n\n\n\u001b[1m   --> STEP: 181/406 -- GLOBAL_STEP: 9925\u001b[0m\n     | > loss: 0.37012  (0.38089)\n     | > log_mle: -0.04855  (-0.03872)\n     | > loss_dur: 0.41867  (0.41961)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 4.71128  (3.05308)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80750  (0.71962)\n     | > loader_time: 0.00540  (0.00662)\n\n\n\u001b[1m   --> STEP: 206/406 -- GLOBAL_STEP: 9950\u001b[0m\n     | > loss: 0.36556  (0.37952)\n     | > log_mle: -0.05256  (-0.04058)\n     | > loss_dur: 0.41811  (0.42010)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.27672  (3.04585)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92720  (0.73992)\n     | > loader_time: 0.00590  (0.00663)\n\n\n\u001b[1m   --> STEP: 231/406 -- GLOBAL_STEP: 9975\u001b[0m\n     | > loss: 0.36658  (0.37815)\n     | > log_mle: -0.06329  (-0.04242)\n     | > loss_dur: 0.42987  (0.42057)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.23597  (3.04446)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97330  (0.76363)\n     | > loader_time: 0.00850  (0.00674)\n\n\n\u001b[1m   --> STEP: 256/406 -- GLOBAL_STEP: 10000\u001b[0m\n     | > loss: 0.35396  (0.37665)\n     | > log_mle: -0.05968  (-0.04407)\n     | > loss_dur: 0.41364  (0.42072)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.13779  (3.02035)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97530  (0.78760)\n     | > loader_time: 0.00580  (0.00690)\n\n\n > CHECKPOINT : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/checkpoint_10000.pth\n\n\u001b[1m   --> STEP: 281/406 -- GLOBAL_STEP: 10025\u001b[0m\n     | > loss: 0.36108  (0.37487)\n     | > log_mle: -0.07052  (-0.04559)\n     | > loss_dur: 0.43160  (0.42046)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.36879  (3.04465)\n     | > current_lr: 0.00001 \n     | > step_time: 1.06960  (0.81206)\n     | > loader_time: 0.00900  (0.00697)\n\n\n\u001b[1m   --> STEP: 306/406 -- GLOBAL_STEP: 10050\u001b[0m\n     | > loss: 0.37407  (0.37390)\n     | > log_mle: -0.05998  (-0.04689)\n     | > loss_dur: 0.43405  (0.42079)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.42713  (3.02801)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08440  (0.83567)\n     | > loader_time: 0.00640  (0.00707)\n\n\n\u001b[1m   --> STEP: 331/406 -- GLOBAL_STEP: 10075\u001b[0m\n     | > loss: 0.36230  (0.37273)\n     | > log_mle: -0.06242  (-0.04807)\n     | > loss_dur: 0.42471  (0.42080)\n     | > amp_scaler: 131072.00000  (69693.87311)\n     | > grad_norm: 5.69688  (3.03924)\n     | > current_lr: 0.00001 \n     | > step_time: 1.19820  (0.86035)\n     | > loader_time: 0.00770  (0.00730)\n\n\n\u001b[1m   --> STEP: 356/406 -- GLOBAL_STEP: 10100\u001b[0m\n     | > loss: 0.36683  (0.37165)\n     | > log_mle: -0.07130  (-0.04935)\n     | > loss_dur: 0.43813  (0.42101)\n     | > amp_scaler: 65536.00000  (71058.69663)\n     | > grad_norm: 4.04895  (3.07534)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23600  (0.88711)\n     | > loader_time: 0.01560  (0.00772)\n\n\n\u001b[1m   --> STEP: 381/406 -- GLOBAL_STEP: 10125\u001b[0m\n     | > loss: 0.36161  (0.37010)\n     | > log_mle: -0.06024  (-0.05056)\n     | > loss_dur: 0.42185  (0.42067)\n     | > amp_scaler: 65536.00000  (70696.31496)\n     | > grad_norm: 4.59035  (3.12440)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24220  (0.91672)\n     | > loader_time: 0.01690  (0.00816)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00517 \u001b[0m(-0.00101)\n     | > avg_loss:\u001b[92m 0.39777 \u001b[0m(-0.04619)\n     | > avg_log_mle:\u001b[92m -0.00617 \u001b[0m(-0.01800)\n     | > avg_loss_dur:\u001b[92m 0.40394 \u001b[0m(-0.02819)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_10150.pth\n\n\u001b[4m\u001b[1m > EPOCH: 25/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 16:59:04) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 0/406 -- GLOBAL_STEP: 10150\u001b[0m\n     | > loss: 0.38963  (0.38963)\n     | > log_mle: -0.02114  (-0.02114)\n     | > loss_dur: 0.41077  (0.41077)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 1.64024  (1.64024)\n     | > current_lr: 0.00001 \n     | > step_time: 1.70970  (1.70970)\n     | > loader_time: 1.22000  (1.21999)\n\n\n\u001b[1m   --> STEP: 25/406 -- GLOBAL_STEP: 10175\u001b[0m\n     | > loss: 0.37945  (0.34955)\n     | > log_mle: -0.03506  (-0.03647)\n     | > loss_dur: 0.41451  (0.38602)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 2.31647  (2.27063)\n     | > current_lr: 0.00001 \n     | > step_time: 0.54600  (0.55391)\n     | > loader_time: 0.00360  (0.00460)\n\n\n\u001b[1m   --> STEP: 50/406 -- GLOBAL_STEP: 10200\u001b[0m\n     | > loss: 0.35457  (0.34983)\n     | > log_mle: -0.05985  (-0.03789)\n     | > loss_dur: 0.41442  (0.38772)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.92484  (2.29128)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84350  (0.58756)\n     | > loader_time: 0.00480  (0.00484)\n\n\n\u001b[1m   --> STEP: 75/406 -- GLOBAL_STEP: 10225\u001b[0m\n     | > loss: 0.32478  (0.34625)\n     | > log_mle: -0.05451  (-0.04176)\n     | > loss_dur: 0.37929  (0.38802)\n     | > amp_scaler: 65536.00000  (65536.00000)\n     | > grad_norm: 3.75010  (2.61845)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63610  (0.61351)\n     | > loader_time: 0.00450  (0.00538)\n\n\n\u001b[1m   --> STEP: 100/406 -- GLOBAL_STEP: 10250\u001b[0m\n     | > loss: 0.35350  (0.34544)\n     | > log_mle: -0.04695  (-0.04486)\n     | > loss_dur: 0.40045  (0.39030)\n     | > amp_scaler: 32768.00000  (60948.48000)\n     | > grad_norm: 1.94148  (2.63395)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73340  (0.63928)\n     | > loader_time: 0.00700  (0.00587)\n\n\n\u001b[1m   --> STEP: 125/406 -- GLOBAL_STEP: 10275\u001b[0m\n     | > loss: 0.31243  (0.34230)\n     | > log_mle: -0.07514  (-0.04819)\n     | > loss_dur: 0.38756  (0.39048)\n     | > amp_scaler: 32768.00000  (55312.38400)\n     | > grad_norm: 2.67853  (2.75841)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71690  (0.65933)\n     | > loader_time: 0.00500  (0.00591)\n\n\n\u001b[1m   --> STEP: 150/406 -- GLOBAL_STEP: 10300\u001b[0m\n     | > loss: 0.33604  (0.34095)\n     | > log_mle: -0.05824  (-0.05061)\n     | > loss_dur: 0.39428  (0.39156)\n     | > amp_scaler: 32768.00000  (51554.98667)\n     | > grad_norm: 2.65375  (2.83095)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81650  (0.68413)\n     | > loader_time: 0.00750  (0.00599)\n\n\n\u001b[1m   --> STEP: 175/406 -- GLOBAL_STEP: 10325\u001b[0m\n     | > loss: 0.34458  (0.34021)\n     | > log_mle: -0.06183  (-0.05251)\n     | > loss_dur: 0.40641  (0.39272)\n     | > amp_scaler: 32768.00000  (48871.13143)\n     | > grad_norm: 7.42151  (3.08079)\n     | > current_lr: 0.00001 \n     | > step_time: 1.13130  (0.70481)\n     | > loader_time: 0.00550  (0.00627)\n\n\n\u001b[1m   --> STEP: 200/406 -- GLOBAL_STEP: 10350\u001b[0m\n     | > loss: 0.31658  (0.33906)\n     | > log_mle: -0.06830  (-0.05443)\n     | > loss_dur: 0.38487  (0.39349)\n     | > amp_scaler: 32768.00000  (46858.24000)\n     | > grad_norm: 4.33925  (3.11067)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80770  (0.72544)\n     | > loader_time: 0.00860  (0.00647)\n\n\n\u001b[1m   --> STEP: 225/406 -- GLOBAL_STEP: 10375\u001b[0m\n     | > loss: 0.35028  (0.33787)\n     | > log_mle: -0.06782  (-0.05619)\n     | > loss_dur: 0.41810  (0.39407)\n     | > amp_scaler: 32768.00000  (45292.65778)\n     | > grad_norm: 1.81392  (3.16921)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93440  (0.75008)\n     | > loader_time: 0.00690  (0.00659)\n\n\n\u001b[1m   --> STEP: 250/406 -- GLOBAL_STEP: 10400\u001b[0m\n     | > loss: 0.32271  (0.33655)\n     | > log_mle: -0.08928  (-0.05787)\n     | > loss_dur: 0.41199  (0.39442)\n     | > amp_scaler: 32768.00000  (44040.19200)\n     | > grad_norm: 10.40061  (3.26841)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95840  (0.77419)\n     | > loader_time: 0.00610  (0.00683)\n\n\n\u001b[1m   --> STEP: 275/406 -- GLOBAL_STEP: 10425\u001b[0m\n     | > loss: 0.33822  (0.33490)\n     | > log_mle: -0.06571  (-0.05938)\n     | > loss_dur: 0.40393  (0.39428)\n     | > amp_scaler: 32768.00000  (43015.44727)\n     | > grad_norm: 3.73833  (3.31959)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98180  (0.79690)\n     | > loader_time: 0.02510  (0.00702)\n\n\n\u001b[1m   --> STEP: 300/406 -- GLOBAL_STEP: 10450\u001b[0m\n     | > loss: 0.35342  (0.33388)\n     | > log_mle: -0.06075  (-0.06057)\n     | > loss_dur: 0.41418  (0.39445)\n     | > amp_scaler: 32768.00000  (42161.49333)\n     | > grad_norm: 5.27281  (3.37250)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11560  (0.81823)\n     | > loader_time: 0.00790  (0.00710)\n\n\n\u001b[1m   --> STEP: 325/406 -- GLOBAL_STEP: 10475\u001b[0m\n     | > loss: 0.34103  (0.33297)\n     | > log_mle: -0.06872  (-0.06175)\n     | > loss_dur: 0.40976  (0.39472)\n     | > amp_scaler: 32768.00000  (41438.91692)\n     | > grad_norm: 5.23214  (3.55870)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08140  (0.84184)\n     | > loader_time: 0.00750  (0.00726)\n\n\n\u001b[1m   --> STEP: 350/406 -- GLOBAL_STEP: 10500\u001b[0m\n     | > loss: 0.29576  (0.33197)\n     | > log_mle: -0.08327  (-0.06298)\n     | > loss_dur: 0.37903  (0.39495)\n     | > amp_scaler: 32768.00000  (40819.56571)\n     | > grad_norm: 4.56080  (3.59409)\n     | > current_lr: 0.00001 \n     | > step_time: 1.07970  (0.86331)\n     | > loader_time: 0.01000  (0.00741)\n\n\n\u001b[1m   --> STEP: 375/406 -- GLOBAL_STEP: 10525\u001b[0m\n     | > loss: 0.30221  (0.33073)\n     | > log_mle: -0.09141  (-0.06422)\n     | > loss_dur: 0.39361  (0.39495)\n     | > amp_scaler: 32768.00000  (40282.79467)\n     | > grad_norm: 5.25354  (3.55946)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25590  (0.88666)\n     | > loader_time: 0.00760  (0.00759)\n\n\n\u001b[1m   --> STEP: 400/406 -- GLOBAL_STEP: 10550\u001b[0m\n     | > loss: 0.32276  (0.32964)\n     | > log_mle: -0.08743  (-0.06533)\n     | > loss_dur: 0.41018  (0.39498)\n     | > amp_scaler: 32768.00000  (39813.12000)\n     | > grad_norm: 2.23862  (3.58905)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92540  (0.90971)\n     | > loader_time: 0.00640  (0.00770)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00716 \u001b[0m(+0.00200)\n     | > avg_loss:\u001b[92m 0.36901 \u001b[0m(-0.02876)\n     | > avg_log_mle:\u001b[92m -0.00849 \u001b[0m(-0.00232)\n     | > avg_loss_dur:\u001b[92m 0.37750 \u001b[0m(-0.02644)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_10556.pth\n\n\u001b[4m\u001b[1m > EPOCH: 26/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:05:46) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 19/406 -- GLOBAL_STEP: 10575\u001b[0m\n     | > loss: 0.31295  (0.31274)\n     | > log_mle: -0.04100  (-0.05010)\n     | > loss_dur: 0.35395  (0.36284)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.57206  (2.15935)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71020  (0.56477)\n     | > loader_time: 0.00430  (0.00559)\n\n\n\u001b[1m   --> STEP: 44/406 -- GLOBAL_STEP: 10600\u001b[0m\n     | > loss: 0.32363  (0.31425)\n     | > log_mle: -0.05745  (-0.05039)\n     | > loss_dur: 0.38108  (0.36464)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.41761  (2.84217)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62880  (0.58536)\n     | > loader_time: 0.00430  (0.00535)\n\n\n\u001b[1m   --> STEP: 69/406 -- GLOBAL_STEP: 10625\u001b[0m\n     | > loss: 0.31153  (0.31390)\n     | > log_mle: -0.06432  (-0.05385)\n     | > loss_dur: 0.37584  (0.36775)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.62440  (3.23603)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61060  (0.61866)\n     | > loader_time: 0.00760  (0.00577)\n\n\n\u001b[1m   --> STEP: 94/406 -- GLOBAL_STEP: 10650\u001b[0m\n     | > loss: 0.30368  (0.31159)\n     | > log_mle: -0.07301  (-0.05764)\n     | > loss_dur: 0.37669  (0.36923)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.75997  (3.44606)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72620  (0.63577)\n     | > loader_time: 0.00460  (0.00611)\n\n\n\u001b[1m   --> STEP: 119/406 -- GLOBAL_STEP: 10675\u001b[0m\n     | > loss: 0.30099  (0.30890)\n     | > log_mle: -0.08396  (-0.06059)\n     | > loss_dur: 0.38494  (0.36949)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.84672  (3.80760)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68620  (0.65443)\n     | > loader_time: 0.00690  (0.00646)\n\n\n\u001b[1m   --> STEP: 144/406 -- GLOBAL_STEP: 10700\u001b[0m\n     | > loss: 0.29346  (0.30723)\n     | > log_mle: -0.07023  (-0.06332)\n     | > loss_dur: 0.36368  (0.37055)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.15155  (3.78331)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69270  (0.67052)\n     | > loader_time: 0.00550  (0.00672)\n\n\n\u001b[1m   --> STEP: 169/406 -- GLOBAL_STEP: 10725\u001b[0m\n     | > loss: 0.31781  (0.30623)\n     | > log_mle: -0.07502  (-0.06520)\n     | > loss_dur: 0.39283  (0.37144)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.17255  (3.96604)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79280  (0.68965)\n     | > loader_time: 0.00510  (0.00676)\n\n\n\u001b[1m   --> STEP: 194/406 -- GLOBAL_STEP: 10750\u001b[0m\n     | > loss: 0.30496  (0.30493)\n     | > log_mle: -0.08036  (-0.06725)\n     | > loss_dur: 0.38532  (0.37218)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.51328  (3.97274)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84020  (0.70877)\n     | > loader_time: 0.01430  (0.00682)\n\n\n\u001b[1m   --> STEP: 219/406 -- GLOBAL_STEP: 10775\u001b[0m\n     | > loss: 0.29241  (0.30375)\n     | > log_mle: -0.08428  (-0.06892)\n     | > loss_dur: 0.37669  (0.37267)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.91649  (3.86062)\n     | > current_lr: 0.00001 \n     | > step_time: 0.88700  (0.72994)\n     | > loader_time: 0.00680  (0.00685)\n\n\n\u001b[1m   --> STEP: 244/406 -- GLOBAL_STEP: 10800\u001b[0m\n     | > loss: 0.27645  (0.30295)\n     | > log_mle: -0.08277  (-0.07068)\n     | > loss_dur: 0.35922  (0.37363)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.05238  (3.89620)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95760  (0.75352)\n     | > loader_time: 0.00850  (0.00685)\n\n\n\u001b[1m   --> STEP: 269/406 -- GLOBAL_STEP: 10825\u001b[0m\n     | > loss: 0.27322  (0.30178)\n     | > log_mle: -0.08971  (-0.07200)\n     | > loss_dur: 0.36292  (0.37379)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.46136  (4.04459)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10140  (0.77554)\n     | > loader_time: 0.00760  (0.00702)\n\n\n\u001b[1m   --> STEP: 294/406 -- GLOBAL_STEP: 10850\u001b[0m\n     | > loss: 0.30869  (0.30088)\n     | > log_mle: -0.09307  (-0.07330)\n     | > loss_dur: 0.40176  (0.37418)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.27104  (4.10755)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21430  (0.79910)\n     | > loader_time: 0.00670  (0.00707)\n\n\n\u001b[1m   --> STEP: 319/406 -- GLOBAL_STEP: 10875\u001b[0m\n     | > loss: 0.29493  (0.30032)\n     | > log_mle: -0.09176  (-0.07448)\n     | > loss_dur: 0.38670  (0.37480)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.77417  (4.13858)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11940  (0.82229)\n     | > loader_time: 0.01410  (0.00733)\n\n\n\u001b[1m   --> STEP: 344/406 -- GLOBAL_STEP: 10900\u001b[0m\n     | > loss: 0.29579  (0.29965)\n     | > log_mle: -0.08405  (-0.07560)\n     | > loss_dur: 0.37985  (0.37525)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.66003  (4.06711)\n     | > current_lr: 0.00001 \n     | > step_time: 1.50210  (0.84910)\n     | > loader_time: 0.02710  (0.00772)\n\n\n\u001b[1m   --> STEP: 369/406 -- GLOBAL_STEP: 10925\u001b[0m\n     | > loss: 0.28543  (0.29853)\n     | > log_mle: -0.08383  (-0.07679)\n     | > loss_dur: 0.36927  (0.37532)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.40744  (4.11370)\n     | > current_lr: 0.00001 \n     | > step_time: 1.46770  (0.88090)\n     | > loader_time: 0.01550  (0.00840)\n\n\n\u001b[1m   --> STEP: 394/406 -- GLOBAL_STEP: 10950\u001b[0m\n     | > loss: 0.29169  (0.29760)\n     | > log_mle: -0.09167  (-0.07787)\n     | > loss_dur: 0.38336  (0.37547)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.22090  (4.08665)\n     | > current_lr: 0.00001 \n     | > step_time: 1.26660  (0.90845)\n     | > loader_time: 0.00870  (0.00877)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00537 \u001b[0m(-0.00179)\n     | > avg_loss:\u001b[92m 0.32715 \u001b[0m(-0.04185)\n     | > avg_log_mle:\u001b[92m -0.03278 \u001b[0m(-0.02429)\n     | > avg_loss_dur:\u001b[92m 0.35993 \u001b[0m(-0.01757)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_10962.pth\n\n\u001b[4m\u001b[1m > EPOCH: 27/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:12:28) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 13/406 -- GLOBAL_STEP: 10975\u001b[0m\n     | > loss: 0.24977  (0.28411)\n     | > log_mle: -0.07077  (-0.06198)\n     | > loss_dur: 0.32054  (0.34608)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.54759  (3.51838)\n     | > current_lr: 0.00001 \n     | > step_time: 0.53330  (0.55044)\n     | > loader_time: 0.00620  (0.00526)\n\n\n\u001b[1m   --> STEP: 38/406 -- GLOBAL_STEP: 11000\u001b[0m\n     | > loss: 0.29376  (0.28761)\n     | > log_mle: -0.07187  (-0.06254)\n     | > loss_dur: 0.36563  (0.35015)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.73899  (3.21989)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67980  (0.58721)\n     | > loader_time: 0.00760  (0.00576)\n\n\n\u001b[1m   --> STEP: 63/406 -- GLOBAL_STEP: 11025\u001b[0m\n     | > loss: 0.27805  (0.28725)\n     | > log_mle: -0.07807  (-0.06504)\n     | > loss_dur: 0.35612  (0.35228)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.81555  (3.67259)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63340  (0.60510)\n     | > loader_time: 0.00770  (0.00586)\n\n\n\u001b[1m   --> STEP: 88/406 -- GLOBAL_STEP: 11050\u001b[0m\n     | > loss: 0.27481  (0.28463)\n     | > log_mle: -0.06611  (-0.06855)\n     | > loss_dur: 0.34091  (0.35318)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.59894  (3.83461)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72300  (0.63334)\n     | > loader_time: 0.00910  (0.00647)\n\n\n\u001b[1m   --> STEP: 113/406 -- GLOBAL_STEP: 11075\u001b[0m\n     | > loss: 0.27984  (0.28181)\n     | > log_mle: -0.08667  (-0.07201)\n     | > loss_dur: 0.36651  (0.35382)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.96937  (4.00760)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71490  (0.64640)\n     | > loader_time: 0.00480  (0.00632)\n\n\n\u001b[1m   --> STEP: 138/406 -- GLOBAL_STEP: 11100\u001b[0m\n     | > loss: 0.29304  (0.28013)\n     | > log_mle: -0.09333  (-0.07468)\n     | > loss_dur: 0.38637  (0.35481)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.76848  (4.17821)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71890  (0.66854)\n     | > loader_time: 0.00800  (0.00644)\n\n\n\u001b[1m   --> STEP: 163/406 -- GLOBAL_STEP: 11125\u001b[0m\n     | > loss: 0.27251  (0.27900)\n     | > log_mle: -0.09635  (-0.07676)\n     | > loss_dur: 0.36886  (0.35576)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 9.72426  (4.02614)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72930  (0.68473)\n     | > loader_time: 0.00530  (0.00653)\n\n\n\u001b[1m   --> STEP: 188/406 -- GLOBAL_STEP: 11150\u001b[0m\n     | > loss: 0.26841  (0.27797)\n     | > log_mle: -0.10927  (-0.07872)\n     | > loss_dur: 0.37768  (0.35670)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.56833  (4.02826)\n     | > current_lr: 0.00001 \n     | > step_time: 0.77020  (0.70688)\n     | > loader_time: 0.00540  (0.00661)\n\n\n\u001b[1m   --> STEP: 213/406 -- GLOBAL_STEP: 11175\u001b[0m\n     | > loss: 0.25721  (0.27642)\n     | > log_mle: -0.09970  (-0.08040)\n     | > loss_dur: 0.35692  (0.35682)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.64785  (4.08247)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84720  (0.72947)\n     | > loader_time: 0.00730  (0.00673)\n\n\n\u001b[1m   --> STEP: 238/406 -- GLOBAL_STEP: 11200\u001b[0m\n     | > loss: 0.24577  (0.27580)\n     | > log_mle: -0.10067  (-0.08208)\n     | > loss_dur: 0.34645  (0.35788)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.00054  (4.02788)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96840  (0.75438)\n     | > loader_time: 0.00590  (0.00689)\n\n\n\u001b[1m   --> STEP: 263/406 -- GLOBAL_STEP: 11225\u001b[0m\n     | > loss: 0.26216  (0.27458)\n     | > log_mle: -0.10135  (-0.08362)\n     | > loss_dur: 0.36351  (0.35820)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.58740  (3.99373)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93240  (0.77584)\n     | > loader_time: 0.00920  (0.00700)\n\n\n\u001b[1m   --> STEP: 288/406 -- GLOBAL_STEP: 11250\u001b[0m\n     | > loss: 0.25977  (0.27333)\n     | > log_mle: -0.09451  (-0.08501)\n     | > loss_dur: 0.35428  (0.35833)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.98202  (3.98770)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09260  (0.80161)\n     | > loader_time: 0.00940  (0.00721)\n\n\n\u001b[1m   --> STEP: 313/406 -- GLOBAL_STEP: 11275\u001b[0m\n     | > loss: 0.27528  (0.27275)\n     | > log_mle: -0.09828  (-0.08617)\n     | > loss_dur: 0.37356  (0.35891)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.49466  (4.03138)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10470  (0.82610)\n     | > loader_time: 0.00960  (0.00736)\n\n\n\u001b[1m   --> STEP: 338/406 -- GLOBAL_STEP: 11300\u001b[0m\n     | > loss: 0.26305  (0.27220)\n     | > log_mle: -0.10190  (-0.08727)\n     | > loss_dur: 0.36495  (0.35947)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.27781  (4.17669)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17340  (0.85255)\n     | > loader_time: 0.00700  (0.00760)\n\n\n\u001b[1m   --> STEP: 363/406 -- GLOBAL_STEP: 11325\u001b[0m\n     | > loss: 0.24923  (0.27131)\n     | > log_mle: -0.11179  (-0.08848)\n     | > loss_dur: 0.36103  (0.35980)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.14681  (4.44711)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24020  (0.88156)\n     | > loader_time: 0.01550  (0.00806)\n\n\n\u001b[1m   --> STEP: 388/406 -- GLOBAL_STEP: 11350\u001b[0m\n     | > loss: 0.24896  (0.27055)\n     | > log_mle: -0.10347  (-0.08951)\n     | > loss_dur: 0.35244  (0.36005)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.69362  (4.40656)\n     | > current_lr: 0.00001 \n     | > step_time: 1.34150  (0.91201)\n     | > loader_time: 0.00970  (0.00850)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00640 \u001b[0m(+0.00103)\n     | > avg_loss:\u001b[92m 0.31352 \u001b[0m(-0.01363)\n     | > avg_log_mle:\u001b[91m -0.03103 \u001b[0m(+0.00175)\n     | > avg_loss_dur:\u001b[92m 0.34455 \u001b[0m(-0.01538)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_11368.pth\n\n\u001b[4m\u001b[1m > EPOCH: 28/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:19:16) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 7/406 -- GLOBAL_STEP: 11375\u001b[0m\n     | > loss: 0.25784  (0.25593)\n     | > log_mle: -0.06470  (-0.06986)\n     | > loss_dur: 0.32253  (0.32579)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.47369  (2.28236)\n     | > current_lr: 0.00001 \n     | > step_time: 0.58610  (0.55433)\n     | > loader_time: 0.00280  (0.00391)\n\n\n\u001b[1m   --> STEP: 32/406 -- GLOBAL_STEP: 11400\u001b[0m\n     | > loss: 0.24767  (0.25761)\n     | > log_mle: -0.07761  (-0.07430)\n     | > loss_dur: 0.32528  (0.33190)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.71918  (2.64444)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72080  (0.59077)\n     | > loader_time: 0.01330  (0.00517)\n\n\n\u001b[1m   --> STEP: 57/406 -- GLOBAL_STEP: 11425\u001b[0m\n     | > loss: 0.25888  (0.25866)\n     | > log_mle: -0.07575  (-0.07613)\n     | > loss_dur: 0.33463  (0.33479)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.36186  (3.11016)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61670  (0.62469)\n     | > loader_time: 0.00440  (0.00533)\n\n\n\u001b[1m   --> STEP: 82/406 -- GLOBAL_STEP: 11450\u001b[0m\n     | > loss: 0.25559  (0.25722)\n     | > log_mle: -0.09383  (-0.07924)\n     | > loss_dur: 0.34943  (0.33646)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.60515  (3.72092)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72930  (0.64591)\n     | > loader_time: 0.00750  (0.00577)\n\n\n\u001b[1m   --> STEP: 107/406 -- GLOBAL_STEP: 11475\u001b[0m\n     | > loss: 0.24255  (0.25564)\n     | > log_mle: -0.09149  (-0.08248)\n     | > loss_dur: 0.33404  (0.33812)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.66757  (3.98720)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70410  (0.66633)\n     | > loader_time: 0.00460  (0.00614)\n\n\n\u001b[1m   --> STEP: 132/406 -- GLOBAL_STEP: 11500\u001b[0m\n     | > loss: 0.23698  (0.25311)\n     | > log_mle: -0.11046  (-0.08548)\n     | > loss_dur: 0.34744  (0.33859)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.14635  (4.08652)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74020  (0.68713)\n     | > loader_time: 0.00500  (0.00628)\n\n\n\u001b[1m   --> STEP: 157/406 -- GLOBAL_STEP: 11525\u001b[0m\n     | > loss: 0.24155  (0.25239)\n     | > log_mle: -0.09881  (-0.08754)\n     | > loss_dur: 0.34036  (0.33993)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.28387  (3.99791)\n     | > current_lr: 0.00001 \n     | > step_time: 0.78620  (0.70392)\n     | > loader_time: 0.00790  (0.00645)\n\n\n\u001b[1m   --> STEP: 182/406 -- GLOBAL_STEP: 11550\u001b[0m\n     | > loss: 0.25242  (0.25154)\n     | > log_mle: -0.10526  (-0.08947)\n     | > loss_dur: 0.35768  (0.34101)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.27779  (4.00417)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81340  (0.72659)\n     | > loader_time: 0.00680  (0.00672)\n\n\n\u001b[1m   --> STEP: 207/406 -- GLOBAL_STEP: 11575\u001b[0m\n     | > loss: 0.23862  (0.25093)\n     | > log_mle: -0.09552  (-0.09113)\n     | > loss_dur: 0.33414  (0.34207)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.79254  (4.03966)\n     | > current_lr: 0.00001 \n     | > step_time: 1.00220  (0.75069)\n     | > loader_time: 0.00590  (0.00679)\n\n\n\u001b[1m   --> STEP: 232/406 -- GLOBAL_STEP: 11600\u001b[0m\n     | > loss: 0.22588  (0.25018)\n     | > log_mle: -0.11327  (-0.09294)\n     | > loss_dur: 0.33915  (0.34312)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.26797  (4.16155)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25900  (0.77565)\n     | > loader_time: 0.00640  (0.00680)\n\n\n\u001b[1m   --> STEP: 257/406 -- GLOBAL_STEP: 11625\u001b[0m\n     | > loss: 0.23593  (0.24912)\n     | > log_mle: -0.10917  (-0.09446)\n     | > loss_dur: 0.34510  (0.34359)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.93251  (4.30245)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12360  (0.79924)\n     | > loader_time: 0.01380  (0.00709)\n\n\n\u001b[1m   --> STEP: 282/406 -- GLOBAL_STEP: 11650\u001b[0m\n     | > loss: 0.23964  (0.24795)\n     | > log_mle: -0.11388  (-0.09586)\n     | > loss_dur: 0.35352  (0.34382)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.55914  (4.40201)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17650  (0.82438)\n     | > loader_time: 0.01480  (0.00723)\n\n\n\u001b[1m   --> STEP: 307/406 -- GLOBAL_STEP: 11675\u001b[0m\n     | > loss: 0.23268  (0.24776)\n     | > log_mle: -0.10667  (-0.09701)\n     | > loss_dur: 0.33935  (0.34477)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.32287  (4.50156)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23620  (0.85505)\n     | > loader_time: 0.01200  (0.00769)\n\n\n\u001b[1m   --> STEP: 332/406 -- GLOBAL_STEP: 11700\u001b[0m\n     | > loss: 0.22941  (0.24719)\n     | > log_mle: -0.10657  (-0.09804)\n     | > loss_dur: 0.33599  (0.34523)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.13466  (4.46374)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20550  (0.88233)\n     | > loader_time: 0.01500  (0.00821)\n\n\n\u001b[1m   --> STEP: 357/406 -- GLOBAL_STEP: 11725\u001b[0m\n     | > loss: 0.24133  (0.24649)\n     | > log_mle: -0.10479  (-0.09922)\n     | > loss_dur: 0.34612  (0.34571)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.00888  (4.57426)\n     | > current_lr: 0.00001 \n     | > step_time: 1.36760  (0.91204)\n     | > loader_time: 0.01710  (0.00878)\n\n\n\u001b[1m   --> STEP: 382/406 -- GLOBAL_STEP: 11750\u001b[0m\n     | > loss: 0.23365  (0.24571)\n     | > log_mle: -0.11334  (-0.10035)\n     | > loss_dur: 0.34699  (0.34606)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.58356  (4.70873)\n     | > current_lr: 0.00001 \n     | > step_time: 1.33060  (0.94112)\n     | > loader_time: 0.01820  (0.00927)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00706 \u001b[0m(+0.00066)\n     | > avg_loss:\u001b[92m 0.27327 \u001b[0m(-0.04025)\n     | > avg_log_mle:\u001b[92m -0.05516 \u001b[0m(-0.02413)\n     | > avg_loss_dur:\u001b[92m 0.32843 \u001b[0m(-0.01612)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_11774.pth\n\n\u001b[4m\u001b[1m > EPOCH: 29/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:26:21) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 1/406 -- GLOBAL_STEP: 11775\u001b[0m\n     | > loss: 0.20716  (0.20716)\n     | > log_mle: -0.08440  (-0.08440)\n     | > loss_dur: 0.29156  (0.29156)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.00021  (2.00021)\n     | > current_lr: 0.00001 \n     | > step_time: 0.55910  (0.55911)\n     | > loader_time: 0.00320  (0.00321)\n\n\n\u001b[1m   --> STEP: 26/406 -- GLOBAL_STEP: 11800\u001b[0m\n     | > loss: 0.23095  (0.23221)\n     | > log_mle: -0.08868  (-0.08452)\n     | > loss_dur: 0.31962  (0.31673)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.70786  (2.80353)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65810  (0.58293)\n     | > loader_time: 0.01020  (0.00517)\n\n\n\u001b[1m   --> STEP: 51/406 -- GLOBAL_STEP: 11825\u001b[0m\n     | > loss: 0.22202  (0.23504)\n     | > log_mle: -0.11169  (-0.08611)\n     | > loss_dur: 0.33371  (0.32114)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.20145  (2.93494)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61780  (0.59822)\n     | > loader_time: 0.00440  (0.00556)\n\n\n\u001b[1m   --> STEP: 76/406 -- GLOBAL_STEP: 11850\u001b[0m\n     | > loss: 0.22228  (0.23379)\n     | > log_mle: -0.08885  (-0.08924)\n     | > loss_dur: 0.31114  (0.32303)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.91731  (3.58275)\n     | > current_lr: 0.00001 \n     | > step_time: 0.85810  (0.62388)\n     | > loader_time: 0.00750  (0.00578)\n\n\n\u001b[1m   --> STEP: 101/406 -- GLOBAL_STEP: 11875\u001b[0m\n     | > loss: 0.24087  (0.23386)\n     | > log_mle: -0.10748  (-0.09230)\n     | > loss_dur: 0.34835  (0.32616)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.10263  (4.38315)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71380  (0.64506)\n     | > loader_time: 0.00680  (0.00587)\n\n\n\u001b[1m   --> STEP: 126/406 -- GLOBAL_STEP: 11900\u001b[0m\n     | > loss: 0.23998  (0.23131)\n     | > log_mle: -0.09843  (-0.09539)\n     | > loss_dur: 0.33840  (0.32670)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.40683  (4.60254)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80200  (0.66981)\n     | > loader_time: 0.00760  (0.00638)\n\n\n\u001b[1m   --> STEP: 151/406 -- GLOBAL_STEP: 11925\u001b[0m\n     | > loss: 0.23359  (0.23044)\n     | > log_mle: -0.10555  (-0.09780)\n     | > loss_dur: 0.33914  (0.32824)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.56314  (4.53179)\n     | > current_lr: 0.00001 \n     | > step_time: 0.77960  (0.69092)\n     | > loader_time: 0.00750  (0.00668)\n\n\n\u001b[1m   --> STEP: 176/406 -- GLOBAL_STEP: 11950\u001b[0m\n     | > loss: 0.22766  (0.22983)\n     | > log_mle: -0.10689  (-0.09965)\n     | > loss_dur: 0.33455  (0.32948)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.55982  (4.40596)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82830  (0.71690)\n     | > loader_time: 0.00820  (0.00674)\n\n\n\u001b[1m   --> STEP: 201/406 -- GLOBAL_STEP: 11975\u001b[0m\n     | > loss: 0.22018  (0.22913)\n     | > log_mle: -0.10893  (-0.10149)\n     | > loss_dur: 0.32910  (0.33063)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.59734  (4.52572)\n     | > current_lr: 0.00001 \n     | > step_time: 0.88530  (0.74274)\n     | > loader_time: 0.00620  (0.00678)\n\n\n\u001b[1m   --> STEP: 226/406 -- GLOBAL_STEP: 12000\u001b[0m\n     | > loss: 0.20551  (0.22821)\n     | > log_mle: -0.12448  (-0.10326)\n     | > loss_dur: 0.32999  (0.33147)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.62539  (4.64803)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99890  (0.76827)\n     | > loader_time: 0.00850  (0.00691)\n\n\n\u001b[1m   --> STEP: 251/406 -- GLOBAL_STEP: 12025\u001b[0m\n     | > loss: 0.22682  (0.22763)\n     | > log_mle: -0.11335  (-0.10480)\n     | > loss_dur: 0.34017  (0.33243)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.42281  (4.80086)\n     | > current_lr: 0.00001 \n     | > step_time: 1.03710  (0.79125)\n     | > loader_time: 0.00730  (0.00703)\n\n\n\u001b[1m   --> STEP: 276/406 -- GLOBAL_STEP: 12050\u001b[0m\n     | > loss: 0.21015  (0.22665)\n     | > log_mle: -0.11937  (-0.10621)\n     | > loss_dur: 0.32951  (0.33286)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.29355  (4.82380)\n     | > current_lr: 0.00001 \n     | > step_time: 1.03860  (0.81689)\n     | > loader_time: 0.01470  (0.00735)\n\n\n\u001b[1m   --> STEP: 301/406 -- GLOBAL_STEP: 12075\u001b[0m\n     | > loss: 0.21441  (0.22643)\n     | > log_mle: -0.13373  (-0.10733)\n     | > loss_dur: 0.34815  (0.33377)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.20560  (4.86406)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09720  (0.84586)\n     | > loader_time: 0.00930  (0.00784)\n\n\n\u001b[1m   --> STEP: 326/406 -- GLOBAL_STEP: 12100\u001b[0m\n     | > loss: 0.20718  (0.22599)\n     | > log_mle: -0.11657  (-0.10834)\n     | > loss_dur: 0.32375  (0.33433)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.19029  (4.97855)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20910  (0.87222)\n     | > loader_time: 0.01560  (0.00834)\n\n\n\u001b[1m   --> STEP: 351/406 -- GLOBAL_STEP: 12125\u001b[0m\n     | > loss: 0.24658  (0.22574)\n     | > log_mle: -0.11727  (-0.10941)\n     | > loss_dur: 0.36385  (0.33515)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.21503  (5.24479)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32570  (0.90200)\n     | > loader_time: 0.01690  (0.00886)\n\n\n\u001b[1m   --> STEP: 376/406 -- GLOBAL_STEP: 12150\u001b[0m\n     | > loss: 0.23142  (0.22502)\n     | > log_mle: -0.12062  (-0.11053)\n     | > loss_dur: 0.35205  (0.33555)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.83096  (5.33721)\n     | > current_lr: 0.00001 \n     | > step_time: 1.39250  (0.93025)\n     | > loader_time: 0.01510  (0.00932)\n\n\n\u001b[1m   --> STEP: 401/406 -- GLOBAL_STEP: 12175\u001b[0m\n     | > loss: 0.22106  (0.22455)\n     | > log_mle: -0.12633  (-0.11151)\n     | > loss_dur: 0.34738  (0.33606)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.30532  (5.41251)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01350  (0.95361)\n     | > loader_time: 0.00640  (0.00960)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00618 \u001b[0m(-0.00089)\n     | > avg_loss:\u001b[92m 0.25126 \u001b[0m(-0.02201)\n     | > avg_log_mle:\u001b[92m -0.06288 \u001b[0m(-0.00772)\n     | > avg_loss_dur:\u001b[92m 0.31414 \u001b[0m(-0.01429)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_12180.pth\n\n\u001b[4m\u001b[1m > EPOCH: 30/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:33:21) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 20/406 -- GLOBAL_STEP: 12200\u001b[0m\n     | > loss: 0.22047  (0.20912)\n     | > log_mle: -0.07836  (-0.09327)\n     | > loss_dur: 0.29883  (0.30239)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 2.51097  (2.73444)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62990  (0.58587)\n     | > loader_time: 0.00650  (0.00478)\n\n\n\u001b[1m   --> STEP: 45/406 -- GLOBAL_STEP: 12225\u001b[0m\n     | > loss: 0.23027  (0.21425)\n     | > log_mle: -0.08942  (-0.09382)\n     | > loss_dur: 0.31968  (0.30807)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 1.83997  (2.98577)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74650  (0.60934)\n     | > loader_time: 0.00400  (0.00529)\n\n\n\u001b[1m   --> STEP: 70/406 -- GLOBAL_STEP: 12250\u001b[0m\n     | > loss: 0.18882  (0.21352)\n     | > log_mle: -0.11588  (-0.09756)\n     | > loss_dur: 0.30470  (0.31108)\n     | > amp_scaler: 65536.00000  (39321.60000)\n     | > grad_norm: 3.68468  (3.42603)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71840  (0.62979)\n     | > loader_time: 0.00780  (0.00581)\n\n\n\u001b[1m   --> STEP: 95/406 -- GLOBAL_STEP: 12275\u001b[0m\n     | > loss: 0.20704  (0.21197)\n     | > log_mle: -0.10890  (-0.10121)\n     | > loss_dur: 0.31594  (0.31318)\n     | > amp_scaler: 65536.00000  (46220.12632)\n     | > grad_norm: 3.22209  (3.59511)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73840  (0.65145)\n     | > loader_time: 0.00480  (0.00608)\n\n\n\u001b[1m   --> STEP: 120/406 -- GLOBAL_STEP: 12300\u001b[0m\n     | > loss: 0.19920  (0.21018)\n     | > log_mle: -0.12273  (-0.10424)\n     | > loss_dur: 0.32193  (0.31442)\n     | > amp_scaler: 65536.00000  (50244.26667)\n     | > grad_norm: 3.24661  (3.74843)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74850  (0.67920)\n     | > loader_time: 0.00520  (0.00645)\n\n\n\u001b[1m   --> STEP: 145/406 -- GLOBAL_STEP: 12325\u001b[0m\n     | > loss: 0.18813  (0.20927)\n     | > log_mle: -0.11187  (-0.10678)\n     | > loss_dur: 0.30000  (0.31604)\n     | > amp_scaler: 65536.00000  (52880.77241)\n     | > grad_norm: 4.10077  (3.95555)\n     | > current_lr: 0.00001 \n     | > step_time: 0.83150  (0.70446)\n     | > loader_time: 0.00960  (0.00661)\n\n\n\u001b[1m   --> STEP: 170/406 -- GLOBAL_STEP: 12350\u001b[0m\n     | > loss: 0.18477  (0.20910)\n     | > log_mle: -0.12079  (-0.10870)\n     | > loss_dur: 0.30556  (0.31780)\n     | > amp_scaler: 65536.00000  (54741.83529)\n     | > grad_norm: 7.82408  (4.19152)\n     | > current_lr: 0.00001 \n     | > step_time: 0.89510  (0.72489)\n     | > loader_time: 0.00600  (0.00661)\n\n\n\u001b[1m   --> STEP: 195/406 -- GLOBAL_STEP: 12375\u001b[0m\n     | > loss: 0.21600  (0.20869)\n     | > log_mle: -0.10799  (-0.11062)\n     | > loss_dur: 0.32399  (0.31932)\n     | > amp_scaler: 32768.00000  (55789.62051)\n     | > grad_norm: 6.87549  (4.35439)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90150  (0.74821)\n     | > loader_time: 0.00710  (0.00670)\n\n\n\u001b[1m   --> STEP: 220/406 -- GLOBAL_STEP: 12400\u001b[0m\n     | > loss: 0.20124  (0.20805)\n     | > log_mle: -0.14633  (-0.11236)\n     | > loss_dur: 0.34757  (0.32041)\n     | > amp_scaler: 32768.00000  (53173.52727)\n     | > grad_norm: 12.27712  (4.77652)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93220  (0.77105)\n     | > loader_time: 0.00840  (0.00681)\n\n\n\u001b[1m   --> STEP: 245/406 -- GLOBAL_STEP: 12425\u001b[0m\n     | > loss: 0.21351  (0.20742)\n     | > log_mle: -0.12323  (-0.11395)\n     | > loss_dur: 0.33674  (0.32137)\n     | > amp_scaler: 32768.00000  (51091.33061)\n     | > grad_norm: 4.86858  (4.93838)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11690  (0.79347)\n     | > loader_time: 0.00590  (0.00681)\n\n\n\u001b[1m   --> STEP: 270/406 -- GLOBAL_STEP: 12450\u001b[0m\n     | > loss: 0.20981  (0.20679)\n     | > log_mle: -0.13008  (-0.11528)\n     | > loss_dur: 0.33989  (0.32207)\n     | > amp_scaler: 32768.00000  (49394.72593)\n     | > grad_norm: 4.18727  (4.98667)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10530  (0.81920)\n     | > loader_time: 0.00640  (0.00688)\n\n\n\u001b[1m   --> STEP: 295/406 -- GLOBAL_STEP: 12475\u001b[0m\n     | > loss: 0.20493  (0.20626)\n     | > log_mle: -0.13156  (-0.11653)\n     | > loss_dur: 0.33650  (0.32279)\n     | > amp_scaler: 32768.00000  (47985.68136)\n     | > grad_norm: 8.00335  (5.12974)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02050  (0.84413)\n     | > loader_time: 0.00870  (0.00699)\n\n\n\u001b[1m   --> STEP: 320/406 -- GLOBAL_STEP: 12500\u001b[0m\n     | > loss: 0.21966  (0.20617)\n     | > log_mle: -0.12066  (-0.11759)\n     | > loss_dur: 0.34033  (0.32376)\n     | > amp_scaler: 32768.00000  (46796.80000)\n     | > grad_norm: 7.66891  (5.28947)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25310  (0.86901)\n     | > loader_time: 0.01660  (0.00727)\n\n\n\u001b[1m   --> STEP: 345/406 -- GLOBAL_STEP: 12525\u001b[0m\n     | > loss: 0.18862  (0.20593)\n     | > log_mle: -0.14177  (-0.11865)\n     | > loss_dur: 0.33039  (0.32458)\n     | > amp_scaler: 32768.00000  (45780.22029)\n     | > grad_norm: 10.22497  (5.39168)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25420  (0.89785)\n     | > loader_time: 0.01700  (0.00773)\n\n\n\u001b[1m   --> STEP: 370/406 -- GLOBAL_STEP: 12550\u001b[0m\n     | > loss: 0.19016  (0.20549)\n     | > log_mle: -0.14765  (-0.11974)\n     | > loss_dur: 0.33781  (0.32523)\n     | > amp_scaler: 32768.00000  (44901.01622)\n     | > grad_norm: 7.90316  (5.58255)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21510  (0.92810)\n     | > loader_time: 0.00700  (0.00829)\n\n\n\u001b[1m   --> STEP: 395/406 -- GLOBAL_STEP: 12575\u001b[0m\n     | > loss: 0.20367  (0.20519)\n     | > log_mle: -0.14308  (-0.12070)\n     | > loss_dur: 0.34675  (0.32589)\n     | > amp_scaler: 32768.00000  (44133.10380)\n     | > grad_norm: 8.98898  (5.74625)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40170  (0.95686)\n     | > loader_time: 0.00950  (0.00874)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00595 \u001b[0m(-0.00022)\n     | > avg_loss:\u001b[92m 0.22437 \u001b[0m(-0.02689)\n     | > avg_log_mle:\u001b[92m -0.07787 \u001b[0m(-0.01499)\n     | > avg_loss_dur:\u001b[92m 0.30224 \u001b[0m(-0.01190)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_12586.pth\n\n\u001b[4m\u001b[1m > EPOCH: 31/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:40:24) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 14/406 -- GLOBAL_STEP: 12600\u001b[0m\n     | > loss: 0.19162  (0.18671)\n     | > log_mle: -0.11880  (-0.10435)\n     | > loss_dur: 0.31042  (0.29106)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 10.20840  (4.42487)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63290  (0.55429)\n     | > loader_time: 0.00370  (0.00446)\n\n\n\u001b[1m   --> STEP: 39/406 -- GLOBAL_STEP: 12625\u001b[0m\n     | > loss: 0.23358  (0.19318)\n     | > log_mle: -0.10238  (-0.10331)\n     | > loss_dur: 0.33596  (0.29650)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.36637  (3.92713)\n     | > current_lr: 0.00001 \n     | > step_time: 0.57820  (0.58561)\n     | > loader_time: 0.00410  (0.00576)\n\n\n\u001b[1m   --> STEP: 64/406 -- GLOBAL_STEP: 12650\u001b[0m\n     | > loss: 0.20357  (0.19463)\n     | > log_mle: -0.10519  (-0.10566)\n     | > loss_dur: 0.30876  (0.30029)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.59184  (4.59581)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73360  (0.62714)\n     | > loader_time: 0.00500  (0.00623)\n\n\n\u001b[1m   --> STEP: 89/406 -- GLOBAL_STEP: 12675\u001b[0m\n     | > loss: 0.19129  (0.19394)\n     | > log_mle: -0.11220  (-0.10911)\n     | > loss_dur: 0.30349  (0.30305)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.74515  (4.97146)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68050  (0.64645)\n     | > loader_time: 0.00600  (0.00618)\n\n\n\u001b[1m   --> STEP: 114/406 -- GLOBAL_STEP: 12700\u001b[0m\n     | > loss: 0.17195  (0.19267)\n     | > log_mle: -0.13884  (-0.11266)\n     | > loss_dur: 0.31078  (0.30533)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 16.28813  (5.86253)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71760  (0.67308)\n     | > loader_time: 0.00600  (0.00630)\n\n\n\u001b[1m   --> STEP: 139/406 -- GLOBAL_STEP: 12725\u001b[0m\n     | > loss: 0.18874  (0.19230)\n     | > log_mle: -0.12727  (-0.11508)\n     | > loss_dur: 0.31601  (0.30738)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 10.47061  (6.22805)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69840  (0.69298)\n     | > loader_time: 0.00790  (0.00649)\n\n\n\u001b[1m   --> STEP: 164/406 -- GLOBAL_STEP: 12750\u001b[0m\n     | > loss: 0.19980  (0.19186)\n     | > log_mle: -0.11959  (-0.11702)\n     | > loss_dur: 0.31939  (0.30888)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 4.66497  (6.21154)\n     | > current_lr: 0.00001 \n     | > step_time: 1.00370  (0.71377)\n     | > loader_time: 0.00700  (0.00658)\n\n\n\u001b[1m   --> STEP: 189/406 -- GLOBAL_STEP: 12775\u001b[0m\n     | > loss: 0.18881  (0.19153)\n     | > log_mle: -0.13238  (-0.11899)\n     | > loss_dur: 0.32119  (0.31052)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.15436  (6.14146)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94100  (0.73869)\n     | > loader_time: 0.00580  (0.00680)\n\n\n\u001b[1m   --> STEP: 214/406 -- GLOBAL_STEP: 12800\u001b[0m\n     | > loss: 0.19740  (0.19099)\n     | > log_mle: -0.13490  (-0.12057)\n     | > loss_dur: 0.33231  (0.31156)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.44796  (6.09993)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01240  (0.76413)\n     | > loader_time: 0.00570  (0.00699)\n\n\n\u001b[1m   --> STEP: 239/406 -- GLOBAL_STEP: 12825\u001b[0m\n     | > loss: 0.18688  (0.19074)\n     | > log_mle: -0.14147  (-0.12221)\n     | > loss_dur: 0.32835  (0.31295)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.05496  (6.18649)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97960  (0.78777)\n     | > loader_time: 0.00620  (0.00711)\n\n\n\u001b[1m   --> STEP: 264/406 -- GLOBAL_STEP: 12850\u001b[0m\n     | > loss: 0.19564  (0.19020)\n     | > log_mle: -0.13217  (-0.12362)\n     | > loss_dur: 0.32781  (0.31382)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.25566  (6.19665)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20880  (0.81037)\n     | > loader_time: 0.00630  (0.00718)\n\n\n\u001b[1m   --> STEP: 289/406 -- GLOBAL_STEP: 12875\u001b[0m\n     | > loss: 0.17311  (0.18947)\n     | > log_mle: -0.13271  (-0.12491)\n     | > loss_dur: 0.30583  (0.31438)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.85467  (6.28378)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99340  (0.83280)\n     | > loader_time: 0.00970  (0.00728)\n\n\n\u001b[1m   --> STEP: 314/406 -- GLOBAL_STEP: 12900\u001b[0m\n     | > loss: 0.18321  (0.18935)\n     | > log_mle: -0.14387  (-0.12600)\n     | > loss_dur: 0.32708  (0.31535)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 13.32412  (6.34091)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15950  (0.85871)\n     | > loader_time: 0.01640  (0.00749)\n\n\n\u001b[1m   --> STEP: 339/406 -- GLOBAL_STEP: 12925\u001b[0m\n     | > loss: 0.18383  (0.18925)\n     | > log_mle: -0.14099  (-0.12697)\n     | > loss_dur: 0.32482  (0.31622)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 11.77887  (6.50022)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31650  (0.88473)\n     | > loader_time: 0.01690  (0.00788)\n\n\n\u001b[1m   --> STEP: 364/406 -- GLOBAL_STEP: 12950\u001b[0m\n     | > loss: 0.19424  (0.18870)\n     | > log_mle: -0.14245  (-0.12808)\n     | > loss_dur: 0.33669  (0.31678)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.99607  (6.72252)\n     | > current_lr: 0.00001 \n     | > step_time: 1.44440  (0.91547)\n     | > loader_time: 0.01510  (0.00850)\n\n\n\u001b[1m   --> STEP: 389/406 -- GLOBAL_STEP: 12975\u001b[0m\n     | > loss: 0.17920  (0.18837)\n     | > log_mle: -0.15475  (-0.12903)\n     | > loss_dur: 0.33395  (0.31740)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.10027  (6.86144)\n     | > current_lr: 0.00001 \n     | > step_time: 1.45980  (0.94535)\n     | > loader_time: 0.01060  (0.00888)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00750 \u001b[0m(+0.00154)\n     | > avg_loss:\u001b[92m 0.22169 \u001b[0m(-0.00268)\n     | > avg_log_mle:\u001b[91m -0.07563 \u001b[0m(+0.00224)\n     | > avg_loss_dur:\u001b[92m 0.29732 \u001b[0m(-0.00491)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_12992.pth\n\n\u001b[4m\u001b[1m > EPOCH: 32/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:47:25) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 8/406 -- GLOBAL_STEP: 13000\u001b[0m\n     | > loss: 0.12591  (0.16488)\n     | > log_mle: -0.12565  (-0.11021)\n     | > loss_dur: 0.25156  (0.27510)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 11.67150  (6.27882)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64190  (0.53621)\n     | > loader_time: 0.00380  (0.00423)\n\n\n\u001b[1m   --> STEP: 33/406 -- GLOBAL_STEP: 13025\u001b[0m\n     | > loss: 0.19266  (0.17627)\n     | > log_mle: -0.10709  (-0.11150)\n     | > loss_dur: 0.29975  (0.28778)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.42291  (5.01898)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56130  (0.57995)\n     | > loader_time: 0.00420  (0.00476)\n\n\n\u001b[1m   --> STEP: 58/406 -- GLOBAL_STEP: 13050\u001b[0m\n     | > loss: 0.17095  (0.17857)\n     | > log_mle: -0.11784  (-0.11340)\n     | > loss_dur: 0.28880  (0.29196)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 8.47486  (5.00893)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65910  (0.62268)\n     | > loader_time: 0.00430  (0.00507)\n\n\n\u001b[1m   --> STEP: 83/406 -- GLOBAL_STEP: 13075\u001b[0m\n     | > loss: 0.18983  (0.17805)\n     | > log_mle: -0.12732  (-0.11657)\n     | > loss_dur: 0.31716  (0.29462)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 3.60449  (5.29210)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67950  (0.65145)\n     | > loader_time: 0.00580  (0.00591)\n\n\n\u001b[1m   --> STEP: 108/406 -- GLOBAL_STEP: 13100\u001b[0m\n     | > loss: 0.16661  (0.17679)\n     | > log_mle: -0.12956  (-0.11980)\n     | > loss_dur: 0.29617  (0.29659)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 6.88715  (5.78029)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67830  (0.66793)\n     | > loader_time: 0.00650  (0.00679)\n\n\n\u001b[1m   --> STEP: 133/406 -- GLOBAL_STEP: 13125\u001b[0m\n     | > loss: 0.17034  (0.17554)\n     | > log_mle: -0.13601  (-0.12278)\n     | > loss_dur: 0.30634  (0.29832)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.40978  (5.84174)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71510  (0.69303)\n     | > loader_time: 0.00500  (0.00700)\n\n\n\u001b[1m   --> STEP: 158/406 -- GLOBAL_STEP: 13150\u001b[0m\n     | > loss: 0.16771  (0.17596)\n     | > log_mle: -0.14389  (-0.12478)\n     | > loss_dur: 0.31160  (0.30074)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 9.92592  (6.02674)\n     | > current_lr: 0.00001 \n     | > step_time: 0.87640  (0.71065)\n     | > loader_time: 0.00800  (0.00699)\n\n\n\u001b[1m   --> STEP: 183/406 -- GLOBAL_STEP: 13175\u001b[0m\n     | > loss: 0.14589  (0.17554)\n     | > log_mle: -0.14160  (-0.12657)\n     | > loss_dur: 0.28749  (0.30211)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 9.96671  (6.29500)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97170  (0.73052)\n     | > loader_time: 0.00490  (0.00720)\n\n\n\u001b[1m   --> STEP: 208/406 -- GLOBAL_STEP: 13200\u001b[0m\n     | > loss: 0.15648  (0.17580)\n     | > log_mle: -0.14774  (-0.12820)\n     | > loss_dur: 0.30422  (0.30400)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 7.99080  (6.36301)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98800  (0.75222)\n     | > loader_time: 0.01700  (0.00727)\n\n\n\u001b[1m   --> STEP: 233/406 -- GLOBAL_STEP: 13225\u001b[0m\n     | > loss: 0.17846  (0.17543)\n     | > log_mle: -0.14592  (-0.12994)\n     | > loss_dur: 0.32438  (0.30536)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.08333  (6.43088)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01130  (0.77753)\n     | > loader_time: 0.00880  (0.00738)\n\n\n\u001b[1m   --> STEP: 258/406 -- GLOBAL_STEP: 13250\u001b[0m\n     | > loss: 0.17669  (0.17477)\n     | > log_mle: -0.14265  (-0.13138)\n     | > loss_dur: 0.31934  (0.30614)\n     | > amp_scaler: 32768.00000  (32768.00000)\n     | > grad_norm: 5.75766  (6.38227)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99130  (0.80036)\n     | > loader_time: 0.00670  (0.00747)\n\n\n\u001b[1m   --> STEP: 283/406 -- GLOBAL_STEP: 13275\u001b[0m\n     | > loss: 0.16807  (0.17422)\n     | > log_mle: -0.14582  (-0.13273)\n     | > loss_dur: 0.31389  (0.30694)\n     | > amp_scaler: 16384.00000  (32536.42403)\n     | > grad_norm: 20.22711  (6.59863)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11590  (0.82520)\n     | > loader_time: 0.00980  (0.00783)\n\n\n\u001b[1m   --> STEP: 308/406 -- GLOBAL_STEP: 13300\u001b[0m\n     | > loss: 0.14983  (0.17424)\n     | > log_mle: -0.15001  (-0.13380)\n     | > loss_dur: 0.29984  (0.30804)\n     | > amp_scaler: 16384.00000  (31225.35065)\n     | > grad_norm: 22.17090  (7.33264)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11890  (0.85424)\n     | > loader_time: 0.00830  (0.00820)\n\n\n\u001b[1m   --> STEP: 333/406 -- GLOBAL_STEP: 13325\u001b[0m\n     | > loss: 0.15533  (0.17413)\n     | > log_mle: -0.15405  (-0.13473)\n     | > loss_dur: 0.30937  (0.30886)\n     | > amp_scaler: 16384.00000  (30111.13514)\n     | > grad_norm: 10.21254  (7.69034)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17440  (0.88319)\n     | > loader_time: 0.01460  (0.00861)\n\n\n\u001b[1m   --> STEP: 358/406 -- GLOBAL_STEP: 13350\u001b[0m\n     | > loss: 0.13898  (0.17379)\n     | > log_mle: -0.16534  (-0.13581)\n     | > loss_dur: 0.30433  (0.30959)\n     | > amp_scaler: 16384.00000  (29152.53631)\n     | > grad_norm: 9.99741  (7.72754)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18100  (0.91267)\n     | > loader_time: 0.01420  (0.00912)\n\n\n\u001b[1m   --> STEP: 383/406 -- GLOBAL_STEP: 13375\u001b[0m\n     | > loss: 0.18273  (0.17343)\n     | > log_mle: -0.14912  (-0.13673)\n     | > loss_dur: 0.33185  (0.31016)\n     | > amp_scaler: 16384.00000  (28319.08094)\n     | > grad_norm: 9.66686  (7.90871)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32160  (0.94216)\n     | > loader_time: 0.00760  (0.00941)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00575 \u001b[0m(-0.00175)\n     | > avg_loss:\u001b[92m 0.22100 \u001b[0m(-0.00069)\n     | > avg_log_mle:\u001b[91m -0.07249 \u001b[0m(+0.00314)\n     | > avg_loss_dur:\u001b[92m 0.29349 \u001b[0m(-0.00383)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_13398.pth\n\n\u001b[4m\u001b[1m > EPOCH: 33/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 17:54:30) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 2/406 -- GLOBAL_STEP: 13400\u001b[0m\n     | > loss: 0.12635  (0.14409)\n     | > log_mle: -0.13306  (-0.12575)\n     | > loss_dur: 0.25941  (0.26985)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.60697  (3.33825)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71880  (0.65923)\n     | > loader_time: 0.00280  (0.00372)\n\n\n\u001b[1m   --> STEP: 27/406 -- GLOBAL_STEP: 13425\u001b[0m\n     | > loss: 0.14793  (0.15807)\n     | > log_mle: -0.11847  (-0.11896)\n     | > loss_dur: 0.26639  (0.27703)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.38726  (4.35475)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60310  (0.60663)\n     | > loader_time: 0.00820  (0.00470)\n\n\n\u001b[1m   --> STEP: 52/406 -- GLOBAL_STEP: 13450\u001b[0m\n     | > loss: 0.17000  (0.16363)\n     | > log_mle: -0.12221  (-0.12036)\n     | > loss_dur: 0.29221  (0.28399)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.14766  (5.11726)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63090  (0.62078)\n     | > loader_time: 0.00580  (0.00537)\n\n\n\u001b[1m   --> STEP: 77/406 -- GLOBAL_STEP: 13475\u001b[0m\n     | > loss: 0.16687  (0.16342)\n     | > log_mle: -0.12385  (-0.12336)\n     | > loss_dur: 0.29072  (0.28678)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.74077  (5.53008)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66800  (0.64316)\n     | > loader_time: 0.00560  (0.00569)\n\n\n\u001b[1m   --> STEP: 102/406 -- GLOBAL_STEP: 13500\u001b[0m\n     | > loss: 0.13282  (0.16254)\n     | > log_mle: -0.14686  (-0.12663)\n     | > loss_dur: 0.27968  (0.28917)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 13.70916  (5.98752)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74870  (0.66228)\n     | > loader_time: 0.00540  (0.00607)\n\n\n\u001b[1m   --> STEP: 127/406 -- GLOBAL_STEP: 13525\u001b[0m\n     | > loss: 0.17103  (0.16103)\n     | > log_mle: -0.14008  (-0.12952)\n     | > loss_dur: 0.31111  (0.29054)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.40561  (6.23048)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82430  (0.68794)\n     | > loader_time: 0.00640  (0.00625)\n\n\n\u001b[1m   --> STEP: 152/406 -- GLOBAL_STEP: 13550\u001b[0m\n     | > loss: 0.18300  (0.16139)\n     | > log_mle: -0.13947  (-0.13173)\n     | > loss_dur: 0.32247  (0.29313)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 15.20543  (6.68964)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74360  (0.70882)\n     | > loader_time: 0.00770  (0.00650)\n\n\n\u001b[1m   --> STEP: 177/406 -- GLOBAL_STEP: 13575\u001b[0m\n     | > loss: 0.13531  (0.16119)\n     | > log_mle: -0.15642  (-0.13349)\n     | > loss_dur: 0.29172  (0.29467)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.05838  (7.02470)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81920  (0.73005)\n     | > loader_time: 0.00640  (0.00661)\n\n\n\u001b[1m   --> STEP: 202/406 -- GLOBAL_STEP: 13600\u001b[0m\n     | > loss: 0.19432  (0.16145)\n     | > log_mle: -0.13973  (-0.13513)\n     | > loss_dur: 0.33405  (0.29658)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.73022  (7.44086)\n     | > current_lr: 0.00001 \n     | > step_time: 0.86810  (0.75335)\n     | > loader_time: 0.00820  (0.00670)\n\n\n\u001b[1m   --> STEP: 227/406 -- GLOBAL_STEP: 13625\u001b[0m\n     | > loss: 0.18338  (0.16104)\n     | > log_mle: -0.13719  (-0.13674)\n     | > loss_dur: 0.32057  (0.29778)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.88535  (7.66367)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99450  (0.78092)\n     | > loader_time: 0.00630  (0.00687)\n\n\n\u001b[1m   --> STEP: 252/406 -- GLOBAL_STEP: 13650\u001b[0m\n     | > loss: 0.16712  (0.16077)\n     | > log_mle: -0.15012  (-0.13818)\n     | > loss_dur: 0.31724  (0.29895)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.88030  (7.76101)\n     | > current_lr: 0.00001 \n     | > step_time: 1.30120  (0.81055)\n     | > loader_time: 0.00660  (0.00700)\n\n\n\u001b[1m   --> STEP: 277/406 -- GLOBAL_STEP: 13675\u001b[0m\n     | > loss: 0.14892  (0.16041)\n     | > log_mle: -0.15018  (-0.13945)\n     | > loss_dur: 0.29910  (0.29986)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.89725  (7.91594)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16270  (0.83822)\n     | > loader_time: 0.01500  (0.00746)\n\n\n\u001b[1m   --> STEP: 302/406 -- GLOBAL_STEP: 13700\u001b[0m\n     | > loss: 0.15781  (0.16026)\n     | > log_mle: -0.15845  (-0.14057)\n     | > loss_dur: 0.31625  (0.30083)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.48148  (7.96857)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20030  (0.86765)\n     | > loader_time: 0.01330  (0.00787)\n\n\n\u001b[1m   --> STEP: 327/406 -- GLOBAL_STEP: 13725\u001b[0m\n     | > loss: 0.15784  (0.16011)\n     | > log_mle: -0.15394  (-0.14149)\n     | > loss_dur: 0.31178  (0.30161)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.43747  (7.89820)\n     | > current_lr: 0.00001 \n     | > step_time: 1.36070  (0.89642)\n     | > loader_time: 0.03450  (0.00841)\n\n\n\u001b[1m   --> STEP: 352/406 -- GLOBAL_STEP: 13750\u001b[0m\n     | > loss: 0.14596  (0.16003)\n     | > log_mle: -0.15028  (-0.14249)\n     | > loss_dur: 0.29623  (0.30252)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.59837  (7.97462)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31550  (0.92454)\n     | > loader_time: 0.01060  (0.00896)\n\n\n\u001b[1m   --> STEP: 377/406 -- GLOBAL_STEP: 13775\u001b[0m\n     | > loss: 0.15789  (0.15954)\n     | > log_mle: -0.15474  (-0.14357)\n     | > loss_dur: 0.31263  (0.30311)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.02933  (8.08158)\n     | > current_lr: 0.00001 \n     | > step_time: 1.30470  (0.95329)\n     | > loader_time: 0.01600  (0.00939)\n\n\n\u001b[1m   --> STEP: 402/406 -- GLOBAL_STEP: 13800\u001b[0m\n     | > loss: 0.15951  (0.15919)\n     | > log_mle: -0.16535  (-0.14455)\n     | > loss_dur: 0.32486  (0.30374)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 16.20112  (8.25836)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94490  (0.97562)\n     | > loader_time: 0.00670  (0.00954)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00794 \u001b[0m(+0.00219)\n     | > avg_loss:\u001b[92m 0.20040 \u001b[0m(-0.02060)\n     | > avg_log_mle:\u001b[92m -0.08369 \u001b[0m(-0.01120)\n     | > avg_loss_dur:\u001b[92m 0.28410 \u001b[0m(-0.00939)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_13804.pth\n\n\u001b[4m\u001b[1m > EPOCH: 34/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:01:41) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 21/406 -- GLOBAL_STEP: 13825\u001b[0m\n     | > loss: 0.14622  (0.14278)\n     | > log_mle: -0.13118  (-0.12600)\n     | > loss_dur: 0.27740  (0.26879)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.34131  (5.56657)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81210  (0.58615)\n     | > loader_time: 0.03100  (0.00587)\n\n\n\u001b[1m   --> STEP: 46/406 -- GLOBAL_STEP: 13850\u001b[0m\n     | > loss: 0.14401  (0.14921)\n     | > log_mle: -0.13073  (-0.12594)\n     | > loss_dur: 0.27473  (0.27515)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.95366  (5.27380)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81900  (0.62244)\n     | > loader_time: 0.00570  (0.00598)\n\n\n\u001b[1m   --> STEP: 71/406 -- GLOBAL_STEP: 13875\u001b[0m\n     | > loss: 0.15226  (0.14880)\n     | > log_mle: -0.13704  (-0.12941)\n     | > loss_dur: 0.28931  (0.27821)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.55304  (6.91645)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69280  (0.63916)\n     | > loader_time: 0.00680  (0.00590)\n\n\n\u001b[1m   --> STEP: 96/406 -- GLOBAL_STEP: 13900\u001b[0m\n     | > loss: 0.13702  (0.14834)\n     | > log_mle: -0.14556  (-0.13300)\n     | > loss_dur: 0.28258  (0.28133)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.98408  (8.25621)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72210  (0.66489)\n     | > loader_time: 0.00720  (0.00627)\n\n\n\u001b[1m   --> STEP: 121/406 -- GLOBAL_STEP: 13925\u001b[0m\n     | > loss: 0.13762  (0.14731)\n     | > log_mle: -0.14367  (-0.13578)\n     | > loss_dur: 0.28129  (0.28310)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.13179  (8.34831)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82990  (0.68585)\n     | > loader_time: 0.01380  (0.00649)\n\n\n\u001b[1m   --> STEP: 146/406 -- GLOBAL_STEP: 13950\u001b[0m\n     | > loss: 0.15122  (0.14712)\n     | > log_mle: -0.13988  (-0.13810)\n     | > loss_dur: 0.29111  (0.28522)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 12.40089  (8.40907)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99680  (0.71214)\n     | > loader_time: 0.00570  (0.00664)\n\n\n\u001b[1m   --> STEP: 171/406 -- GLOBAL_STEP: 13975\u001b[0m\n     | > loss: 0.13578  (0.14743)\n     | > log_mle: -0.15271  (-0.13996)\n     | > loss_dur: 0.28850  (0.28739)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.48186  (8.54151)\n     | > current_lr: 0.00001 \n     | > step_time: 1.07260  (0.73519)\n     | > loader_time: 0.00850  (0.00681)\n\n\n\u001b[1m   --> STEP: 196/406 -- GLOBAL_STEP: 14000\u001b[0m\n     | > loss: 0.14084  (0.14752)\n     | > log_mle: -0.15696  (-0.14173)\n     | > loss_dur: 0.29780  (0.28925)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.91071  (8.69130)\n     | > current_lr: 0.00001 \n     | > step_time: 1.27230  (0.76092)\n     | > loader_time: 0.02380  (0.00702)\n\n\n\u001b[1m   --> STEP: 221/406 -- GLOBAL_STEP: 14025\u001b[0m\n     | > loss: 0.16215  (0.14705)\n     | > log_mle: -0.14747  (-0.14332)\n     | > loss_dur: 0.30961  (0.29036)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.96461  (8.83612)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91120  (0.77832)\n     | > loader_time: 0.00610  (0.00707)\n\n\n\u001b[1m   --> STEP: 246/406 -- GLOBAL_STEP: 14050\u001b[0m\n     | > loss: 0.15340  (0.14684)\n     | > log_mle: -0.15369  (-0.14479)\n     | > loss_dur: 0.30710  (0.29163)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 3.89595  (8.97425)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01250  (0.80577)\n     | > loader_time: 0.00610  (0.00714)\n\n\n\u001b[1m   --> STEP: 271/406 -- GLOBAL_STEP: 14075\u001b[0m\n     | > loss: 0.12055  (0.14655)\n     | > log_mle: -0.17127  (-0.14604)\n     | > loss_dur: 0.29182  (0.29259)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 12.35349  (9.11551)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11290  (0.83006)\n     | > loader_time: 0.00930  (0.00732)\n\n\n\u001b[1m   --> STEP: 296/406 -- GLOBAL_STEP: 14100\u001b[0m\n     | > loss: 0.16546  (0.14646)\n     | > log_mle: -0.15883  (-0.14711)\n     | > loss_dur: 0.32428  (0.29357)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.77321  (9.13419)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05480  (0.85162)\n     | > loader_time: 0.00980  (0.00745)\n\n\n\u001b[1m   --> STEP: 321/406 -- GLOBAL_STEP: 14125\u001b[0m\n     | > loss: 0.13921  (0.14648)\n     | > log_mle: -0.15499  (-0.14807)\n     | > loss_dur: 0.29420  (0.29454)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.91902  (9.29393)\n     | > current_lr: 0.00001 \n     | > step_time: 1.06520  (0.87344)\n     | > loader_time: 0.00710  (0.00756)\n\n\n\u001b[1m   --> STEP: 346/406 -- GLOBAL_STEP: 14150\u001b[0m\n     | > loss: 0.15350  (0.14656)\n     | > log_mle: -0.15699  (-0.14903)\n     | > loss_dur: 0.31049  (0.29558)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.70663  (9.43021)\n     | > current_lr: 0.00001 \n     | > step_time: 1.33250  (0.90113)\n     | > loader_time: 0.00950  (0.00768)\n\n\n\u001b[1m   --> STEP: 371/406 -- GLOBAL_STEP: 14175\u001b[0m\n     | > loss: 0.14539  (0.14613)\n     | > log_mle: -0.16774  (-0.15006)\n     | > loss_dur: 0.31313  (0.29619)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.08027  (9.61267)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31410  (0.92832)\n     | > loader_time: 0.00820  (0.00779)\n\n\n\u001b[1m   --> STEP: 396/406 -- GLOBAL_STEP: 14200\u001b[0m\n     | > loss: 0.13734  (0.14596)\n     | > log_mle: -0.17833  (-0.15096)\n     | > loss_dur: 0.31567  (0.29692)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.32771  (9.68182)\n     | > current_lr: 0.00001 \n     | > step_time: 1.34790  (0.95363)\n     | > loader_time: 0.00960  (0.00792)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00710 \u001b[0m(-0.00084)\n     | > avg_loss:\u001b[92m 0.17051 \u001b[0m(-0.02990)\n     | > avg_log_mle:\u001b[92m -0.10611 \u001b[0m(-0.02241)\n     | > avg_loss_dur:\u001b[92m 0.27661 \u001b[0m(-0.00749)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_14210.pth\n\n\u001b[4m\u001b[1m > EPOCH: 35/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:08:42) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 15/406 -- GLOBAL_STEP: 14225\u001b[0m\n     | > loss: 0.12631  (0.13372)\n     | > log_mle: -0.13470  (-0.13359)\n     | > loss_dur: 0.26101  (0.26731)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 4.61386  (5.79751)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56680  (0.57458)\n     | > loader_time: 0.00580  (0.00489)\n\n\n\u001b[1m   --> STEP: 40/406 -- GLOBAL_STEP: 14250\u001b[0m\n     | > loss: 0.17011  (0.13977)\n     | > log_mle: -0.12357  (-0.13189)\n     | > loss_dur: 0.29368  (0.27167)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 2.90372  (4.83274)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56670  (0.59115)\n     | > loader_time: 0.00680  (0.00572)\n\n\n\u001b[1m   --> STEP: 65/406 -- GLOBAL_STEP: 14275\u001b[0m\n     | > loss: 0.13558  (0.14087)\n     | > log_mle: -0.14533  (-0.13442)\n     | > loss_dur: 0.28092  (0.27529)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.76014  (5.92302)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69660  (0.61868)\n     | > loader_time: 0.00460  (0.00609)\n\n\n\u001b[1m   --> STEP: 90/406 -- GLOBAL_STEP: 14300\u001b[0m\n     | > loss: 0.11740  (0.13986)\n     | > log_mle: -0.14946  (-0.13776)\n     | > loss_dur: 0.26686  (0.27762)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.20942  (7.08421)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74960  (0.64510)\n     | > loader_time: 0.00640  (0.00608)\n\n\n\u001b[1m   --> STEP: 115/406 -- GLOBAL_STEP: 14325\u001b[0m\n     | > loss: 0.15081  (0.13831)\n     | > log_mle: -0.14047  (-0.14122)\n     | > loss_dur: 0.29129  (0.27953)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.02101  (7.65170)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72360  (0.66452)\n     | > loader_time: 0.00470  (0.00625)\n\n\n\u001b[1m   --> STEP: 140/406 -- GLOBAL_STEP: 14350\u001b[0m\n     | > loss: 0.13484  (0.13783)\n     | > log_mle: -0.15963  (-0.14374)\n     | > loss_dur: 0.29447  (0.28157)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.95040  (8.04885)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71480  (0.68226)\n     | > loader_time: 0.00740  (0.00646)\n\n\n\u001b[1m   --> STEP: 165/406 -- GLOBAL_STEP: 14375\u001b[0m\n     | > loss: 0.12519  (0.13714)\n     | > log_mle: -0.16703  (-0.14559)\n     | > loss_dur: 0.29222  (0.28273)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.40676  (8.12988)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75270  (0.70504)\n     | > loader_time: 0.00570  (0.00661)\n\n\n\u001b[1m   --> STEP: 190/406 -- GLOBAL_STEP: 14400\u001b[0m\n     | > loss: 0.13957  (0.13697)\n     | > log_mle: -0.16517  (-0.14747)\n     | > loss_dur: 0.30474  (0.28444)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 7.96888  (8.31165)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80990  (0.72596)\n     | > loader_time: 0.00560  (0.00674)\n\n\n\u001b[1m   --> STEP: 215/406 -- GLOBAL_STEP: 14425\u001b[0m\n     | > loss: 0.12841  (0.13662)\n     | > log_mle: -0.16514  (-0.14895)\n     | > loss_dur: 0.29355  (0.28557)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 16.01501  (9.25973)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02310  (0.75167)\n     | > loader_time: 0.00860  (0.00681)\n\n\n\u001b[1m   --> STEP: 240/406 -- GLOBAL_STEP: 14450\u001b[0m\n     | > loss: 0.11610  (0.13650)\n     | > log_mle: -0.17252  (-0.15057)\n     | > loss_dur: 0.28862  (0.28707)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 18.99049  (9.61663)\n     | > current_lr: 0.00001 \n     | > step_time: 1.14380  (0.77833)\n     | > loader_time: 0.00650  (0.00691)\n\n\n\u001b[1m   --> STEP: 265/406 -- GLOBAL_STEP: 14475\u001b[0m\n     | > loss: 0.12960  (0.13607)\n     | > log_mle: -0.16759  (-0.15188)\n     | > loss_dur: 0.29718  (0.28795)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 15.00930  (9.71975)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08960  (0.80876)\n     | > loader_time: 0.00790  (0.00714)\n\n\n\u001b[1m   --> STEP: 290/406 -- GLOBAL_STEP: 14500\u001b[0m\n     | > loss: 0.11064  (0.13560)\n     | > log_mle: -0.17593  (-0.15310)\n     | > loss_dur: 0.28657  (0.28870)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.22916  (9.82517)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09650  (0.83772)\n     | > loader_time: 0.01420  (0.00761)\n\n\n\u001b[1m   --> STEP: 315/406 -- GLOBAL_STEP: 14525\u001b[0m\n     | > loss: 0.11681  (0.13573)\n     | > log_mle: -0.17749  (-0.15414)\n     | > loss_dur: 0.29430  (0.28987)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.45302  (9.80986)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40020  (0.86847)\n     | > loader_time: 0.01040  (0.00820)\n\n\n\u001b[1m   --> STEP: 340/406 -- GLOBAL_STEP: 14550\u001b[0m\n     | > loss: 0.16573  (0.13607)\n     | > log_mle: -0.15453  (-0.15493)\n     | > loss_dur: 0.32026  (0.29100)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.67887  (9.88909)\n     | > current_lr: 0.00001 \n     | > step_time: 1.64920  (0.91749)\n     | > loader_time: 0.01750  (0.00900)\n\n\n\u001b[1m   --> STEP: 365/406 -- GLOBAL_STEP: 14575\u001b[0m\n     | > loss: 0.10546  (0.13568)\n     | > log_mle: -0.16877  (-0.15595)\n     | > loss_dur: 0.27422  (0.29163)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.91803  (10.16346)\n     | > current_lr: 0.00001 \n     | > step_time: 1.69690  (0.97226)\n     | > loader_time: 0.01000  (0.00982)\n\n\n\u001b[1m   --> STEP: 390/406 -- GLOBAL_STEP: 14600\u001b[0m\n     | > loss: 0.13410  (0.13547)\n     | > log_mle: -0.17787  (-0.15684)\n     | > loss_dur: 0.31196  (0.29231)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.10352  (10.29642)\n     | > current_lr: 0.00001 \n     | > step_time: 1.58680  (1.02351)\n     | > loader_time: 0.01170  (0.01008)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.01342 \u001b[0m(+0.00632)\n     | > avg_loss:\u001b[92m 0.16007 \u001b[0m(-0.01044)\n     | > avg_log_mle:\u001b[92m -0.11331 \u001b[0m(-0.00720)\n     | > avg_loss_dur:\u001b[92m 0.27338 \u001b[0m(-0.00323)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_14616.pth\n\n\u001b[4m\u001b[1m > EPOCH: 36/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:16:27) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 9/406 -- GLOBAL_STEP: 14625\u001b[0m\n     | > loss: 0.15903  (0.12302)\n     | > log_mle: -0.13610  (-0.13683)\n     | > loss_dur: 0.29513  (0.25985)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.22972  (6.22257)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73670  (0.72191)\n     | > loader_time: 0.00480  (0.00470)\n\n\n\u001b[1m   --> STEP: 34/406 -- GLOBAL_STEP: 14650\u001b[0m\n     | > loss: 0.15442  (0.12529)\n     | > log_mle: -0.12800  (-0.13778)\n     | > loss_dur: 0.28242  (0.26307)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 5.41849  (7.26589)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98470  (0.78596)\n     | > loader_time: 0.00560  (0.00585)\n\n\n\u001b[1m   --> STEP: 59/406 -- GLOBAL_STEP: 14675\u001b[0m\n     | > loss: 0.11793  (0.12730)\n     | > log_mle: -0.15369  (-0.13998)\n     | > loss_dur: 0.27162  (0.26728)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 11.57956  (8.57133)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92110  (0.83372)\n     | > loader_time: 0.00950  (0.00692)\n\n\n\u001b[1m   --> STEP: 84/406 -- GLOBAL_STEP: 14700\u001b[0m\n     | > loss: 0.12575  (0.12815)\n     | > log_mle: -0.15613  (-0.14286)\n     | > loss_dur: 0.28187  (0.27100)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 10.89586  (9.05143)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20560  (0.88519)\n     | > loader_time: 0.00650  (0.00750)\n\n\n\u001b[1m   --> STEP: 109/406 -- GLOBAL_STEP: 14725\u001b[0m\n     | > loss: 0.12429  (0.12680)\n     | > log_mle: -0.16463  (-0.14596)\n     | > loss_dur: 0.28892  (0.27276)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 9.86873  (9.12353)\n     | > current_lr: 0.00001 \n     | > step_time: 1.03650  (0.92716)\n     | > loader_time: 0.01820  (0.00832)\n\n\n\u001b[1m   --> STEP: 134/406 -- GLOBAL_STEP: 14750\u001b[0m\n     | > loss: 0.13900  (0.12604)\n     | > log_mle: -0.15757  (-0.14880)\n     | > loss_dur: 0.29657  (0.27484)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 8.44126  (9.37869)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12550  (0.96865)\n     | > loader_time: 0.00840  (0.00870)\n\n\n\u001b[1m   --> STEP: 159/406 -- GLOBAL_STEP: 14775\u001b[0m\n     | > loss: 0.13653  (0.12647)\n     | > log_mle: -0.15935  (-0.15074)\n     | > loss_dur: 0.29588  (0.27721)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 6.56221  (9.53183)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12090  (1.00191)\n     | > loader_time: 0.00790  (0.00903)\n\n\n\u001b[1m   --> STEP: 184/406 -- GLOBAL_STEP: 14800\u001b[0m\n     | > loss: 0.13077  (0.12620)\n     | > log_mle: -0.17828  (-0.15258)\n     | > loss_dur: 0.30905  (0.27878)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 12.31449  (9.84570)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23710  (1.03487)\n     | > loader_time: 0.00800  (0.00927)\n\n\n\u001b[1m   --> STEP: 209/406 -- GLOBAL_STEP: 14825\u001b[0m\n     | > loss: 0.10222  (0.12617)\n     | > log_mle: -0.17446  (-0.15413)\n     | > loss_dur: 0.27668  (0.28030)\n     | > amp_scaler: 16384.00000  (16384.00000)\n     | > grad_norm: 15.37117  (10.22238)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11820  (1.06660)\n     | > loader_time: 0.00830  (0.00960)\n\n\n\u001b[1m   --> STEP: 234/406 -- GLOBAL_STEP: 14850\u001b[0m\n     | > loss: 0.12441  (0.12600)\n     | > log_mle: -0.17532  (-0.15577)\n     | > loss_dur: 0.29973  (0.28177)\n     | > amp_scaler: 8192.00000  (16208.95726)\n     | > grad_norm: 29.69146  (10.93735)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24380  (1.09328)\n     | > loader_time: 0.01650  (0.00991)\n\n\n\u001b[1m   --> STEP: 259/406 -- GLOBAL_STEP: 14875\u001b[0m\n     | > loss: 0.12150  (0.12594)\n     | > log_mle: -0.17957  (-0.15703)\n     | > loss_dur: 0.30107  (0.28296)\n     | > amp_scaler: 4096.00000  (15371.86100)\n     | > grad_norm: 31.85588  (12.63632)\n     | > current_lr: 0.00001 \n     | > step_time: 1.58570  (1.12110)\n     | > loader_time: 0.04350  (0.01050)\n\n\n\u001b[1m   --> STEP: 284/406 -- GLOBAL_STEP: 14900\u001b[0m\n     | > loss: 0.12809  (0.12549)\n     | > log_mle: -0.17607  (-0.15815)\n     | > loss_dur: 0.30416  (0.28364)\n     | > amp_scaler: 4096.00000  (14379.26761)\n     | > grad_norm: 14.99175  (13.29127)\n     | > current_lr: 0.00001 \n     | > step_time: 1.43750  (1.14669)\n     | > loader_time: 0.01850  (0.01081)\n\n\n\u001b[1m   --> STEP: 309/406 -- GLOBAL_STEP: 14925\u001b[0m\n     | > loss: 0.10636  (0.12561)\n     | > log_mle: -0.17303  (-0.15911)\n     | > loss_dur: 0.27939  (0.28472)\n     | > amp_scaler: 4096.00000  (13547.28803)\n     | > grad_norm: 7.05391  (13.04494)\n     | > current_lr: 0.00001 \n     | > step_time: 1.53990  (1.17274)\n     | > loader_time: 0.02550  (0.01116)\n\n\n\u001b[1m   --> STEP: 334/406 -- GLOBAL_STEP: 14950\u001b[0m\n     | > loss: 0.13584  (0.12571)\n     | > log_mle: -0.17940  (-0.15997)\n     | > loss_dur: 0.31524  (0.28568)\n     | > amp_scaler: 4096.00000  (12839.85629)\n     | > grad_norm: 15.30645  (13.27838)\n     | > current_lr: 0.00001 \n     | > step_time: 1.44670  (1.20207)\n     | > loader_time: 0.00990  (0.01166)\n\n\n\u001b[1m   --> STEP: 359/406 -- GLOBAL_STEP: 14975\u001b[0m\n     | > loss: 0.11659  (0.12572)\n     | > log_mle: -0.16927  (-0.16094)\n     | > loss_dur: 0.28585  (0.28666)\n     | > amp_scaler: 4096.00000  (12230.95265)\n     | > grad_norm: 7.82219  (13.62214)\n     | > current_lr: 0.00001 \n     | > step_time: 1.65790  (1.23643)\n     | > loader_time: 0.01730  (0.01236)\n\n\n\u001b[1m   --> STEP: 384/406 -- GLOBAL_STEP: 15000\u001b[0m\n     | > loss: 0.11589  (0.12547)\n     | > log_mle: -0.16982  (-0.16184)\n     | > loss_dur: 0.28571  (0.28730)\n     | > amp_scaler: 4096.00000  (11701.33333)\n     | > grad_norm: 20.28541  (13.83803)\n     | > current_lr: 0.00001 \n     | > step_time: 1.71220  (1.27053)\n     | > loader_time: 0.01960  (0.01288)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00927 \u001b[0m(-0.00415)\n     | > avg_loss:\u001b[91m 0.16586 \u001b[0m(+0.00580)\n     | > avg_log_mle:\u001b[91m -0.10491 \u001b[0m(+0.00840)\n     | > avg_loss_dur:\u001b[92m 0.27077 \u001b[0m(-0.00260)\n\n\n\u001b[4m\u001b[1m > EPOCH: 37/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:25:55) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 3/406 -- GLOBAL_STEP: 15025\u001b[0m\n     | > loss: 0.13606  (0.10249)\n     | > log_mle: -0.14694  (-0.14934)\n     | > loss_dur: 0.28300  (0.25184)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.15004  (14.17087)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73220  (0.71653)\n     | > loader_time: 0.00450  (0.00429)\n\n\n\u001b[1m   --> STEP: 28/406 -- GLOBAL_STEP: 15050\u001b[0m\n     | > loss: 0.13850  (0.11159)\n     | > log_mle: -0.14345  (-0.14327)\n     | > loss_dur: 0.28195  (0.25486)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 6.39519  (8.00408)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90080  (0.80231)\n     | > loader_time: 0.00570  (0.00628)\n\n\n\u001b[1m   --> STEP: 53/406 -- GLOBAL_STEP: 15075\u001b[0m\n     | > loss: 0.11279  (0.11777)\n     | > log_mle: -0.14240  (-0.14442)\n     | > loss_dur: 0.25519  (0.26218)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 6.88815  (7.34693)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84670  (0.85288)\n     | > loader_time: 0.00870  (0.00691)\n\n\n\u001b[1m   --> STEP: 78/406 -- GLOBAL_STEP: 15100\u001b[0m\n     | > loss: 0.11885  (0.11898)\n     | > log_mle: -0.15213  (-0.14743)\n     | > loss_dur: 0.27098  (0.26642)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.76944  (8.13867)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10240  (0.89038)\n     | > loader_time: 0.01690  (0.00727)\n\n\n\u001b[1m   --> STEP: 103/406 -- GLOBAL_STEP: 15125\u001b[0m\n     | > loss: 0.09056  (0.11859)\n     | > log_mle: -0.18200  (-0.15093)\n     | > loss_dur: 0.27256  (0.26952)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.64350  (8.82367)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99250  (0.93487)\n     | > loader_time: 0.01100  (0.00786)\n\n\n\u001b[1m   --> STEP: 128/406 -- GLOBAL_STEP: 15150\u001b[0m\n     | > loss: 0.10992  (0.11793)\n     | > log_mle: -0.17161  (-0.15363)\n     | > loss_dur: 0.28152  (0.27157)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.50527  (9.33030)\n     | > current_lr: 0.00001 \n     | > step_time: 2.03590  (0.98500)\n     | > loader_time: 0.02830  (0.00815)\n\n\n\u001b[1m   --> STEP: 153/406 -- GLOBAL_STEP: 15175\u001b[0m\n     | > loss: 0.12990  (0.11798)\n     | > log_mle: -0.16158  (-0.15574)\n     | > loss_dur: 0.29147  (0.27372)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.51553  (9.70328)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09750  (1.00963)\n     | > loader_time: 0.00780  (0.00836)\n\n\n\u001b[1m   --> STEP: 178/406 -- GLOBAL_STEP: 15200\u001b[0m\n     | > loss: 0.13432  (0.11778)\n     | > log_mle: -0.15529  (-0.15739)\n     | > loss_dur: 0.28961  (0.27517)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.47738  (10.25333)\n     | > current_lr: 0.00001 \n     | > step_time: 1.13730  (1.03734)\n     | > loader_time: 0.00970  (0.00870)\n\n\n\u001b[1m   --> STEP: 203/406 -- GLOBAL_STEP: 15225\u001b[0m\n     | > loss: 0.13044  (0.11798)\n     | > log_mle: -0.16085  (-0.15899)\n     | > loss_dur: 0.29129  (0.27697)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.07463  (10.40598)\n     | > current_lr: 0.00001 \n     | > step_time: 1.29850  (1.06741)\n     | > loader_time: 0.00890  (0.00899)\n\n\n\u001b[1m   --> STEP: 228/406 -- GLOBAL_STEP: 15250\u001b[0m\n     | > loss: 0.10106  (0.11761)\n     | > log_mle: -0.18138  (-0.16065)\n     | > loss_dur: 0.28244  (0.27826)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.88629  (10.63737)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24330  (1.09906)\n     | > loader_time: 0.00730  (0.00935)\n\n\n\u001b[1m   --> STEP: 253/406 -- GLOBAL_STEP: 15275\u001b[0m\n     | > loss: 0.10607  (0.11748)\n     | > log_mle: -0.18899  (-0.16213)\n     | > loss_dur: 0.29506  (0.27961)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.66484  (10.78065)\n     | > current_lr: 0.00001 \n     | > step_time: 1.57830  (1.13164)\n     | > loader_time: 0.00950  (0.00977)\n\n\n\u001b[1m   --> STEP: 278/406 -- GLOBAL_STEP: 15300\u001b[0m\n     | > loss: 0.10728  (0.11702)\n     | > log_mle: -0.16051  (-0.16330)\n     | > loss_dur: 0.26779  (0.28033)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.03090  (11.08289)\n     | > current_lr: 0.00001 \n     | > step_time: 1.39080  (1.15921)\n     | > loader_time: 0.01140  (0.01013)\n\n\n\u001b[1m   --> STEP: 303/406 -- GLOBAL_STEP: 15325\u001b[0m\n     | > loss: 0.13245  (0.11710)\n     | > log_mle: -0.17384  (-0.16439)\n     | > loss_dur: 0.30629  (0.28149)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 10.62187  (11.44602)\n     | > current_lr: 0.00001 \n     | > step_time: 1.51150  (1.18927)\n     | > loader_time: 0.01100  (0.01077)\n\n\n\u001b[1m   --> STEP: 328/406 -- GLOBAL_STEP: 15350\u001b[0m\n     | > loss: 0.13852  (0.11709)\n     | > log_mle: -0.17982  (-0.16526)\n     | > loss_dur: 0.31834  (0.28235)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.27385  (12.09631)\n     | > current_lr: 0.00001 \n     | > step_time: 1.48830  (1.22072)\n     | > loader_time: 0.01820  (0.01120)\n\n\n\u001b[1m   --> STEP: 353/406 -- GLOBAL_STEP: 15375\u001b[0m\n     | > loss: 0.10207  (0.11706)\n     | > log_mle: -0.19039  (-0.16624)\n     | > loss_dur: 0.29246  (0.28329)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.85585  (12.10498)\n     | > current_lr: 0.00001 \n     | > step_time: 1.54670  (1.25271)\n     | > loader_time: 0.02300  (0.01186)\n\n\n\u001b[1m   --> STEP: 378/406 -- GLOBAL_STEP: 15400\u001b[0m\n     | > loss: 0.11853  (0.11658)\n     | > log_mle: -0.17508  (-0.16722)\n     | > loss_dur: 0.29361  (0.28380)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.98914  (12.29457)\n     | > current_lr: 0.00001 \n     | > step_time: 1.75110  (1.28879)\n     | > loader_time: 0.02360  (0.01276)\n\n\n\u001b[1m   --> STEP: 403/406 -- GLOBAL_STEP: 15425\u001b[0m\n     | > loss: 0.08749  (0.11628)\n     | > log_mle: -0.17651  (-0.16816)\n     | > loss_dur: 0.26400  (0.28445)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 21.09650  (12.58636)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02530  (1.30866)\n     | > loader_time: 0.00860  (0.01305)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00829 \u001b[0m(-0.00099)\n     | > avg_loss:\u001b[92m 0.14402 \u001b[0m(-0.02184)\n     | > avg_log_mle:\u001b[92m -0.12513 \u001b[0m(-0.02023)\n     | > avg_loss_dur:\u001b[92m 0.26916 \u001b[0m(-0.00162)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_15428.pth\n\n\u001b[4m\u001b[1m > EPOCH: 38/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:35:34) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 22/406 -- GLOBAL_STEP: 15450\u001b[0m\n     | > loss: 0.11089  (0.10267)\n     | > log_mle: -0.14439  (-0.14865)\n     | > loss_dur: 0.25528  (0.25133)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 7.59961  (7.16536)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91540  (0.80426)\n     | > loader_time: 0.00540  (0.00649)\n\n\n\u001b[1m   --> STEP: 47/406 -- GLOBAL_STEP: 15475\u001b[0m\n     | > loss: 0.09400  (0.11048)\n     | > log_mle: -0.15970  (-0.14854)\n     | > loss_dur: 0.25370  (0.25902)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.03445  (7.78379)\n     | > current_lr: 0.00001 \n     | > step_time: 1.26930  (0.83738)\n     | > loader_time: 0.00580  (0.00650)\n\n\n\u001b[1m   --> STEP: 72/406 -- GLOBAL_STEP: 15500\u001b[0m\n     | > loss: 0.11018  (0.11250)\n     | > log_mle: -0.14725  (-0.15141)\n     | > loss_dur: 0.25743  (0.26391)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.99851  (9.46682)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96060  (0.88495)\n     | > loader_time: 0.00580  (0.00713)\n\n\n\u001b[1m   --> STEP: 97/406 -- GLOBAL_STEP: 15525\u001b[0m\n     | > loss: 0.12539  (0.11241)\n     | > log_mle: -0.15863  (-0.15488)\n     | > loss_dur: 0.28402  (0.26729)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.93998  (11.20320)\n     | > current_lr: 0.00001 \n     | > step_time: 1.04130  (0.92623)\n     | > loader_time: 0.00660  (0.00725)\n\n\n\u001b[1m   --> STEP: 122/406 -- GLOBAL_STEP: 15550\u001b[0m\n     | > loss: 0.11013  (0.11129)\n     | > log_mle: -0.15609  (-0.15758)\n     | > loss_dur: 0.26622  (0.26888)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.59906  (11.95160)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11770  (0.96847)\n     | > loader_time: 0.00630  (0.00747)\n\n\n\u001b[1m   --> STEP: 147/406 -- GLOBAL_STEP: 15575\u001b[0m\n     | > loss: 0.10715  (0.11090)\n     | > log_mle: -0.17159  (-0.15991)\n     | > loss_dur: 0.27874  (0.27081)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.41943  (12.37899)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18740  (1.00310)\n     | > loader_time: 0.00740  (0.00785)\n\n\n\u001b[1m   --> STEP: 172/406 -- GLOBAL_STEP: 15600\u001b[0m\n     | > loss: 0.10334  (0.11078)\n     | > log_mle: -0.17514  (-0.16175)\n     | > loss_dur: 0.27847  (0.27253)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.70473  (12.53507)\n     | > current_lr: 0.00001 \n     | > step_time: 1.22990  (1.03822)\n     | > loader_time: 0.02540  (0.00831)\n\n\n\u001b[1m   --> STEP: 197/406 -- GLOBAL_STEP: 15625\u001b[0m\n     | > loss: 0.12073  (0.11063)\n     | > log_mle: -0.17436  (-0.16349)\n     | > loss_dur: 0.29509  (0.27412)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.61955  (12.77784)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15670  (1.06574)\n     | > loader_time: 0.00850  (0.00857)\n\n\n\u001b[1m   --> STEP: 222/406 -- GLOBAL_STEP: 15650\u001b[0m\n     | > loss: 0.10818  (0.11041)\n     | > log_mle: -0.18145  (-0.16506)\n     | > loss_dur: 0.28963  (0.27547)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.45071  (13.52536)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23790  (1.09335)\n     | > loader_time: 0.02140  (0.00893)\n\n\n\u001b[1m   --> STEP: 247/406 -- GLOBAL_STEP: 15675\u001b[0m\n     | > loss: 0.11289  (0.11041)\n     | > log_mle: -0.16444  (-0.16644)\n     | > loss_dur: 0.27733  (0.27685)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.41495  (13.72856)\n     | > current_lr: 0.00001 \n     | > step_time: 1.58710  (1.12405)\n     | > loader_time: 0.00880  (0.00934)\n\n\n\u001b[1m   --> STEP: 272/406 -- GLOBAL_STEP: 15700\u001b[0m\n     | > loss: 0.09042  (0.10979)\n     | > log_mle: -0.18631  (-0.16775)\n     | > loss_dur: 0.27673  (0.27754)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.97470  (14.06418)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42600  (1.15043)\n     | > loader_time: 0.02850  (0.00975)\n\n\n\u001b[1m   --> STEP: 297/406 -- GLOBAL_STEP: 15725\u001b[0m\n     | > loss: 0.11485  (0.10968)\n     | > log_mle: -0.17829  (-0.16875)\n     | > loss_dur: 0.29314  (0.27843)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 21.90389  (14.63959)\n     | > current_lr: 0.00001 \n     | > step_time: 1.36530  (1.17790)\n     | > loader_time: 0.00830  (0.00993)\n\n\n\u001b[1m   --> STEP: 322/406 -- GLOBAL_STEP: 15750\u001b[0m\n     | > loss: 0.11577  (0.10976)\n     | > log_mle: -0.18598  (-0.16969)\n     | > loss_dur: 0.30175  (0.27945)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.09928  (14.90672)\n     | > current_lr: 0.00001 \n     | > step_time: 2.25170  (1.20913)\n     | > loader_time: 0.01960  (0.01029)\n\n\n\u001b[1m   --> STEP: 347/406 -- GLOBAL_STEP: 15775\u001b[0m\n     | > loss: 0.09444  (0.10970)\n     | > log_mle: -0.19343  (-0.17062)\n     | > loss_dur: 0.28787  (0.28031)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.74800  (15.09003)\n     | > current_lr: 0.00001 \n     | > step_time: 1.51220  (1.23450)\n     | > loader_time: 0.01170  (0.01052)\n\n\n\u001b[1m   --> STEP: 372/406 -- GLOBAL_STEP: 15800\u001b[0m\n     | > loss: 0.12002  (0.10950)\n     | > log_mle: -0.17404  (-0.17152)\n     | > loss_dur: 0.29406  (0.28101)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.67711  (15.64614)\n     | > current_lr: 0.00001 \n     | > step_time: 1.59710  (1.26526)\n     | > loader_time: 0.01240  (0.01109)\n\n\n\u001b[1m   --> STEP: 397/406 -- GLOBAL_STEP: 15825\u001b[0m\n     | > loss: 0.09715  (0.10931)\n     | > log_mle: -0.18686  (-0.17239)\n     | > loss_dur: 0.28401  (0.28170)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.70313  (15.73552)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31800  (1.29458)\n     | > loader_time: 0.01190  (0.01121)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00904 \u001b[0m(+0.00075)\n     | > avg_loss:\u001b[92m 0.13416 \u001b[0m(-0.00986)\n     | > avg_log_mle:\u001b[92m -0.13285 \u001b[0m(-0.00772)\n     | > avg_loss_dur:\u001b[92m 0.26701 \u001b[0m(-0.00214)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_15834.pth\n\n\u001b[4m\u001b[1m > EPOCH: 39/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:45:08) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 16/406 -- GLOBAL_STEP: 15850\u001b[0m\n     | > loss: 0.11867  (0.09460)\n     | > log_mle: -0.15771  (-0.15482)\n     | > loss_dur: 0.27638  (0.24942)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 6.65205  (7.77592)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75150  (0.77697)\n     | > loader_time: 0.00500  (0.00519)\n\n\n\u001b[1m   --> STEP: 41/406 -- GLOBAL_STEP: 15875\u001b[0m\n     | > loss: 0.11126  (0.10252)\n     | > log_mle: -0.14552  (-0.15286)\n     | > loss_dur: 0.25678  (0.25538)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 4.53386  (7.35919)\n     | > current_lr: 0.00001 \n     | > step_time: 0.85910  (0.82466)\n     | > loader_time: 0.00610  (0.00670)\n\n\n\u001b[1m   --> STEP: 66/406 -- GLOBAL_STEP: 15900\u001b[0m\n     | > loss: 0.12653  (0.10426)\n     | > log_mle: -0.16033  (-0.15554)\n     | > loss_dur: 0.28685  (0.25981)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 7.92250  (8.55329)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99750  (0.87198)\n     | > loader_time: 0.00760  (0.00687)\n\n\n\u001b[1m   --> STEP: 91/406 -- GLOBAL_STEP: 15925\u001b[0m\n     | > loss: 0.09329  (0.10379)\n     | > log_mle: -0.18412  (-0.15909)\n     | > loss_dur: 0.27741  (0.26288)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.69451  (10.78051)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12010  (0.92425)\n     | > loader_time: 0.01790  (0.00740)\n\n\n\u001b[1m   --> STEP: 116/406 -- GLOBAL_STEP: 15950\u001b[0m\n     | > loss: 0.10829  (0.10282)\n     | > log_mle: -0.16208  (-0.16214)\n     | > loss_dur: 0.27037  (0.26496)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.72079  (13.04157)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01460  (0.96167)\n     | > loader_time: 0.00960  (0.00774)\n\n\n\u001b[1m   --> STEP: 141/406 -- GLOBAL_STEP: 15975\u001b[0m\n     | > loss: 0.09056  (0.10233)\n     | > log_mle: -0.18052  (-0.16470)\n     | > loss_dur: 0.27108  (0.26703)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.64806  (13.55295)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08060  (0.99399)\n     | > loader_time: 0.00760  (0.00802)\n\n\n\u001b[1m   --> STEP: 166/406 -- GLOBAL_STEP: 16000\u001b[0m\n     | > loss: 0.09006  (0.10226)\n     | > log_mle: -0.18782  (-0.16653)\n     | > loss_dur: 0.27788  (0.26879)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.52894  (13.35956)\n     | > current_lr: 0.00001 \n     | > step_time: 1.22810  (1.02522)\n     | > loader_time: 0.01250  (0.00839)\n\n\n\u001b[1m   --> STEP: 191/406 -- GLOBAL_STEP: 16025\u001b[0m\n     | > loss: 0.09075  (0.10192)\n     | > log_mle: -0.18296  (-0.16829)\n     | > loss_dur: 0.27371  (0.27021)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.20759  (13.39548)\n     | > current_lr: 0.00001 \n     | > step_time: 1.41610  (1.05642)\n     | > loader_time: 0.00790  (0.00885)\n\n\n\u001b[1m   --> STEP: 216/406 -- GLOBAL_STEP: 16050\u001b[0m\n     | > loss: 0.10113  (0.10159)\n     | > log_mle: -0.18846  (-0.16973)\n     | > loss_dur: 0.28959  (0.27132)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.29687  (13.24605)\n     | > current_lr: 0.00001 \n     | > step_time: 1.47230  (1.08338)\n     | > loader_time: 0.01030  (0.00901)\n\n\n\u001b[1m   --> STEP: 241/406 -- GLOBAL_STEP: 16075\u001b[0m\n     | > loss: 0.10253  (0.10163)\n     | > log_mle: -0.17568  (-0.17123)\n     | > loss_dur: 0.27821  (0.27287)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.74144  (13.54854)\n     | > current_lr: 0.00001 \n     | > step_time: 2.15840  (1.11487)\n     | > loader_time: 0.01050  (0.00941)\n\n\n\u001b[1m   --> STEP: 266/406 -- GLOBAL_STEP: 16100\u001b[0m\n     | > loss: 0.09914  (0.10124)\n     | > log_mle: -0.18047  (-0.17251)\n     | > loss_dur: 0.27961  (0.27375)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 6.58624  (13.71780)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32790  (1.14231)\n     | > loader_time: 0.00820  (0.00968)\n\n\n\u001b[1m   --> STEP: 291/406 -- GLOBAL_STEP: 16125\u001b[0m\n     | > loss: 0.10876  (0.10072)\n     | > log_mle: -0.17285  (-0.17368)\n     | > loss_dur: 0.28161  (0.27440)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.96045  (13.90843)\n     | > current_lr: 0.00001 \n     | > step_time: 1.60810  (1.17341)\n     | > loader_time: 0.02060  (0.01027)\n\n\n\u001b[1m   --> STEP: 316/406 -- GLOBAL_STEP: 16150\u001b[0m\n     | > loss: 0.10864  (0.10063)\n     | > log_mle: -0.18613  (-0.17477)\n     | > loss_dur: 0.29477  (0.27540)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.53537  (14.12717)\n     | > current_lr: 0.00001 \n     | > step_time: 1.49700  (1.20513)\n     | > loader_time: 0.01780  (0.01090)\n\n\n\u001b[1m   --> STEP: 341/406 -- GLOBAL_STEP: 16175\u001b[0m\n     | > loss: 0.08326  (0.10072)\n     | > log_mle: -0.20550  (-0.17561)\n     | > loss_dur: 0.28876  (0.27633)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.75394  (14.18634)\n     | > current_lr: 0.00001 \n     | > step_time: 1.53610  (1.23545)\n     | > loader_time: 0.02080  (0.01183)\n\n\n\u001b[1m   --> STEP: 366/406 -- GLOBAL_STEP: 16200\u001b[0m\n     | > loss: 0.08830  (0.10040)\n     | > log_mle: -0.19658  (-0.17662)\n     | > loss_dur: 0.28488  (0.27702)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.32978  (14.30250)\n     | > current_lr: 0.00001 \n     | > step_time: 1.60840  (1.26865)\n     | > loader_time: 0.01100  (0.01263)\n\n\n\u001b[1m   --> STEP: 391/406 -- GLOBAL_STEP: 16225\u001b[0m\n     | > loss: 0.08989  (0.10031)\n     | > log_mle: -0.19827  (-0.17747)\n     | > loss_dur: 0.28816  (0.27779)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.02334  (14.47140)\n     | > current_lr: 0.00001 \n     | > step_time: 1.76730  (1.30225)\n     | > loader_time: 0.01320  (0.01313)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00884 \u001b[0m(-0.00020)\n     | > avg_loss:\u001b[91m 0.16659 \u001b[0m(+0.03243)\n     | > avg_log_mle:\u001b[91m -0.09642 \u001b[0m(+0.03643)\n     | > avg_loss_dur:\u001b[92m 0.26301 \u001b[0m(-0.00400)\n\n\n\u001b[4m\u001b[1m > EPOCH: 40/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 18:54:44) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 10/406 -- GLOBAL_STEP: 16250\u001b[0m\n     | > loss: 0.07388  (0.08473)\n     | > log_mle: -0.15673  (-0.15738)\n     | > loss_dur: 0.23061  (0.24211)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 3.87952  (5.23140)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68220  (0.76500)\n     | > loader_time: 0.00490  (0.00754)\n\n\n\u001b[1m   --> STEP: 35/406 -- GLOBAL_STEP: 16275\u001b[0m\n     | > loss: 0.08468  (0.09287)\n     | > log_mle: -0.17738  (-0.15764)\n     | > loss_dur: 0.26206  (0.25051)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.86773  (8.16864)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82190  (0.80627)\n     | > loader_time: 0.00610  (0.00675)\n\n\n\u001b[1m   --> STEP: 60/406 -- GLOBAL_STEP: 16300\u001b[0m\n     | > loss: 0.10046  (0.09404)\n     | > log_mle: -0.16385  (-0.15969)\n     | > loss_dur: 0.26431  (0.25373)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.94052  (8.74094)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84030  (0.86066)\n     | > loader_time: 0.00600  (0.00745)\n\n\n\u001b[1m   --> STEP: 85/406 -- GLOBAL_STEP: 16325\u001b[0m\n     | > loss: 0.06411  (0.09420)\n     | > log_mle: -0.18669  (-0.16286)\n     | > loss_dur: 0.25081  (0.25706)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.21127  (9.84808)\n     | > current_lr: 0.00001 \n     | > step_time: 1.04780  (0.90218)\n     | > loader_time: 0.00950  (0.00837)\n\n\n\u001b[1m   --> STEP: 110/406 -- GLOBAL_STEP: 16350\u001b[0m\n     | > loss: 0.07888  (0.09329)\n     | > log_mle: -0.19169  (-0.16606)\n     | > loss_dur: 0.27057  (0.25935)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.68795  (11.06842)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98510  (0.95019)\n     | > loader_time: 0.00740  (0.00900)\n\n\n\u001b[1m   --> STEP: 135/406 -- GLOBAL_STEP: 16375\u001b[0m\n     | > loss: 0.09990  (0.09258)\n     | > log_mle: -0.18429  (-0.16887)\n     | > loss_dur: 0.28419  (0.26146)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.18538  (11.58301)\n     | > current_lr: 0.00001 \n     | > step_time: 1.06050  (0.99379)\n     | > loader_time: 0.00760  (0.00881)\n\n\n\u001b[1m   --> STEP: 160/406 -- GLOBAL_STEP: 16400\u001b[0m\n     | > loss: 0.09325  (0.09325)\n     | > log_mle: -0.18388  (-0.17075)\n     | > loss_dur: 0.27712  (0.26400)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 6.97696  (11.88546)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24480  (1.02690)\n     | > loader_time: 0.00800  (0.00894)\n\n\n\u001b[1m   --> STEP: 185/406 -- GLOBAL_STEP: 16425\u001b[0m\n     | > loss: 0.10170  (0.09315)\n     | > log_mle: -0.18402  (-0.17250)\n     | > loss_dur: 0.28572  (0.26564)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.02640  (12.57607)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24140  (1.05999)\n     | > loader_time: 0.00740  (0.00934)\n\n\n\u001b[1m   --> STEP: 210/406 -- GLOBAL_STEP: 16450\u001b[0m\n     | > loss: 0.08598  (0.09316)\n     | > log_mle: -0.18261  (-0.17403)\n     | > loss_dur: 0.26858  (0.26719)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.92568  (13.14901)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17570  (1.08544)\n     | > loader_time: 0.00730  (0.00938)\n\n\n\u001b[1m   --> STEP: 235/406 -- GLOBAL_STEP: 16475\u001b[0m\n     | > loss: 0.10339  (0.09285)\n     | > log_mle: -0.18793  (-0.17575)\n     | > loss_dur: 0.29131  (0.26860)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.14539  (13.66443)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32390  (1.11154)\n     | > loader_time: 0.00890  (0.00954)\n\n\n\u001b[1m   --> STEP: 260/406 -- GLOBAL_STEP: 16500\u001b[0m\n     | > loss: 0.08242  (0.09261)\n     | > log_mle: -0.19527  (-0.17714)\n     | > loss_dur: 0.27769  (0.26975)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.51949  (14.09500)\n     | > current_lr: 0.00001 \n     | > step_time: 1.47080  (1.13847)\n     | > loader_time: 0.02240  (0.00980)\n\n\n\u001b[1m   --> STEP: 285/406 -- GLOBAL_STEP: 16525\u001b[0m\n     | > loss: 0.10342  (0.09210)\n     | > log_mle: -0.18521  (-0.17832)\n     | > loss_dur: 0.28863  (0.27042)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.15491  (14.44356)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28000  (1.16184)\n     | > loader_time: 0.00930  (0.00986)\n\n\n\u001b[1m   --> STEP: 310/406 -- GLOBAL_STEP: 16550\u001b[0m\n     | > loss: 0.10774  (0.09230)\n     | > log_mle: -0.18253  (-0.17924)\n     | > loss_dur: 0.29026  (0.27155)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.25133  (14.56971)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42990  (1.19097)\n     | > loader_time: 0.00920  (0.01020)\n\n\n\u001b[1m   --> STEP: 335/406 -- GLOBAL_STEP: 16575\u001b[0m\n     | > loss: 0.12357  (0.09256)\n     | > log_mle: -0.19374  (-0.18007)\n     | > loss_dur: 0.31731  (0.27262)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.95462  (15.13893)\n     | > current_lr: 0.00001 \n     | > step_time: 1.49140  (1.21934)\n     | > loader_time: 0.01030  (0.01039)\n\n\n\u001b[1m   --> STEP: 360/406 -- GLOBAL_STEP: 16600\u001b[0m\n     | > loss: 0.07767  (0.09240)\n     | > log_mle: -0.19435  (-0.18092)\n     | > loss_dur: 0.27201  (0.27332)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.97441  (15.74866)\n     | > current_lr: 0.00001 \n     | > step_time: 1.65010  (1.25087)\n     | > loader_time: 0.01360  (0.01056)\n\n\n\u001b[1m   --> STEP: 385/406 -- GLOBAL_STEP: 16625\u001b[0m\n     | > loss: 0.09049  (0.09222)\n     | > log_mle: -0.18962  (-0.18175)\n     | > loss_dur: 0.28011  (0.27397)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.02775  (15.79679)\n     | > current_lr: 0.00001 \n     | > step_time: 2.36980  (1.28318)\n     | > loader_time: 0.01070  (0.01077)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.01239 \u001b[0m(+0.00355)\n     | > avg_loss:\u001b[92m 0.14584 \u001b[0m(-0.02075)\n     | > avg_log_mle:\u001b[92m -0.11682 \u001b[0m(-0.02040)\n     | > avg_loss_dur:\u001b[92m 0.26266 \u001b[0m(-0.00035)\n\n\n\u001b[4m\u001b[1m > EPOCH: 41/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:04:13) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 4/406 -- GLOBAL_STEP: 16650\u001b[0m\n     | > loss: 0.09982  (0.08575)\n     | > log_mle: -0.14246  (-0.16292)\n     | > loss_dur: 0.24228  (0.24867)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 4.72714  (7.54154)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75490  (0.73545)\n     | > loader_time: 0.00630  (0.00506)\n\n\n\u001b[1m   --> STEP: 29/406 -- GLOBAL_STEP: 16675\u001b[0m\n     | > loss: 0.07422  (0.08382)\n     | > log_mle: -0.16211  (-0.16243)\n     | > loss_dur: 0.23633  (0.24625)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 7.00524  (11.49855)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82610  (0.78226)\n     | > loader_time: 0.00530  (0.00610)\n\n\n\u001b[1m   --> STEP: 54/406 -- GLOBAL_STEP: 16700\u001b[0m\n     | > loss: 0.06650  (0.08527)\n     | > log_mle: -0.16828  (-0.16368)\n     | > loss_dur: 0.23478  (0.24895)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.16910  (11.42014)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84940  (0.84189)\n     | > loader_time: 0.00920  (0.00704)\n\n\n\u001b[1m   --> STEP: 79/406 -- GLOBAL_STEP: 16725\u001b[0m\n     | > loss: 0.09981  (0.08638)\n     | > log_mle: -0.17702  (-0.16667)\n     | > loss_dur: 0.27683  (0.25305)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 45.02300  (13.44490)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18840  (0.89349)\n     | > loader_time: 0.00930  (0.00785)\n\n\n\u001b[1m   --> STEP: 104/406 -- GLOBAL_STEP: 16750\u001b[0m\n     | > loss: 0.07946  (0.08567)\n     | > log_mle: -0.17242  (-0.17001)\n     | > loss_dur: 0.25187  (0.25568)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.83316  (13.55555)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02500  (0.93159)\n     | > loader_time: 0.00730  (0.00810)\n\n\n\u001b[1m   --> STEP: 129/406 -- GLOBAL_STEP: 16775\u001b[0m\n     | > loss: 0.09434  (0.08510)\n     | > log_mle: -0.17446  (-0.17264)\n     | > loss_dur: 0.26880  (0.25774)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.23513  (13.89511)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40470  (0.97342)\n     | > loader_time: 0.02640  (0.00836)\n\n\n\u001b[1m   --> STEP: 154/406 -- GLOBAL_STEP: 16800\u001b[0m\n     | > loss: 0.07670  (0.08516)\n     | > log_mle: -0.17754  (-0.17473)\n     | > loss_dur: 0.25424  (0.25989)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.13505  (14.26487)\n     | > current_lr: 0.00001 \n     | > step_time: 1.52320  (1.00525)\n     | > loader_time: 0.00780  (0.00888)\n\n\n\u001b[1m   --> STEP: 179/406 -- GLOBAL_STEP: 16825\u001b[0m\n     | > loss: 0.08886  (0.08489)\n     | > log_mle: -0.19247  (-0.17643)\n     | > loss_dur: 0.28133  (0.26132)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.89716  (14.45947)\n     | > current_lr: 0.00001 \n     | > step_time: 1.41700  (1.04033)\n     | > loader_time: 0.00800  (0.00916)\n\n\n\u001b[1m   --> STEP: 204/406 -- GLOBAL_STEP: 16850\u001b[0m\n     | > loss: 0.08363  (0.08494)\n     | > log_mle: -0.18664  (-0.17800)\n     | > loss_dur: 0.27027  (0.26294)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.65123  (14.71158)\n     | > current_lr: 0.00001 \n     | > step_time: 1.81940  (1.07158)\n     | > loader_time: 0.00740  (0.00943)\n\n\n\u001b[1m   --> STEP: 229/406 -- GLOBAL_STEP: 16875\u001b[0m\n     | > loss: 0.08197  (0.08459)\n     | > log_mle: -0.18447  (-0.17963)\n     | > loss_dur: 0.26644  (0.26421)\n     | > amp_scaler: 8192.00000  (4167.54585)\n     | > grad_norm: 13.11896  (14.95662)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25280  (1.10260)\n     | > loader_time: 0.00850  (0.00966)\n\n\n\u001b[1m   --> STEP: 254/406 -- GLOBAL_STEP: 16900\u001b[0m\n     | > loss: 0.08780  (0.08440)\n     | > log_mle: -0.18213  (-0.18105)\n     | > loss_dur: 0.26992  (0.26544)\n     | > amp_scaler: 8192.00000  (4563.65354)\n     | > grad_norm: 14.62882  (15.17972)\n     | > current_lr: 0.00001 \n     | > step_time: 1.62360  (1.13414)\n     | > loader_time: 0.02360  (0.00993)\n\n\n\u001b[1m   --> STEP: 279/406 -- GLOBAL_STEP: 16925\u001b[0m\n     | > loss: 0.10594  (0.08378)\n     | > log_mle: -0.18655  (-0.18220)\n     | > loss_dur: 0.29249  (0.26598)\n     | > amp_scaler: 8192.00000  (4888.77419)\n     | > grad_norm: 11.93240  (15.30984)\n     | > current_lr: 0.00001 \n     | > step_time: 1.48490  (1.16257)\n     | > loader_time: 0.01730  (0.01021)\n\n\n\u001b[1m   --> STEP: 304/406 -- GLOBAL_STEP: 16950\u001b[0m\n     | > loss: 0.07404  (0.08369)\n     | > log_mle: -0.19920  (-0.18329)\n     | > loss_dur: 0.27323  (0.26698)\n     | > amp_scaler: 8192.00000  (5160.42105)\n     | > grad_norm: 16.38988  (15.63112)\n     | > current_lr: 0.00001 \n     | > step_time: 1.69230  (1.19333)\n     | > loader_time: 0.01600  (0.01091)\n\n\n\u001b[1m   --> STEP: 329/406 -- GLOBAL_STEP: 16975\u001b[0m\n     | > loss: 0.07433  (0.08385)\n     | > log_mle: -0.18672  (-0.18408)\n     | > loss_dur: 0.26105  (0.26793)\n     | > amp_scaler: 8192.00000  (5390.78419)\n     | > grad_norm: 29.44160  (15.81623)\n     | > current_lr: 0.00001 \n     | > step_time: 1.69010  (1.22464)\n     | > loader_time: 0.01760  (0.01184)\n\n\n\u001b[1m   --> STEP: 354/406 -- GLOBAL_STEP: 17000\u001b[0m\n     | > loss: 0.08631  (0.08394)\n     | > log_mle: -0.19367  (-0.18502)\n     | > loss_dur: 0.27999  (0.26897)\n     | > amp_scaler: 8192.00000  (5588.61017)\n     | > grad_norm: 24.78215  (16.42353)\n     | > current_lr: 0.00001 \n     | > step_time: 1.63060  (1.26227)\n     | > loader_time: 0.02010  (0.01248)\n\n\n\u001b[1m   --> STEP: 379/406 -- GLOBAL_STEP: 17025\u001b[0m\n     | > loss: 0.07299  (0.08357)\n     | > log_mle: -0.20317  (-0.18601)\n     | > loss_dur: 0.27616  (0.26958)\n     | > amp_scaler: 8192.00000  (5760.33773)\n     | > grad_norm: 12.60970  (16.65786)\n     | > current_lr: 0.00001 \n     | > step_time: 1.80950  (1.29588)\n     | > loader_time: 0.02190  (0.01296)\n\n\n\u001b[1m   --> STEP: 404/406 -- GLOBAL_STEP: 17050\u001b[0m\n     | > loss: 0.08818  (0.08330)\n     | > log_mle: -0.19857  (-0.18690)\n     | > loss_dur: 0.28674  (0.27019)\n     | > amp_scaler: 8192.00000  (5910.81188)\n     | > grad_norm: 23.51313  (16.98921)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05250  (1.31572)\n     | > loader_time: 0.00970  (0.01309)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00733 \u001b[0m(-0.00506)\n     | > avg_loss:\u001b[92m 0.12999 \u001b[0m(-0.01585)\n     | > avg_log_mle:\u001b[92m -0.12913 \u001b[0m(-0.01231)\n     | > avg_loss_dur:\u001b[92m 0.25912 \u001b[0m(-0.00354)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_17052.pth\n\n\u001b[4m\u001b[1m > EPOCH: 42/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:13:55) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 23/406 -- GLOBAL_STEP: 17075\u001b[0m\n     | > loss: 0.07994  (0.07091)\n     | > log_mle: -0.16504  (-0.16710)\n     | > loss_dur: 0.24498  (0.23801)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 9.88915  (9.10573)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99170  (0.76511)\n     | > loader_time: 0.00830  (0.00543)\n\n\n\u001b[1m   --> STEP: 48/406 -- GLOBAL_STEP: 17100\u001b[0m\n     | > loss: 0.08308  (0.07765)\n     | > log_mle: -0.17194  (-0.16723)\n     | > loss_dur: 0.25502  (0.24489)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 23.66413  (9.44966)\n     | > current_lr: 0.00001 \n     | > step_time: 1.03320  (0.82206)\n     | > loader_time: 0.00600  (0.00610)\n\n\n\u001b[1m   --> STEP: 73/406 -- GLOBAL_STEP: 17125\u001b[0m\n     | > loss: 0.05941  (0.07818)\n     | > log_mle: -0.19413  (-0.17044)\n     | > loss_dur: 0.25355  (0.24862)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 6.28910  (10.68190)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93390  (0.87240)\n     | > loader_time: 0.00880  (0.00720)\n\n\n\u001b[1m   --> STEP: 98/406 -- GLOBAL_STEP: 17150\u001b[0m\n     | > loss: 0.08900  (0.07816)\n     | > log_mle: -0.17604  (-0.17368)\n     | > loss_dur: 0.26504  (0.25184)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 7.32569  (12.31801)\n     | > current_lr: 0.00001 \n     | > step_time: 1.53250  (0.91497)\n     | > loader_time: 0.00760  (0.00779)\n\n\n\u001b[1m   --> STEP: 123/406 -- GLOBAL_STEP: 17175\u001b[0m\n     | > loss: 0.08057  (0.07732)\n     | > log_mle: -0.19308  (-0.17649)\n     | > loss_dur: 0.27365  (0.25381)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 6.95324  (12.77282)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17590  (0.95524)\n     | > loader_time: 0.00680  (0.00787)\n\n\n\u001b[1m   --> STEP: 148/406 -- GLOBAL_STEP: 17200\u001b[0m\n     | > loss: 0.07555  (0.07709)\n     | > log_mle: -0.19929  (-0.17885)\n     | > loss_dur: 0.27483  (0.25594)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 26.79339  (13.35442)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16830  (0.99233)\n     | > loader_time: 0.00770  (0.00805)\n\n\n\u001b[1m   --> STEP: 173/406 -- GLOBAL_STEP: 17225\u001b[0m\n     | > loss: 0.08490  (0.07713)\n     | > log_mle: -0.18921  (-0.18063)\n     | > loss_dur: 0.27411  (0.25776)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 20.62811  (14.28016)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16610  (1.02705)\n     | > loader_time: 0.00780  (0.00833)\n\n\n\u001b[1m   --> STEP: 198/406 -- GLOBAL_STEP: 17250\u001b[0m\n     | > loss: 0.07640  (0.07707)\n     | > log_mle: -0.19635  (-0.18240)\n     | > loss_dur: 0.27275  (0.25947)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 27.46706  (15.01845)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16860  (1.05715)\n     | > loader_time: 0.00810  (0.00847)\n\n\n\u001b[1m   --> STEP: 223/406 -- GLOBAL_STEP: 17275\u001b[0m\n     | > loss: 0.07056  (0.07659)\n     | > log_mle: -0.19223  (-0.18399)\n     | > loss_dur: 0.26279  (0.26058)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 18.68929  (15.67484)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25650  (1.08609)\n     | > loader_time: 0.01120  (0.00896)\n\n\n\u001b[1m   --> STEP: 248/406 -- GLOBAL_STEP: 17300\u001b[0m\n     | > loss: 0.06745  (0.07662)\n     | > log_mle: -0.19374  (-0.18544)\n     | > loss_dur: 0.26119  (0.26206)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 21.51094  (16.07522)\n     | > current_lr: 0.00001 \n     | > step_time: 1.34390  (1.11416)\n     | > loader_time: 0.00890  (0.00914)\n\n\n\u001b[1m   --> STEP: 273/406 -- GLOBAL_STEP: 17325\u001b[0m\n     | > loss: 0.06179  (0.07622)\n     | > log_mle: -0.20714  (-0.18678)\n     | > loss_dur: 0.26893  (0.26300)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 14.27569  (16.20105)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31660  (1.14586)\n     | > loader_time: 0.01390  (0.00953)\n\n\n\u001b[1m   --> STEP: 298/406 -- GLOBAL_STEP: 17350\u001b[0m\n     | > loss: 0.08609  (0.07611)\n     | > log_mle: -0.18659  (-0.18772)\n     | > loss_dur: 0.27268  (0.26383)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 7.17084  (16.03627)\n     | > current_lr: 0.00001 \n     | > step_time: 1.63420  (1.17397)\n     | > loader_time: 0.01660  (0.01016)\n\n\n\u001b[1m   --> STEP: 323/406 -- GLOBAL_STEP: 17375\u001b[0m\n     | > loss: 0.06105  (0.07605)\n     | > log_mle: -0.20284  (-0.18869)\n     | > loss_dur: 0.26390  (0.26474)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 25.85154  (16.37889)\n     | > current_lr: 0.00001 \n     | > step_time: 1.44810  (1.20278)\n     | > loader_time: 0.01800  (0.01076)\n\n\n\u001b[1m   --> STEP: 348/406 -- GLOBAL_STEP: 17400\u001b[0m\n     | > loss: 0.07682  (0.07613)\n     | > log_mle: -0.19263  (-0.18954)\n     | > loss_dur: 0.26945  (0.26567)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 50.40665  (17.42793)\n     | > current_lr: 0.00001 \n     | > step_time: 1.85630  (1.23577)\n     | > loader_time: 0.02210  (0.01153)\n\n\n\u001b[1m   --> STEP: 373/406 -- GLOBAL_STEP: 17425\u001b[0m\n     | > loss: 0.05938  (0.07584)\n     | > log_mle: -0.20612  (-0.19050)\n     | > loss_dur: 0.26550  (0.26634)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 18.77343  (18.27225)\n     | > current_lr: 0.00001 \n     | > step_time: 1.75980  (1.26917)\n     | > loader_time: 0.03690  (0.01223)\n\n\n\u001b[1m   --> STEP: 398/406 -- GLOBAL_STEP: 17450\u001b[0m\n     | > loss: 0.05522  (0.07554)\n     | > log_mle: -0.21113  (-0.19141)\n     | > loss_dur: 0.26634  (0.26695)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 27.10267  (18.42775)\n     | > current_lr: 0.00001 \n     | > step_time: 1.04270  (1.30019)\n     | > loader_time: 0.00830  (0.01257)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00959 \u001b[0m(+0.00226)\n     | > avg_loss:\u001b[91m 0.13155 \u001b[0m(+0.00156)\n     | > avg_log_mle:\u001b[91m -0.12588 \u001b[0m(+0.00325)\n     | > avg_loss_dur:\u001b[92m 0.25743 \u001b[0m(-0.00169)\n\n\n\u001b[4m\u001b[1m > EPOCH: 43/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:23:27) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 17/406 -- GLOBAL_STEP: 17475\u001b[0m\n     | > loss: 0.07047  (0.05826)\n     | > log_mle: -0.17054  (-0.17397)\n     | > loss_dur: 0.24100  (0.23223)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 7.10043  (12.13435)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75230  (0.75582)\n     | > loader_time: 0.00840  (0.00640)\n\n\n\u001b[1m   --> STEP: 42/406 -- GLOBAL_STEP: 17500\u001b[0m\n     | > loss: 0.06505  (0.06651)\n     | > log_mle: -0.17993  (-0.17171)\n     | > loss_dur: 0.24498  (0.23822)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 8.32498  (12.12287)\n     | > current_lr: 0.00001 \n     | > step_time: 0.87030  (0.82098)\n     | > loader_time: 0.00600  (0.00684)\n\n\n\u001b[1m   --> STEP: 67/406 -- GLOBAL_STEP: 17525\u001b[0m\n     | > loss: 0.05815  (0.06909)\n     | > log_mle: -0.18858  (-0.17437)\n     | > loss_dur: 0.24673  (0.24345)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 8.43071  (11.92067)\n     | > current_lr: 0.00001 \n     | > step_time: 0.87130  (0.86329)\n     | > loader_time: 0.00660  (0.00693)\n\n\n\u001b[1m   --> STEP: 92/406 -- GLOBAL_STEP: 17550\u001b[0m\n     | > loss: 0.06427  (0.06957)\n     | > log_mle: -0.19960  (-0.17773)\n     | > loss_dur: 0.26387  (0.24729)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 11.77278  (12.53722)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09840  (0.91053)\n     | > loader_time: 0.01580  (0.00775)\n\n\n\u001b[1m   --> STEP: 117/406 -- GLOBAL_STEP: 17575\u001b[0m\n     | > loss: 0.06225  (0.06890)\n     | > log_mle: -0.19191  (-0.18066)\n     | > loss_dur: 0.25416  (0.24956)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 22.91154  (13.62670)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09870  (0.95360)\n     | > loader_time: 0.03720  (0.00846)\n\n\n\u001b[1m   --> STEP: 142/406 -- GLOBAL_STEP: 17600\u001b[0m\n     | > loss: 0.06416  (0.06885)\n     | > log_mle: -0.19901  (-0.18313)\n     | > loss_dur: 0.26317  (0.25198)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 20.31470  (14.83292)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18380  (0.99301)\n     | > loader_time: 0.02710  (0.00871)\n\n\n\u001b[1m   --> STEP: 167/406 -- GLOBAL_STEP: 17625\u001b[0m\n     | > loss: 0.07011  (0.06893)\n     | > log_mle: -0.19877  (-0.18487)\n     | > loss_dur: 0.26888  (0.25380)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 18.96891  (15.15853)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16900  (1.02780)\n     | > loader_time: 0.00840  (0.00902)\n\n\n\u001b[1m   --> STEP: 192/406 -- GLOBAL_STEP: 17650\u001b[0m\n     | > loss: 0.06069  (0.06888)\n     | > log_mle: -0.19777  (-0.18655)\n     | > loss_dur: 0.25846  (0.25543)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 24.10303  (15.59679)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25460  (1.06029)\n     | > loader_time: 0.00950  (0.00919)\n\n\n\u001b[1m   --> STEP: 217/406 -- GLOBAL_STEP: 17675\u001b[0m\n     | > loss: 0.06026  (0.06896)\n     | > log_mle: -0.20004  (-0.18784)\n     | > loss_dur: 0.26031  (0.25679)\n     | > amp_scaler: 4096.00000  (7984.36866)\n     | > grad_norm: 31.48521  (17.02515)\n     | > current_lr: 0.00001 \n     | > step_time: 1.35150  (1.09071)\n     | > loader_time: 0.00890  (0.00952)\n\n\n\u001b[1m   --> STEP: 242/406 -- GLOBAL_STEP: 17700\u001b[0m\n     | > loss: 0.05137  (0.06902)\n     | > log_mle: -0.21763  (-0.18935)\n     | > loss_dur: 0.26900  (0.25836)\n     | > amp_scaler: 4096.00000  (7582.67769)\n     | > grad_norm: 47.18741  (17.79608)\n     | > current_lr: 0.00001 \n     | > step_time: 1.32740  (1.11739)\n     | > loader_time: 0.00750  (0.00965)\n\n\n\u001b[1m   --> STEP: 267/406 -- GLOBAL_STEP: 17725\u001b[0m\n     | > loss: 0.06300  (0.06884)\n     | > log_mle: -0.20318  (-0.19042)\n     | > loss_dur: 0.26618  (0.25926)\n     | > amp_scaler: 4096.00000  (7256.20974)\n     | > grad_norm: 32.90409  (18.05087)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28460  (1.14174)\n     | > loader_time: 0.00850  (0.00968)\n\n\n\u001b[1m   --> STEP: 292/406 -- GLOBAL_STEP: 17750\u001b[0m\n     | > loss: 0.07135  (0.06855)\n     | > log_mle: -0.19889  (-0.19150)\n     | > loss_dur: 0.27025  (0.26005)\n     | > amp_scaler: 4096.00000  (6985.64384)\n     | > grad_norm: 40.71008  (18.55292)\n     | > current_lr: 0.00001 \n     | > step_time: 1.69850  (1.17051)\n     | > loader_time: 0.00950  (0.00988)\n\n\n\u001b[1m   --> STEP: 317/406 -- GLOBAL_STEP: 17775\u001b[0m\n     | > loss: 0.07580  (0.06851)\n     | > log_mle: -0.19826  (-0.19255)\n     | > loss_dur: 0.27406  (0.26106)\n     | > amp_scaler: 4096.00000  (6757.75394)\n     | > grad_norm: 25.72182  (18.97788)\n     | > current_lr: 0.00001 \n     | > step_time: 1.68050  (1.19701)\n     | > loader_time: 0.01280  (0.01017)\n\n\n\u001b[1m   --> STEP: 342/406 -- GLOBAL_STEP: 17800\u001b[0m\n     | > loss: 0.04713  (0.06861)\n     | > log_mle: -0.20937  (-0.19342)\n     | > loss_dur: 0.25650  (0.26203)\n     | > amp_scaler: 4096.00000  (6563.18129)\n     | > grad_norm: 23.91712  (19.14475)\n     | > current_lr: 0.00001 \n     | > step_time: 1.63150  (1.22719)\n     | > loader_time: 0.02480  (0.01031)\n\n\n\u001b[1m   --> STEP: 367/406 -- GLOBAL_STEP: 17825\u001b[0m\n     | > loss: 0.06093  (0.06825)\n     | > log_mle: -0.20419  (-0.19443)\n     | > loss_dur: 0.26513  (0.26268)\n     | > amp_scaler: 4096.00000  (6395.11717)\n     | > grad_norm: 21.42726  (19.51624)\n     | > current_lr: 0.00001 \n     | > step_time: 1.80120  (1.25710)\n     | > loader_time: 0.01170  (0.01051)\n\n\n\u001b[1m   --> STEP: 392/406 -- GLOBAL_STEP: 17850\u001b[0m\n     | > loss: 0.06324  (0.06811)\n     | > log_mle: -0.20864  (-0.19532)\n     | > loss_dur: 0.27188  (0.26343)\n     | > amp_scaler: 4096.00000  (6248.48980)\n     | > grad_norm: 18.41723  (19.71573)\n     | > current_lr: 0.00001 \n     | > step_time: 1.81170  (1.28640)\n     | > loader_time: 0.01270  (0.01089)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00948 \u001b[0m(-0.00010)\n     | > avg_loss:\u001b[92m 0.12775 \u001b[0m(-0.00380)\n     | > avg_log_mle:\u001b[92m -0.12704 \u001b[0m(-0.00116)\n     | > avg_loss_dur:\u001b[92m 0.25479 \u001b[0m(-0.00264)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_17864.pth\n\n\u001b[4m\u001b[1m > EPOCH: 44/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:32:57) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 11/406 -- GLOBAL_STEP: 17875\u001b[0m\n     | > loss: 0.03812  (0.05098)\n     | > log_mle: -0.17752  (-0.17672)\n     | > loss_dur: 0.21564  (0.22770)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 2.83132  (7.48184)\n     | > current_lr: 0.00001 \n     | > step_time: 1.43420  (0.81436)\n     | > loader_time: 0.00510  (0.00607)\n\n\n\u001b[1m   --> STEP: 36/406 -- GLOBAL_STEP: 17900\u001b[0m\n     | > loss: 0.06135  (0.05825)\n     | > log_mle: -0.16888  (-0.17623)\n     | > loss_dur: 0.23023  (0.23448)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 10.05108  (10.71478)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91120  (0.81827)\n     | > loader_time: 0.01520  (0.00644)\n\n\n\u001b[1m   --> STEP: 61/406 -- GLOBAL_STEP: 17925\u001b[0m\n     | > loss: 0.08423  (0.06138)\n     | > log_mle: -0.18373  (-0.17821)\n     | > loss_dur: 0.26796  (0.23959)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.17935  (12.67417)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97610  (0.87277)\n     | > loader_time: 0.00560  (0.00720)\n\n\n\u001b[1m   --> STEP: 86/406 -- GLOBAL_STEP: 17950\u001b[0m\n     | > loss: 0.07780  (0.06140)\n     | > log_mle: -0.18447  (-0.18126)\n     | > loss_dur: 0.26227  (0.24266)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.01953  (13.94297)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93860  (0.91054)\n     | > loader_time: 0.00610  (0.00740)\n\n\n\u001b[1m   --> STEP: 111/406 -- GLOBAL_STEP: 17975\u001b[0m\n     | > loss: 0.05073  (0.06079)\n     | > log_mle: -0.20832  (-0.18443)\n     | > loss_dur: 0.25905  (0.24522)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.62172  (14.39352)\n     | > current_lr: 0.00001 \n     | > step_time: 1.55470  (0.94884)\n     | > loader_time: 0.01990  (0.00763)\n\n\n\u001b[1m   --> STEP: 136/406 -- GLOBAL_STEP: 18000\u001b[0m\n     | > loss: 0.07102  (0.06087)\n     | > log_mle: -0.20148  (-0.18686)\n     | > loss_dur: 0.27249  (0.24773)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.05282  (14.34830)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02880  (0.97811)\n     | > loader_time: 0.00700  (0.00791)\n\n\n\u001b[1m   --> STEP: 161/406 -- GLOBAL_STEP: 18025\u001b[0m\n     | > loss: 0.08106  (0.06173)\n     | > log_mle: -0.18826  (-0.18854)\n     | > loss_dur: 0.26932  (0.25027)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.18290  (14.94913)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20000  (1.01602)\n     | > loader_time: 0.00720  (0.00819)\n\n\n\u001b[1m   --> STEP: 186/406 -- GLOBAL_STEP: 18050\u001b[0m\n     | > loss: 0.05684  (0.06175)\n     | > log_mle: -0.20745  (-0.19038)\n     | > loss_dur: 0.26429  (0.25213)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.84059  (16.05302)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25740  (1.05067)\n     | > loader_time: 0.01150  (0.00874)\n\n\n\u001b[1m   --> STEP: 211/406 -- GLOBAL_STEP: 18075\u001b[0m\n     | > loss: 0.03622  (0.06171)\n     | > log_mle: -0.21606  (-0.19187)\n     | > loss_dur: 0.25228  (0.25357)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 31.54627  (16.52375)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20590  (1.07936)\n     | > loader_time: 0.00780  (0.00896)\n\n\n\u001b[1m   --> STEP: 236/406 -- GLOBAL_STEP: 18100\u001b[0m\n     | > loss: 0.07259  (0.06168)\n     | > log_mle: -0.19273  (-0.19339)\n     | > loss_dur: 0.26532  (0.25507)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.97636  (16.78175)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24880  (1.10595)\n     | > loader_time: 0.00970  (0.00924)\n\n\n\u001b[1m   --> STEP: 261/406 -- GLOBAL_STEP: 18125\u001b[0m\n     | > loss: 0.04481  (0.06126)\n     | > log_mle: -0.20530  (-0.19480)\n     | > loss_dur: 0.25011  (0.25606)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 28.77971  (17.62258)\n     | > current_lr: 0.00001 \n     | > step_time: 1.35980  (1.13352)\n     | > loader_time: 0.04390  (0.00968)\n\n\n\u001b[1m   --> STEP: 286/406 -- GLOBAL_STEP: 18150\u001b[0m\n     | > loss: 0.05694  (0.06090)\n     | > log_mle: -0.21066  (-0.19599)\n     | > loss_dur: 0.26759  (0.25689)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.79598  (18.25219)\n     | > current_lr: 0.00001 \n     | > step_time: 1.58900  (1.16208)\n     | > loader_time: 0.00810  (0.01008)\n\n\n\u001b[1m   --> STEP: 311/406 -- GLOBAL_STEP: 18175\u001b[0m\n     | > loss: 0.05528  (0.06084)\n     | > log_mle: -0.21550  (-0.19698)\n     | > loss_dur: 0.27078  (0.25782)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.72932  (18.69762)\n     | > current_lr: 0.00001 \n     | > step_time: 1.47370  (1.19211)\n     | > loader_time: 0.01790  (0.01079)\n\n\n\u001b[1m   --> STEP: 336/406 -- GLOBAL_STEP: 18200\u001b[0m\n     | > loss: 0.05809  (0.06097)\n     | > log_mle: -0.19931  (-0.19781)\n     | > loss_dur: 0.25740  (0.25878)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.51031  (19.06530)\n     | > current_lr: 0.00001 \n     | > step_time: 1.49780  (1.22406)\n     | > loader_time: 0.01770  (0.01168)\n\n\n\u001b[1m   --> STEP: 361/406 -- GLOBAL_STEP: 18225\u001b[0m\n     | > loss: 0.06344  (0.06086)\n     | > log_mle: -0.20376  (-0.19871)\n     | > loss_dur: 0.26719  (0.25957)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.03360  (19.86080)\n     | > current_lr: 0.00001 \n     | > step_time: 1.64770  (1.25992)\n     | > loader_time: 0.02110  (0.01238)\n\n\n\u001b[1m   --> STEP: 386/406 -- GLOBAL_STEP: 18250\u001b[0m\n     | > loss: 0.07145  (0.06079)\n     | > log_mle: -0.21017  (-0.19946)\n     | > loss_dur: 0.28162  (0.26025)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.25766  (20.36420)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40900  (1.28964)\n     | > loader_time: 0.00770  (0.01296)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00716 \u001b[0m(-0.00232)\n     | > avg_loss:\u001b[92m 0.10167 \u001b[0m(-0.02609)\n     | > avg_log_mle:\u001b[92m -0.14934 \u001b[0m(-0.02230)\n     | > avg_loss_dur:\u001b[92m 0.25100 \u001b[0m(-0.00379)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_18270.pth\n\n\u001b[4m\u001b[1m > EPOCH: 45/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:42:17) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 5/406 -- GLOBAL_STEP: 18275\u001b[0m\n     | > loss: 0.07239  (0.05380)\n     | > log_mle: -0.16844  (-0.17778)\n     | > loss_dur: 0.24083  (0.23158)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.72316  (9.24626)\n     | > current_lr: 0.00001 \n     | > step_time: 0.48790  (0.51243)\n     | > loader_time: 0.00320  (0.00370)\n\n\n\u001b[1m   --> STEP: 30/406 -- GLOBAL_STEP: 18300\u001b[0m\n     | > loss: 0.07135  (0.05418)\n     | > log_mle: -0.18422  (-0.17853)\n     | > loss_dur: 0.25558  (0.23271)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.78444  (13.02057)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60120  (0.56110)\n     | > loader_time: 0.00680  (0.00451)\n\n\n\u001b[1m   --> STEP: 55/406 -- GLOBAL_STEP: 18325\u001b[0m\n     | > loss: 0.05570  (0.05843)\n     | > log_mle: -0.19107  (-0.17932)\n     | > loss_dur: 0.24676  (0.23776)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.30413  (13.98571)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72990  (0.59315)\n     | > loader_time: 0.00400  (0.00553)\n\n\n\u001b[1m   --> STEP: 80/406 -- GLOBAL_STEP: 18350\u001b[0m\n     | > loss: 0.05295  (0.05793)\n     | > log_mle: -0.17826  (-0.18233)\n     | > loss_dur: 0.23121  (0.24026)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.25098  (14.80744)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66820  (0.61513)\n     | > loader_time: 0.00770  (0.00597)\n\n\n\u001b[1m   --> STEP: 105/406 -- GLOBAL_STEP: 18375\u001b[0m\n     | > loss: 0.04461  (0.05730)\n     | > log_mle: -0.18904  (-0.18601)\n     | > loss_dur: 0.23365  (0.24332)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 21.71599  (15.19353)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68310  (0.64241)\n     | > loader_time: 0.00460  (0.00644)\n\n\n\u001b[1m   --> STEP: 130/406 -- GLOBAL_STEP: 18400\u001b[0m\n     | > loss: 0.04262  (0.05633)\n     | > log_mle: -0.20287  (-0.18888)\n     | > loss_dur: 0.24549  (0.24522)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 10.57392  (16.32373)\n     | > current_lr: 0.00001 \n     | > step_time: 0.76460  (0.65978)\n     | > loader_time: 0.00700  (0.00643)\n\n\n\u001b[1m   --> STEP: 155/406 -- GLOBAL_STEP: 18425\u001b[0m\n     | > loss: 0.06964  (0.05658)\n     | > log_mle: -0.19455  (-0.19093)\n     | > loss_dur: 0.26419  (0.24751)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.24764  (16.98406)\n     | > current_lr: 0.00001 \n     | > step_time: 0.83070  (0.68090)\n     | > loader_time: 0.00500  (0.00658)\n\n\n\u001b[1m   --> STEP: 180/406 -- GLOBAL_STEP: 18450\u001b[0m\n     | > loss: 0.05106  (0.05630)\n     | > log_mle: -0.20727  (-0.19280)\n     | > loss_dur: 0.25833  (0.24910)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.77123  (17.74592)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84300  (0.70049)\n     | > loader_time: 0.00810  (0.00682)\n\n\n\u001b[1m   --> STEP: 205/406 -- GLOBAL_STEP: 18475\u001b[0m\n     | > loss: 0.05236  (0.05632)\n     | > log_mle: -0.21659  (-0.19449)\n     | > loss_dur: 0.26895  (0.25080)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 47.14865  (18.20809)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92830  (0.72057)\n     | > loader_time: 0.00790  (0.00681)\n\n\n\u001b[1m   --> STEP: 230/406 -- GLOBAL_STEP: 18500\u001b[0m\n     | > loss: 0.05407  (0.05613)\n     | > log_mle: -0.20894  (-0.19608)\n     | > loss_dur: 0.26301  (0.25221)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.03189  (19.07198)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93080  (0.74500)\n     | > loader_time: 0.00980  (0.00700)\n\n\n\u001b[1m   --> STEP: 255/406 -- GLOBAL_STEP: 18525\u001b[0m\n     | > loss: 0.06197  (0.05593)\n     | > log_mle: -0.20599  (-0.19747)\n     | > loss_dur: 0.26796  (0.25340)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.39157  (20.07095)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93680  (0.77198)\n     | > loader_time: 0.00770  (0.00715)\n\n\n\u001b[1m   --> STEP: 280/406 -- GLOBAL_STEP: 18550\u001b[0m\n     | > loss: 0.03609  (0.05544)\n     | > log_mle: -0.21848  (-0.19865)\n     | > loss_dur: 0.25456  (0.25410)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.66775  (20.57795)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05740  (0.79954)\n     | > loader_time: 0.00650  (0.00740)\n\n\n\u001b[1m   --> STEP: 305/406 -- GLOBAL_STEP: 18575\u001b[0m\n     | > loss: 0.06692  (0.05558)\n     | > log_mle: -0.20813  (-0.19962)\n     | > loss_dur: 0.27505  (0.25520)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 31.71749  (21.41266)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16450  (0.82865)\n     | > loader_time: 0.01070  (0.00781)\n\n\n\u001b[1m   --> STEP: 330/406 -- GLOBAL_STEP: 18600\u001b[0m\n     | > loss: 0.05566  (0.05553)\n     | > log_mle: -0.22152  (-0.20045)\n     | > loss_dur: 0.27719  (0.25598)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 31.51047  (21.86290)\n     | > current_lr: 0.00001 \n     | > step_time: 1.26160  (0.85713)\n     | > loader_time: 0.01170  (0.00828)\n\n\n\u001b[1m   --> STEP: 355/406 -- GLOBAL_STEP: 18625\u001b[0m\n     | > loss: 0.04203  (0.05552)\n     | > log_mle: -0.22187  (-0.20136)\n     | > loss_dur: 0.26390  (0.25688)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 24.38360  (22.45507)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40760  (0.88546)\n     | > loader_time: 0.01550  (0.00881)\n\n\n\u001b[1m   --> STEP: 380/406 -- GLOBAL_STEP: 18650\u001b[0m\n     | > loss: 0.04685  (0.05515)\n     | > log_mle: -0.21150  (-0.20228)\n     | > loss_dur: 0.25835  (0.25743)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 36.19524  (22.53926)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28450  (0.91438)\n     | > loader_time: 0.02800  (0.00928)\n\n\n\u001b[1m   --> STEP: 405/406 -- GLOBAL_STEP: 18675\u001b[0m\n     | > loss: 0.03471  (0.05508)\n     | > log_mle: -0.21045  (-0.20302)\n     | > loss_dur: 0.24515  (0.25810)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.87045  (23.15174)\n     | > current_lr: 0.00001 \n     | > step_time: 0.41440  (0.93088)\n     | > loader_time: 0.00520  (0.00933)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00716 \u001b[0m(-0.00000)\n     | > avg_loss:\u001b[92m 0.06836 \u001b[0m(-0.03330)\n     | > avg_log_mle:\u001b[92m -0.17792 \u001b[0m(-0.02858)\n     | > avg_loss_dur:\u001b[92m 0.24628 \u001b[0m(-0.00472)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_18676.pth\n\n\u001b[4m\u001b[1m > EPOCH: 46/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:49:09) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 24/406 -- GLOBAL_STEP: 18700\u001b[0m\n     | > loss: 0.05088  (0.03899)\n     | > log_mle: -0.18867  (-0.18356)\n     | > loss_dur: 0.23956  (0.22255)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.87301  (11.85629)\n     | > current_lr: 0.00001 \n     | > step_time: 0.55630  (0.55095)\n     | > loader_time: 0.00310  (0.00764)\n\n\n\u001b[1m   --> STEP: 49/406 -- GLOBAL_STEP: 18725\u001b[0m\n     | > loss: 0.05568  (0.04857)\n     | > log_mle: -0.18610  (-0.18296)\n     | > loss_dur: 0.24178  (0.23153)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.84415  (16.63133)\n     | > current_lr: 0.00001 \n     | > step_time: 0.85140  (0.58749)\n     | > loader_time: 0.00450  (0.00755)\n\n\n\u001b[1m   --> STEP: 74/406 -- GLOBAL_STEP: 18750\u001b[0m\n     | > loss: 0.03459  (0.04944)\n     | > log_mle: -0.21381  (-0.18663)\n     | > loss_dur: 0.24840  (0.23607)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.17004  (17.09388)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68110  (0.60792)\n     | > loader_time: 0.01600  (0.00750)\n\n\n\u001b[1m   --> STEP: 99/406 -- GLOBAL_STEP: 18775\u001b[0m\n     | > loss: 0.05726  (0.04971)\n     | > log_mle: -0.19462  (-0.18949)\n     | > loss_dur: 0.25187  (0.23920)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.40868  (19.51507)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75570  (0.63443)\n     | > loader_time: 0.00900  (0.00774)\n\n\n\u001b[1m   --> STEP: 124/406 -- GLOBAL_STEP: 18800\u001b[0m\n     | > loss: 0.05801  (0.04916)\n     | > log_mle: -0.20463  (-0.19229)\n     | > loss_dur: 0.26264  (0.24145)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.71967  (20.44022)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82970  (0.65215)\n     | > loader_time: 0.00720  (0.00804)\n\n\n\u001b[1m   --> STEP: 149/406 -- GLOBAL_STEP: 18825\u001b[0m\n     | > loss: 0.07461  (0.04953)\n     | > log_mle: -0.21109  (-0.19454)\n     | > loss_dur: 0.28570  (0.24407)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.74301  (20.73689)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74210  (0.67339)\n     | > loader_time: 0.00420  (0.00851)\n\n\n\u001b[1m   --> STEP: 174/406 -- GLOBAL_STEP: 18850\u001b[0m\n     | > loss: 0.04773  (0.04968)\n     | > log_mle: -0.20208  (-0.19595)\n     | > loss_dur: 0.24981  (0.24563)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 40.12802  (20.77019)\n     | > current_lr: 0.00001 \n     | > step_time: 0.99300  (0.69598)\n     | > loader_time: 0.02210  (0.00872)\n\n\n\u001b[1m   --> STEP: 199/406 -- GLOBAL_STEP: 18875\u001b[0m\n     | > loss: 0.06330  (0.04986)\n     | > log_mle: -0.20012  (-0.19760)\n     | > loss_dur: 0.26342  (0.24746)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.35385  (20.57766)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98270  (0.71974)\n     | > loader_time: 0.00550  (0.00895)\n\n\n\u001b[1m   --> STEP: 224/406 -- GLOBAL_STEP: 18900\u001b[0m\n     | > loss: 0.02975  (0.04957)\n     | > log_mle: -0.20874  (-0.19915)\n     | > loss_dur: 0.23849  (0.24872)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.76906  (20.99430)\n     | > current_lr: 0.00001 \n     | > step_time: 1.07540  (0.74612)\n     | > loader_time: 0.00520  (0.00927)\n\n\n\u001b[1m   --> STEP: 249/406 -- GLOBAL_STEP: 18925\u001b[0m\n     | > loss: 0.05153  (0.04972)\n     | > log_mle: -0.20367  (-0.20053)\n     | > loss_dur: 0.25519  (0.25025)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.73323  (21.26691)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98780  (0.77229)\n     | > loader_time: 0.00540  (0.00944)\n\n\n\u001b[1m   --> STEP: 274/406 -- GLOBAL_STEP: 18950\u001b[0m\n     | > loss: 0.04925  (0.04939)\n     | > log_mle: -0.21575  (-0.20187)\n     | > loss_dur: 0.26500  (0.25126)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.30112  (20.98917)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02180  (0.79738)\n     | > loader_time: 0.01700  (0.00982)\n\n\n\u001b[1m   --> STEP: 299/406 -- GLOBAL_STEP: 18975\u001b[0m\n     | > loss: 0.02666  (0.04927)\n     | > log_mle: -0.23570  (-0.20284)\n     | > loss_dur: 0.26236  (0.25210)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 28.11202  (20.93820)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09270  (0.82170)\n     | > loader_time: 0.00560  (0.00992)\n\n\n\u001b[1m   --> STEP: 324/406 -- GLOBAL_STEP: 19000\u001b[0m\n     | > loss: 0.03758  (0.04933)\n     | > log_mle: -0.21504  (-0.20372)\n     | > loss_dur: 0.25262  (0.25305)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.86405  (21.59837)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15420  (0.84775)\n     | > loader_time: 0.01690  (0.01013)\n\n\n\u001b[1m   --> STEP: 349/406 -- GLOBAL_STEP: 19025\u001b[0m\n     | > loss: 0.08153  (0.04949)\n     | > log_mle: -0.21265  (-0.20462)\n     | > loss_dur: 0.29418  (0.25411)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 25.42273  (21.49666)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21210  (0.87500)\n     | > loader_time: 0.00820  (0.01026)\n\n\n\u001b[1m   --> STEP: 374/406 -- GLOBAL_STEP: 19050\u001b[0m\n     | > loss: 0.03758  (0.04902)\n     | > log_mle: -0.21741  (-0.20563)\n     | > loss_dur: 0.25500  (0.25465)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.77641  (21.91453)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42130  (0.90471)\n     | > loader_time: 0.01480  (0.01045)\n\n\n\u001b[1m   --> STEP: 399/406 -- GLOBAL_STEP: 19075\u001b[0m\n     | > loss: 0.05132  (0.04869)\n     | > log_mle: -0.22063  (-0.20658)\n     | > loss_dur: 0.27195  (0.25527)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.68774  (22.30558)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98760  (0.93156)\n     | > loader_time: 0.00820  (0.01067)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00697 \u001b[0m(-0.00019)\n     | > avg_loss:\u001b[91m 0.10490 \u001b[0m(+0.03654)\n     | > avg_log_mle:\u001b[91m -0.14109 \u001b[0m(+0.03683)\n     | > avg_loss_dur:\u001b[92m 0.24599 \u001b[0m(-0.00029)\n\n\n\u001b[4m\u001b[1m > EPOCH: 47/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 19:56:00) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 18/406 -- GLOBAL_STEP: 19100\u001b[0m\n     | > loss: 0.05821  (0.03356)\n     | > log_mle: -0.17902  (-0.18894)\n     | > loss_dur: 0.23723  (0.22250)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.55335  (14.45449)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60950  (0.54833)\n     | > loader_time: 0.00290  (0.00483)\n\n\n\u001b[1m   --> STEP: 43/406 -- GLOBAL_STEP: 19125\u001b[0m\n     | > loss: 0.04152  (0.04229)\n     | > log_mle: -0.17611  (-0.18570)\n     | > loss_dur: 0.21763  (0.22799)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.53048  (14.59967)\n     | > current_lr: 0.00001 \n     | > step_time: 0.58650  (0.57936)\n     | > loader_time: 0.00570  (0.00653)\n\n\n\u001b[1m   --> STEP: 68/406 -- GLOBAL_STEP: 19150\u001b[0m\n     | > loss: 0.06952  (0.04473)\n     | > log_mle: -0.19830  (-0.18851)\n     | > loss_dur: 0.26782  (0.23324)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.80825  (15.82283)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68470  (0.61434)\n     | > loader_time: 0.01750  (0.00746)\n\n\n\u001b[1m   --> STEP: 93/406 -- GLOBAL_STEP: 19175\u001b[0m\n     | > loss: 0.02694  (0.04486)\n     | > log_mle: -0.22114  (-0.19206)\n     | > loss_dur: 0.24808  (0.23691)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 33.94220  (17.47458)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90850  (0.64192)\n     | > loader_time: 0.01410  (0.00787)\n\n\n\u001b[1m   --> STEP: 118/406 -- GLOBAL_STEP: 19200\u001b[0m\n     | > loss: 0.04073  (0.04436)\n     | > log_mle: -0.20690  (-0.19508)\n     | > loss_dur: 0.24762  (0.23945)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.28263  (17.76616)\n     | > current_lr: 0.00001 \n     | > step_time: 0.76250  (0.65893)\n     | > loader_time: 0.00680  (0.00839)\n\n\n\u001b[1m   --> STEP: 143/406 -- GLOBAL_STEP: 19225\u001b[0m\n     | > loss: 0.02990  (0.04391)\n     | > log_mle: -0.21939  (-0.19793)\n     | > loss_dur: 0.24929  (0.24184)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.07535  (17.95727)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73820  (0.68311)\n     | > loader_time: 0.00580  (0.00891)\n\n\n\u001b[1m   --> STEP: 168/406 -- GLOBAL_STEP: 19250\u001b[0m\n     | > loss: 0.04214  (0.04416)\n     | > log_mle: -0.20088  (-0.19964)\n     | > loss_dur: 0.24302  (0.24380)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.17479  (19.69676)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93070  (0.70525)\n     | > loader_time: 0.01490  (0.00933)\n\n\n\u001b[1m   --> STEP: 193/406 -- GLOBAL_STEP: 19275\u001b[0m\n     | > loss: 0.04317  (0.04379)\n     | > log_mle: -0.21891  (-0.20152)\n     | > loss_dur: 0.26208  (0.24531)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 24.86361  (20.42340)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93040  (0.72979)\n     | > loader_time: 0.00670  (0.00979)\n\n\n\u001b[1m   --> STEP: 218/406 -- GLOBAL_STEP: 19300\u001b[0m\n     | > loss: 0.03219  (0.04346)\n     | > log_mle: -0.21455  (-0.20303)\n     | > loss_dur: 0.24674  (0.24649)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.60511  (21.07291)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91570  (0.75336)\n     | > loader_time: 0.00700  (0.01004)\n\n\n\u001b[1m   --> STEP: 243/406 -- GLOBAL_STEP: 19325\u001b[0m\n     | > loss: 0.03245  (0.04347)\n     | > log_mle: -0.23250  (-0.20476)\n     | > loss_dur: 0.26495  (0.24823)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 35.54704  (21.74419)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05280  (0.78060)\n     | > loader_time: 0.00670  (0.01034)\n\n\n\u001b[1m   --> STEP: 268/406 -- GLOBAL_STEP: 19350\u001b[0m\n     | > loss: 0.04165  (0.04301)\n     | > log_mle: -0.22315  (-0.20596)\n     | > loss_dur: 0.26480  (0.24897)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 40.29594  (22.15499)\n     | > current_lr: 0.00001 \n     | > step_time: 1.36940  (0.80711)\n     | > loader_time: 0.01880  (0.01047)\n\n\n\u001b[1m   --> STEP: 293/406 -- GLOBAL_STEP: 19375\u001b[0m\n     | > loss: 0.03610  (0.04266)\n     | > log_mle: -0.22368  (-0.20711)\n     | > loss_dur: 0.25978  (0.24976)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 34.58645  (22.66566)\n     | > current_lr: 0.00001 \n     | > step_time: 1.14130  (0.83371)\n     | > loader_time: 0.01670  (0.01081)\n\n\n\u001b[1m   --> STEP: 318/406 -- GLOBAL_STEP: 19400\u001b[0m\n     | > loss: 0.05259  (0.04263)\n     | > log_mle: -0.21643  (-0.20815)\n     | > loss_dur: 0.26903  (0.25078)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.05672  (22.83232)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25490  (0.86415)\n     | > loader_time: 0.01500  (0.01119)\n\n\n\u001b[1m   --> STEP: 343/406 -- GLOBAL_STEP: 19425\u001b[0m\n     | > loss: 0.04194  (0.04276)\n     | > log_mle: -0.23144  (-0.20905)\n     | > loss_dur: 0.27338  (0.25181)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.33214  (23.16988)\n     | > current_lr: 0.00001 \n     | > step_time: 1.24350  (0.89303)\n     | > loader_time: 0.01610  (0.01162)\n\n\n\u001b[1m   --> STEP: 368/406 -- GLOBAL_STEP: 19450\u001b[0m\n     | > loss: 0.03657  (0.04227)\n     | > log_mle: -0.23032  (-0.21003)\n     | > loss_dur: 0.26688  (0.25231)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.90124  (23.53580)\n     | > current_lr: 0.00001 \n     | > step_time: 1.71140  (0.92533)\n     | > loader_time: 0.01640  (0.01198)\n\n\n\u001b[1m   --> STEP: 393/406 -- GLOBAL_STEP: 19475\u001b[0m\n     | > loss: 0.02927  (0.04205)\n     | > log_mle: -0.22159  (-0.21088)\n     | > loss_dur: 0.25086  (0.25293)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.71257  (23.84515)\n     | > current_lr: 0.00001 \n     | > step_time: 1.44070  (0.95585)\n     | > loader_time: 0.01660  (0.01224)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00686 \u001b[0m(-0.00010)\n     | > avg_loss:\u001b[92m 0.09389 \u001b[0m(-0.01102)\n     | > avg_log_mle:\u001b[92m -0.15257 \u001b[0m(-0.01148)\n     | > avg_loss_dur:\u001b[91m 0.24646 \u001b[0m(+0.00046)\n\n\n\u001b[4m\u001b[1m > EPOCH: 48/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:03:07) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 12/406 -- GLOBAL_STEP: 19500\u001b[0m\n     | > loss: 0.02365  (0.01857)\n     | > log_mle: -0.19922  (-0.19276)\n     | > loss_dur: 0.22287  (0.21132)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 12.07992  (15.16781)\n     | > current_lr: 0.00001 \n     | > step_time: 0.55020  (0.55230)\n     | > loader_time: 0.00920  (0.00695)\n\n\n\u001b[1m   --> STEP: 37/406 -- GLOBAL_STEP: 19525\u001b[0m\n     | > loss: 0.04222  (0.03071)\n     | > log_mle: -0.18636  (-0.19114)\n     | > loss_dur: 0.22857  (0.22185)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.97223  (16.11736)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63420  (0.58753)\n     | > loader_time: 0.00490  (0.00754)\n\n\n\u001b[1m   --> STEP: 62/406 -- GLOBAL_STEP: 19550\u001b[0m\n     | > loss: 0.05071  (0.03626)\n     | > log_mle: -0.18289  (-0.19229)\n     | > loss_dur: 0.23361  (0.22855)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 8.78602  (16.67842)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68300  (0.61204)\n     | > loader_time: 0.00520  (0.00826)\n\n\n\u001b[1m   --> STEP: 87/406 -- GLOBAL_STEP: 19575\u001b[0m\n     | > loss: 0.03784  (0.03692)\n     | > log_mle: -0.22314  (-0.19556)\n     | > loss_dur: 0.26099  (0.23249)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.13157  (17.18225)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72260  (0.63461)\n     | > loader_time: 0.00600  (0.00874)\n\n\n\u001b[1m   --> STEP: 112/406 -- GLOBAL_STEP: 19600\u001b[0m\n     | > loss: 0.02487  (0.03656)\n     | > log_mle: -0.21772  (-0.19870)\n     | > loss_dur: 0.24259  (0.23526)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 36.36127  (18.29881)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70590  (0.65241)\n     | > loader_time: 0.01410  (0.00936)\n\n\n\u001b[1m   --> STEP: 137/406 -- GLOBAL_STEP: 19625\u001b[0m\n     | > loss: 0.05776  (0.03646)\n     | > log_mle: -0.21160  (-0.20127)\n     | > loss_dur: 0.26936  (0.23774)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.36354  (19.55600)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79800  (0.67778)\n     | > loader_time: 0.01690  (0.00990)\n\n\n\u001b[1m   --> STEP: 162/406 -- GLOBAL_STEP: 19650\u001b[0m\n     | > loss: 0.02516  (0.03652)\n     | > log_mle: -0.23191  (-0.20319)\n     | > loss_dur: 0.25707  (0.23971)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 25.99561  (20.15944)\n     | > current_lr: 0.00001 \n     | > step_time: 0.89430  (0.70217)\n     | > loader_time: 0.00480  (0.01034)\n\n\n\u001b[1m   --> STEP: 187/406 -- GLOBAL_STEP: 19675\u001b[0m\n     | > loss: 0.04174  (0.03629)\n     | > log_mle: -0.21747  (-0.20495)\n     | > loss_dur: 0.25921  (0.24124)\n     | > amp_scaler: 8192.00000  (4336.94118)\n     | > grad_norm: 16.80011  (20.23231)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95220  (0.72269)\n     | > loader_time: 0.00470  (0.01036)\n\n\n\u001b[1m   --> STEP: 212/406 -- GLOBAL_STEP: 19700\u001b[0m\n     | > loss: 0.04707  (0.03622)\n     | > log_mle: -0.22143  (-0.20642)\n     | > loss_dur: 0.26849  (0.24264)\n     | > amp_scaler: 8192.00000  (4791.54717)\n     | > grad_norm: 28.16204  (21.02350)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97030  (0.74858)\n     | > loader_time: 0.00510  (0.01039)\n\n\n\u001b[1m   --> STEP: 237/406 -- GLOBAL_STEP: 19725\u001b[0m\n     | > loss: 0.04218  (0.03629)\n     | > log_mle: -0.22811  (-0.20798)\n     | > loss_dur: 0.27029  (0.24427)\n     | > amp_scaler: 8192.00000  (5150.24473)\n     | > grad_norm: 32.24755  (21.95091)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01820  (0.77420)\n     | > loader_time: 0.00810  (0.01048)\n\n\n\u001b[1m   --> STEP: 262/406 -- GLOBAL_STEP: 19750\u001b[0m\n     | > loss: 0.03269  (0.03596)\n     | > log_mle: -0.21058  (-0.20935)\n     | > loss_dur: 0.24326  (0.24531)\n     | > amp_scaler: 8192.00000  (5440.48855)\n     | > grad_norm: 12.23542  (22.49126)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11950  (0.80083)\n     | > loader_time: 0.01300  (0.01068)\n\n\n\u001b[1m   --> STEP: 287/406 -- GLOBAL_STEP: 19775\u001b[0m\n     | > loss: 0.06446  (0.03578)\n     | > log_mle: -0.21321  (-0.21052)\n     | > loss_dur: 0.27767  (0.24630)\n     | > amp_scaler: 8192.00000  (5680.16725)\n     | > grad_norm: 27.87315  (22.90632)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11490  (0.82914)\n     | > loader_time: 0.01470  (0.01096)\n\n\n\u001b[1m   --> STEP: 312/406 -- GLOBAL_STEP: 19800\u001b[0m\n     | > loss: 0.03861  (0.03588)\n     | > log_mle: -0.21524  (-0.21148)\n     | > loss_dur: 0.25385  (0.24736)\n     | > amp_scaler: 8192.00000  (5881.43590)\n     | > grad_norm: 20.74170  (22.94642)\n     | > current_lr: 0.00001 \n     | > step_time: 1.52390  (0.85787)\n     | > loader_time: 0.03240  (0.01156)\n\n\n\u001b[1m   --> STEP: 337/406 -- GLOBAL_STEP: 19825\u001b[0m\n     | > loss: 0.03217  (0.03607)\n     | > log_mle: -0.22804  (-0.21229)\n     | > loss_dur: 0.26021  (0.24836)\n     | > amp_scaler: 8192.00000  (6052.84273)\n     | > grad_norm: 24.80147  (22.94944)\n     | > current_lr: 0.00001 \n     | > step_time: 1.49980  (0.88681)\n     | > loader_time: 0.03460  (0.01202)\n\n\n\u001b[1m   --> STEP: 362/406 -- GLOBAL_STEP: 19850\u001b[0m\n     | > loss: 0.02754  (0.03590)\n     | > log_mle: -0.23193  (-0.21321)\n     | > loss_dur: 0.25947  (0.24911)\n     | > amp_scaler: 8192.00000  (6200.57459)\n     | > grad_norm: 13.24564  (23.10637)\n     | > current_lr: 0.00001 \n     | > step_time: 1.33910  (0.91624)\n     | > loader_time: 0.01680  (0.01261)\n\n\n\u001b[1m   --> STEP: 387/406 -- GLOBAL_STEP: 19875\u001b[0m\n     | > loss: 0.03558  (0.03596)\n     | > log_mle: -0.22544  (-0.21401)\n     | > loss_dur: 0.26102  (0.24997)\n     | > amp_scaler: 8192.00000  (6329.21964)\n     | > grad_norm: 18.01791  (23.28240)\n     | > current_lr: 0.00001 \n     | > step_time: 1.33910  (0.94584)\n     | > loader_time: 0.02140  (0.01307)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00692 \u001b[0m(+0.00006)\n     | > avg_loss:\u001b[92m 0.09056 \u001b[0m(-0.00333)\n     | > avg_log_mle:\u001b[92m -0.15507 \u001b[0m(-0.00250)\n     | > avg_loss_dur:\u001b[92m 0.24563 \u001b[0m(-0.00083)\n\n\n\u001b[4m\u001b[1m > EPOCH: 49/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:10:09) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 6/406 -- GLOBAL_STEP: 19900\u001b[0m\n     | > loss: 0.01322  (0.02088)\n     | > log_mle: -0.19358  (-0.19382)\n     | > loss_dur: 0.20681  (0.21470)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 8.55737  (15.06022)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56290  (0.53805)\n     | > loader_time: 0.00700  (0.00632)\n\n\n\u001b[1m   --> STEP: 31/406 -- GLOBAL_STEP: 19925\u001b[0m\n     | > loss: 0.02412  (0.02377)\n     | > log_mle: -0.19071  (-0.19549)\n     | > loss_dur: 0.21483  (0.21926)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 15.88654  (17.29928)\n     | > current_lr: 0.00001 \n     | > step_time: 0.59280  (0.58184)\n     | > loader_time: 0.00340  (0.00684)\n\n\n\u001b[1m   --> STEP: 56/406 -- GLOBAL_STEP: 19950\u001b[0m\n     | > loss: 0.02866  (0.02908)\n     | > log_mle: -0.20276  (-0.19667)\n     | > loss_dur: 0.23141  (0.22575)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 21.17547  (17.36461)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70650  (0.61158)\n     | > loader_time: 0.00930  (0.00767)\n\n\n\u001b[1m   --> STEP: 81/406 -- GLOBAL_STEP: 19975\u001b[0m\n     | > loss: 0.01867  (0.03004)\n     | > log_mle: -0.21227  (-0.19896)\n     | > loss_dur: 0.23094  (0.22900)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 10.35485  (17.57631)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71400  (0.64255)\n     | > loader_time: 0.01140  (0.00806)\n\n\n\u001b[1m   --> STEP: 106/406 -- GLOBAL_STEP: 20000\u001b[0m\n     | > loss: 0.01615  (0.03033)\n     | > log_mle: -0.20581  (-0.20204)\n     | > loss_dur: 0.22196  (0.23238)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 22.15171  (19.15159)\n     | > current_lr: 0.00001 \n     | > step_time: 1.06720  (0.66984)\n     | > loader_time: 0.01300  (0.00848)\n\n\n > CHECKPOINT : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/checkpoint_20000.pth\n\n\u001b[1m   --> STEP: 131/406 -- GLOBAL_STEP: 20025\u001b[0m\n     | > loss: 0.00264  (0.02972)\n     | > log_mle: -0.21579  (-0.20481)\n     | > loss_dur: 0.21842  (0.23454)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 21.66594  (20.54672)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71130  (0.68654)\n     | > loader_time: 0.01160  (0.00934)\n\n\n\u001b[1m   --> STEP: 156/406 -- GLOBAL_STEP: 20050\u001b[0m\n     | > loss: 0.00985  (0.03008)\n     | > log_mle: -0.22114  (-0.20678)\n     | > loss_dur: 0.23100  (0.23687)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 35.23187  (21.53342)\n     | > current_lr: 0.00001 \n     | > step_time: 0.85510  (0.71068)\n     | > loader_time: 0.00680  (0.00971)\n\n\n\u001b[1m   --> STEP: 181/406 -- GLOBAL_STEP: 20075\u001b[0m\n     | > loss: 0.04484  (0.03002)\n     | > log_mle: -0.21665  (-0.20844)\n     | > loss_dur: 0.26149  (0.23846)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 35.44828  (22.45199)\n     | > current_lr: 0.00001 \n     | > step_time: 0.84470  (0.73428)\n     | > loader_time: 0.02290  (0.01013)\n\n\n\u001b[1m   --> STEP: 206/406 -- GLOBAL_STEP: 20100\u001b[0m\n     | > loss: 0.02887  (0.03016)\n     | > log_mle: -0.22045  (-0.21002)\n     | > loss_dur: 0.24933  (0.24018)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 30.79611  (23.10749)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02930  (0.75832)\n     | > loader_time: 0.00550  (0.01031)\n\n\n\u001b[1m   --> STEP: 231/406 -- GLOBAL_STEP: 20125\u001b[0m\n     | > loss: 0.05312  (0.03003)\n     | > log_mle: -0.22771  (-0.21158)\n     | > loss_dur: 0.28083  (0.24161)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 45.20132  (23.37074)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95870  (0.78129)\n     | > loader_time: 0.01310  (0.01075)\n\n\n\u001b[1m   --> STEP: 256/406 -- GLOBAL_STEP: 20150\u001b[0m\n     | > loss: 0.02730  (0.03015)\n     | > log_mle: -0.22468  (-0.21283)\n     | > loss_dur: 0.25198  (0.24298)\n     | > amp_scaler: 8192.00000  (8192.00000)\n     | > grad_norm: 24.70424  (24.32340)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11100  (0.80491)\n     | > loader_time: 0.02560  (0.01097)\n\n\n\u001b[1m   --> STEP: 281/406 -- GLOBAL_STEP: 20175\u001b[0m\n     | > loss: 0.01728  (0.02985)\n     | > log_mle: -0.23761  (-0.21396)\n     | > loss_dur: 0.25489  (0.24381)\n     | > amp_scaler: 4096.00000  (7944.19929)\n     | > grad_norm: 49.36655  (25.16644)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01220  (0.83039)\n     | > loader_time: 0.02060  (0.01113)\n\n\n\u001b[1m   --> STEP: 306/406 -- GLOBAL_STEP: 20200\u001b[0m\n     | > loss: 0.04076  (0.03033)\n     | > log_mle: -0.22425  (-0.21491)\n     | > loss_dur: 0.26501  (0.24525)\n     | > amp_scaler: 4096.00000  (7629.80392)\n     | > grad_norm: 27.16721  (25.53724)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09470  (0.85556)\n     | > loader_time: 0.00570  (0.01115)\n\n\n\u001b[1m   --> STEP: 331/406 -- GLOBAL_STEP: 20225\u001b[0m\n     | > loss: 0.04596  (0.03035)\n     | > log_mle: -0.22362  (-0.21572)\n     | > loss_dur: 0.26959  (0.24607)\n     | > amp_scaler: 4096.00000  (7362.90030)\n     | > grad_norm: 33.61241  (25.37570)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16330  (0.88060)\n     | > loader_time: 0.00810  (0.01124)\n\n\n\u001b[1m   --> STEP: 356/406 -- GLOBAL_STEP: 20250\u001b[0m\n     | > loss: 0.02580  (0.03037)\n     | > log_mle: -0.23680  (-0.21669)\n     | > loss_dur: 0.26260  (0.24705)\n     | > amp_scaler: 4096.00000  (7133.48315)\n     | > grad_norm: 37.00618  (25.83048)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28690  (0.90744)\n     | > loader_time: 0.01690  (0.01154)\n\n\n\u001b[1m   --> STEP: 381/406 -- GLOBAL_STEP: 20275\u001b[0m\n     | > loss: 0.04621  (0.03006)\n     | > log_mle: -0.22236  (-0.21761)\n     | > loss_dur: 0.26858  (0.24767)\n     | > amp_scaler: 4096.00000  (6934.17323)\n     | > grad_norm: 19.60396  (26.23811)\n     | > current_lr: 0.00001 \n     | > step_time: 1.30540  (0.93666)\n     | > loader_time: 0.01790  (0.01181)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00644 \u001b[0m(-0.00048)\n     | > avg_loss:\u001b[92m 0.06543 \u001b[0m(-0.02513)\n     | > avg_log_mle:\u001b[92m -0.17911 \u001b[0m(-0.02404)\n     | > avg_loss_dur:\u001b[92m 0.24453 \u001b[0m(-0.00109)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_20300.pth\n\n\u001b[4m\u001b[1m > EPOCH: 50/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:17:19) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 0/406 -- GLOBAL_STEP: 20300\u001b[0m\n     | > loss: 0.02288  (0.02288)\n     | > log_mle: -0.19279  (-0.19279)\n     | > loss_dur: 0.21567  (0.21567)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.47651  (13.47651)\n     | > current_lr: 0.00001 \n     | > step_time: 1.52830  (1.52825)\n     | > loader_time: 0.91470  (0.91466)\n\n\n\u001b[1m   --> STEP: 25/406 -- GLOBAL_STEP: 20325\u001b[0m\n     | > loss: 0.01323  (0.01390)\n     | > log_mle: -0.19544  (-0.19898)\n     | > loss_dur: 0.20866  (0.21287)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.32123  (18.32866)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60260  (0.56736)\n     | > loader_time: 0.00390  (0.00498)\n\n\n\u001b[1m   --> STEP: 50/406 -- GLOBAL_STEP: 20350\u001b[0m\n     | > loss: 0.04132  (0.02310)\n     | > log_mle: -0.21872  (-0.19896)\n     | > loss_dur: 0.26004  (0.22206)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.17200  (17.49654)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64610  (0.61175)\n     | > loader_time: 0.00410  (0.00536)\n\n\n\u001b[1m   --> STEP: 75/406 -- GLOBAL_STEP: 20375\u001b[0m\n     | > loss: 0.03472  (0.02440)\n     | > log_mle: -0.21149  (-0.20151)\n     | > loss_dur: 0.24621  (0.22591)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 29.61379  (18.63882)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67640  (0.62880)\n     | > loader_time: 0.00460  (0.00538)\n\n\n\u001b[1m   --> STEP: 100/406 -- GLOBAL_STEP: 20400\u001b[0m\n     | > loss: 0.03626  (0.02511)\n     | > log_mle: -0.20600  (-0.20430)\n     | > loss_dur: 0.24226  (0.22942)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 29.52157  (20.85165)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67810  (0.65527)\n     | > loader_time: 0.00810  (0.00553)\n\n\n\u001b[1m   --> STEP: 125/406 -- GLOBAL_STEP: 20425\u001b[0m\n     | > loss: 0.00869  (0.02416)\n     | > log_mle: -0.23371  (-0.20741)\n     | > loss_dur: 0.24240  (0.23157)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 54.53961  (21.76059)\n     | > current_lr: 0.00001 \n     | > step_time: 0.77700  (0.67471)\n     | > loader_time: 0.00660  (0.00578)\n\n\n\u001b[1m   --> STEP: 150/406 -- GLOBAL_STEP: 20450\u001b[0m\n     | > loss: 0.01298  (0.02452)\n     | > log_mle: -0.21679  (-0.20959)\n     | > loss_dur: 0.22976  (0.23412)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.43102  (23.22422)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90510  (0.69786)\n     | > loader_time: 0.01070  (0.00609)\n\n\n\u001b[1m   --> STEP: 175/406 -- GLOBAL_STEP: 20475\u001b[0m\n     | > loss: 0.02253  (0.02467)\n     | > log_mle: -0.22069  (-0.21114)\n     | > loss_dur: 0.24322  (0.23582)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.71413  (23.72074)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94450  (0.72164)\n     | > loader_time: 0.01420  (0.00639)\n\n\n\u001b[1m   --> STEP: 200/406 -- GLOBAL_STEP: 20500\u001b[0m\n     | > loss: 0.01573  (0.02464)\n     | > log_mle: -0.22366  (-0.21280)\n     | > loss_dur: 0.23938  (0.23743)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 53.78358  (24.73441)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16910  (0.74364)\n     | > loader_time: 0.01420  (0.00660)\n\n\n\u001b[1m   --> STEP: 225/406 -- GLOBAL_STEP: 20525\u001b[0m\n     | > loss: 0.04682  (0.02461)\n     | > log_mle: -0.22429  (-0.21431)\n     | > loss_dur: 0.27111  (0.23892)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.45734  (25.28298)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96010  (0.76681)\n     | > loader_time: 0.00660  (0.00687)\n\n\n\u001b[1m   --> STEP: 250/406 -- GLOBAL_STEP: 20550\u001b[0m\n     | > loss: 0.02016  (0.02491)\n     | > log_mle: -0.24709  (-0.21569)\n     | > loss_dur: 0.26725  (0.24060)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 37.65654  (26.61463)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01490  (0.79232)\n     | > loader_time: 0.00610  (0.00703)\n\n\n\u001b[1m   --> STEP: 275/406 -- GLOBAL_STEP: 20575\u001b[0m\n     | > loss: 0.03637  (0.02457)\n     | > log_mle: -0.22021  (-0.21692)\n     | > loss_dur: 0.25658  (0.24149)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 43.24091  (27.17479)\n     | > current_lr: 0.00001 \n     | > step_time: 1.03150  (0.81756)\n     | > loader_time: 0.00620  (0.00726)\n\n\n\u001b[1m   --> STEP: 300/406 -- GLOBAL_STEP: 20600\u001b[0m\n     | > loss: 0.04193  (0.02444)\n     | > log_mle: -0.21571  (-0.21789)\n     | > loss_dur: 0.25764  (0.24233)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 36.34972  (27.37476)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12030  (0.84500)\n     | > loader_time: 0.01150  (0.00769)\n\n\n\u001b[1m   --> STEP: 325/406 -- GLOBAL_STEP: 20625\u001b[0m\n     | > loss: 0.04074  (0.02450)\n     | > log_mle: -0.22080  (-0.21879)\n     | > loss_dur: 0.26154  (0.24329)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 35.45376  (27.53675)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11470  (0.87247)\n     | > loader_time: 0.01560  (0.00809)\n\n\n\u001b[1m   --> STEP: 350/406 -- GLOBAL_STEP: 20650\u001b[0m\n     | > loss: 0.01622  (0.02465)\n     | > log_mle: -0.23573  (-0.21966)\n     | > loss_dur: 0.25195  (0.24431)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 45.96412  (27.83640)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28180  (0.90054)\n     | > loader_time: 0.01300  (0.00852)\n\n\n\u001b[1m   --> STEP: 375/406 -- GLOBAL_STEP: 20675\u001b[0m\n     | > loss: 0.01271  (0.02430)\n     | > log_mle: -0.24296  (-0.22061)\n     | > loss_dur: 0.25567  (0.24491)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 42.47160  (27.80092)\n     | > current_lr: 0.00001 \n     | > step_time: 1.22480  (0.92891)\n     | > loader_time: 0.01150  (0.00893)\n\n\n\u001b[1m   --> STEP: 400/406 -- GLOBAL_STEP: 20700\u001b[0m\n     | > loss: 0.03802  (0.02417)\n     | > log_mle: -0.23837  (-0.22145)\n     | > loss_dur: 0.27639  (0.24562)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 24.27493  (27.63029)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95400  (0.95345)\n     | > loader_time: 0.00600  (0.00915)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00630 \u001b[0m(-0.00014)\n     | > avg_loss:\u001b[91m 0.07875 \u001b[0m(+0.01332)\n     | > avg_log_mle:\u001b[91m -0.16378 \u001b[0m(+0.01532)\n     | > avg_loss_dur:\u001b[92m 0.24253 \u001b[0m(-0.00200)\n\n\n\u001b[4m\u001b[1m > EPOCH: 51/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:24:18) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 19/406 -- GLOBAL_STEP: 20725\u001b[0m\n     | > loss: 0.01581  (0.01074)\n     | > log_mle: -0.18459  (-0.20245)\n     | > loss_dur: 0.20040  (0.21319)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.45022  (23.67825)\n     | > current_lr: 0.00001 \n     | > step_time: 0.58220  (0.52883)\n     | > loader_time: 0.00340  (0.00402)\n\n\n\u001b[1m   --> STEP: 44/406 -- GLOBAL_STEP: 20750\u001b[0m\n     | > loss: 0.04002  (0.02673)\n     | > log_mle: -0.20760  (-0.19822)\n     | > loss_dur: 0.24762  (0.22494)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 5.95055  (20.68863)\n     | > current_lr: 0.00001 \n     | > step_time: 0.59640  (0.57163)\n     | > loader_time: 0.00420  (0.00454)\n\n\n\u001b[1m   --> STEP: 69/406 -- GLOBAL_STEP: 20775\u001b[0m\n     | > loss: 0.02822  (0.02732)\n     | > log_mle: -0.21229  (-0.20132)\n     | > loss_dur: 0.24050  (0.22864)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 29.52581  (20.81640)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64930  (0.59926)\n     | > loader_time: 0.00630  (0.00499)\n\n\n\u001b[1m   --> STEP: 94/406 -- GLOBAL_STEP: 20800\u001b[0m\n     | > loss: 0.02476  (0.02569)\n     | > log_mle: -0.22204  (-0.20549)\n     | > loss_dur: 0.24680  (0.23118)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.25038  (23.49364)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72660  (0.62154)\n     | > loader_time: 0.00490  (0.00523)\n\n\n\u001b[1m   --> STEP: 119/406 -- GLOBAL_STEP: 20825\u001b[0m\n     | > loss: 0.01546  (0.02411)\n     | > log_mle: -0.23334  (-0.20863)\n     | > loss_dur: 0.24880  (0.23274)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.86766  (24.44910)\n     | > current_lr: 0.00001 \n     | > step_time: 0.76120  (0.64228)\n     | > loader_time: 0.00750  (0.00538)\n\n\n\u001b[1m   --> STEP: 144/406 -- GLOBAL_STEP: 20850\u001b[0m\n     | > loss: 0.02613  (0.02332)\n     | > log_mle: -0.21515  (-0.21128)\n     | > loss_dur: 0.24129  (0.23460)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 39.20099  (24.89254)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79080  (0.66641)\n     | > loader_time: 0.00500  (0.00591)\n\n\n\u001b[1m   --> STEP: 169/406 -- GLOBAL_STEP: 20875\u001b[0m\n     | > loss: 0.02889  (0.02304)\n     | > log_mle: -0.22176  (-0.21303)\n     | > loss_dur: 0.25064  (0.23607)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 46.91191  (26.74605)\n     | > current_lr: 0.00001 \n     | > step_time: 0.88110  (0.69308)\n     | > loader_time: 0.00500  (0.00609)\n\n\n\u001b[1m   --> STEP: 194/406 -- GLOBAL_STEP: 20900\u001b[0m\n     | > loss: 0.02999  (0.02232)\n     | > log_mle: -0.22594  (-0.21496)\n     | > loss_dur: 0.25593  (0.23728)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.01530  (26.71247)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94650  (0.71580)\n     | > loader_time: 0.00600  (0.00614)\n\n\n\u001b[1m   --> STEP: 219/406 -- GLOBAL_STEP: 20925\u001b[0m\n     | > loss: 0.01644  (0.02214)\n     | > log_mle: -0.23169  (-0.21647)\n     | > loss_dur: 0.24812  (0.23861)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 25.73814  (27.65371)\n     | > current_lr: 0.00001 \n     | > step_time: 0.88460  (0.74101)\n     | > loader_time: 0.00640  (0.00636)\n\n\n\u001b[1m   --> STEP: 244/406 -- GLOBAL_STEP: 20950\u001b[0m\n     | > loss: 0.01044  (0.02207)\n     | > log_mle: -0.23019  (-0.21813)\n     | > loss_dur: 0.24063  (0.24020)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 16.00641  (27.60233)\n     | > current_lr: 0.00001 \n     | > step_time: 0.92360  (0.76661)\n     | > loader_time: 0.00720  (0.00644)\n\n\n\u001b[1m   --> STEP: 269/406 -- GLOBAL_STEP: 20975\u001b[0m\n     | > loss: 0.01502  (0.02174)\n     | > log_mle: -0.23754  (-0.21932)\n     | > loss_dur: 0.25256  (0.24106)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.16549  (27.65327)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98530  (0.78989)\n     | > loader_time: 0.00730  (0.00656)\n\n\n\u001b[1m   --> STEP: 294/406 -- GLOBAL_STEP: 21000\u001b[0m\n     | > loss: 0.01447  (0.02121)\n     | > log_mle: -0.24055  (-0.22051)\n     | > loss_dur: 0.25501  (0.24172)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.15686  (27.65988)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12370  (0.81333)\n     | > loader_time: 0.01080  (0.00664)\n\n\n\u001b[1m   --> STEP: 319/406 -- GLOBAL_STEP: 21025\u001b[0m\n     | > loss: 0.02324  (0.02127)\n     | > log_mle: -0.23776  (-0.22155)\n     | > loss_dur: 0.26100  (0.24283)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 42.50417  (28.08074)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11040  (0.83584)\n     | > loader_time: 0.01370  (0.00682)\n\n\n\u001b[1m   --> STEP: 344/406 -- GLOBAL_STEP: 21050\u001b[0m\n     | > loss: 0.02929  (0.02118)\n     | > log_mle: -0.22801  (-0.22240)\n     | > loss_dur: 0.25730  (0.24358)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 25.13355  (28.36143)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11200  (0.86082)\n     | > loader_time: 0.00700  (0.00717)\n\n\n\u001b[1m   --> STEP: 369/406 -- GLOBAL_STEP: 21075\u001b[0m\n     | > loss: 0.02324  (0.02077)\n     | > log_mle: -0.22774  (-0.22342)\n     | > loss_dur: 0.25098  (0.24419)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.61558  (28.58312)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42260  (0.89118)\n     | > loader_time: 0.01650  (0.00772)\n\n\n\u001b[1m   --> STEP: 394/406 -- GLOBAL_STEP: 21100\u001b[0m\n     | > loss: 0.01510  (0.02042)\n     | > log_mle: -0.23466  (-0.22431)\n     | > loss_dur: 0.24976  (0.24473)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.71484  (28.50159)\n     | > current_lr: 0.00001 \n     | > step_time: 1.89770  (0.92096)\n     | > loader_time: 0.00720  (0.00816)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00583 \u001b[0m(-0.00047)\n     | > avg_loss:\u001b[92m 0.06642 \u001b[0m(-0.01233)\n     | > avg_log_mle:\u001b[92m -0.17447 \u001b[0m(-0.01069)\n     | > avg_loss_dur:\u001b[92m 0.24090 \u001b[0m(-0.00164)\n\n\n\u001b[4m\u001b[1m > EPOCH: 52/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:31:06) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 13/406 -- GLOBAL_STEP: 21125\u001b[0m\n     | > loss: -0.00296  (-0.00086)\n     | > log_mle: -0.21223  (-0.20675)\n     | > loss_dur: 0.20927  (0.20589)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.83727  (15.58880)\n     | > current_lr: 0.00001 \n     | > step_time: 0.54660  (0.54676)\n     | > loader_time: 0.00460  (0.00433)\n\n\n\u001b[1m   --> STEP: 38/406 -- GLOBAL_STEP: 21150\u001b[0m\n     | > loss: 0.01799  (0.01325)\n     | > log_mle: -0.21383  (-0.20417)\n     | > loss_dur: 0.23182  (0.21742)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.15477  (18.88504)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62020  (0.57608)\n     | > loader_time: 0.00680  (0.00486)\n\n\n\u001b[1m   --> STEP: 63/406 -- GLOBAL_STEP: 21175\u001b[0m\n     | > loss: 0.01817  (0.01625)\n     | > log_mle: -0.21809  (-0.20577)\n     | > loss_dur: 0.23625  (0.22203)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.87062  (19.34952)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65520  (0.61190)\n     | > loader_time: 0.00420  (0.00565)\n\n\n\u001b[1m   --> STEP: 88/406 -- GLOBAL_STEP: 21200\u001b[0m\n     | > loss: 0.01897  (0.01588)\n     | > log_mle: -0.20992  (-0.20933)\n     | > loss_dur: 0.22890  (0.22520)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 13.59543  (21.23991)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65970  (0.63288)\n     | > loader_time: 0.00700  (0.00605)\n\n\n\u001b[1m   --> STEP: 113/406 -- GLOBAL_STEP: 21225\u001b[0m\n     | > loss: 0.01104  (0.01455)\n     | > log_mle: -0.22815  (-0.21284)\n     | > loss_dur: 0.23919  (0.22739)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.55800  (21.74942)\n     | > current_lr: 0.00001 \n     | > step_time: 0.68250  (0.65125)\n     | > loader_time: 0.00700  (0.00648)\n\n\n\u001b[1m   --> STEP: 138/406 -- GLOBAL_STEP: 21250\u001b[0m\n     | > loss: 0.02850  (0.01465)\n     | > log_mle: -0.22929  (-0.21535)\n     | > loss_dur: 0.25779  (0.23000)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.83124  (22.79950)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73740  (0.67282)\n     | > loader_time: 0.00750  (0.00665)\n\n\n\u001b[1m   --> STEP: 163/406 -- GLOBAL_STEP: 21275\u001b[0m\n     | > loss: 0.02570  (0.01489)\n     | > log_mle: -0.23641  (-0.21724)\n     | > loss_dur: 0.26211  (0.23212)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 33.55990  (23.52178)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01820  (0.69460)\n     | > loader_time: 0.00770  (0.00664)\n\n\n\u001b[1m   --> STEP: 188/406 -- GLOBAL_STEP: 21300\u001b[0m\n     | > loss: 0.01955  (0.01452)\n     | > log_mle: -0.24748  (-0.21908)\n     | > loss_dur: 0.26703  (0.23360)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 61.14793  (24.72740)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96050  (0.71903)\n     | > loader_time: 0.01440  (0.00679)\n\n\n\u001b[1m   --> STEP: 213/406 -- GLOBAL_STEP: 21325\u001b[0m\n     | > loss: 0.00762  (0.01458)\n     | > log_mle: -0.23819  (-0.22049)\n     | > loss_dur: 0.24582  (0.23507)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 34.91326  (25.69906)\n     | > current_lr: 0.00001 \n     | > step_time: 0.93960  (0.74503)\n     | > loader_time: 0.00640  (0.00688)\n\n\n\u001b[1m   --> STEP: 238/406 -- GLOBAL_STEP: 21350\u001b[0m\n     | > loss: 0.01156  (0.01458)\n     | > log_mle: -0.24137  (-0.22211)\n     | > loss_dur: 0.25293  (0.23670)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.59228  (26.77321)\n     | > current_lr: 0.00001 \n     | > step_time: 1.04990  (0.77109)\n     | > loader_time: 0.00730  (0.00696)\n\n\n\u001b[1m   --> STEP: 263/406 -- GLOBAL_STEP: 21375\u001b[0m\n     | > loss: 0.00208  (0.01405)\n     | > log_mle: -0.23924  (-0.22351)\n     | > loss_dur: 0.24132  (0.23756)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 41.14494  (27.28746)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08700  (0.79607)\n     | > loader_time: 0.00750  (0.00699)\n\n\n\u001b[1m   --> STEP: 288/406 -- GLOBAL_STEP: 21400\u001b[0m\n     | > loss: 0.00996  (0.01375)\n     | > log_mle: -0.23011  (-0.22468)\n     | > loss_dur: 0.24007  (0.23844)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 62.74839  (27.89389)\n     | > current_lr: 0.00001 \n     | > step_time: 1.05830  (0.82266)\n     | > loader_time: 0.00860  (0.00717)\n\n\n\u001b[1m   --> STEP: 313/406 -- GLOBAL_STEP: 21425\u001b[0m\n     | > loss: 0.01666  (0.01376)\n     | > log_mle: -0.23331  (-0.22569)\n     | > loss_dur: 0.24997  (0.23945)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 48.11010  (28.44434)\n     | > current_lr: 0.00001 \n     | > step_time: 1.19580  (0.84671)\n     | > loader_time: 0.00640  (0.00728)\n\n\n\u001b[1m   --> STEP: 338/406 -- GLOBAL_STEP: 21450\u001b[0m\n     | > loss: 0.00991  (0.01389)\n     | > log_mle: -0.23948  (-0.22657)\n     | > loss_dur: 0.24939  (0.24047)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 35.45943  (28.83342)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09160  (0.87398)\n     | > loader_time: 0.00950  (0.00758)\n\n\n\u001b[1m   --> STEP: 363/406 -- GLOBAL_STEP: 21475\u001b[0m\n     | > loss: -0.00447  (0.01357)\n     | > log_mle: -0.24947  (-0.22758)\n     | > loss_dur: 0.24500  (0.24115)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.95587  (29.10862)\n     | > current_lr: 0.00001 \n     | > step_time: 1.40040  (0.90505)\n     | > loader_time: 0.01480  (0.00806)\n\n\n\u001b[1m   --> STEP: 388/406 -- GLOBAL_STEP: 21500\u001b[0m\n     | > loss: 0.00977  (0.01345)\n     | > log_mle: -0.23837  (-0.22840)\n     | > loss_dur: 0.24814  (0.24185)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.36332  (29.51320)\n     | > current_lr: 0.00001 \n     | > step_time: 1.29900  (0.93304)\n     | > loader_time: 0.00980  (0.00839)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00694 \u001b[0m(+0.00111)\n     | > avg_loss:\u001b[92m 0.06589 \u001b[0m(-0.00053)\n     | > avg_log_mle:\u001b[91m -0.17105 \u001b[0m(+0.00342)\n     | > avg_loss_dur:\u001b[92m 0.23695 \u001b[0m(-0.00395)\n\n\n\u001b[4m\u001b[1m > EPOCH: 53/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:38:00) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 7/406 -- GLOBAL_STEP: 21525\u001b[0m\n     | > loss: 0.00049  (-0.00180)\n     | > log_mle: -0.19923  (-0.20704)\n     | > loss_dur: 0.19972  (0.20523)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.69182  (15.08737)\n     | > current_lr: 0.00001 \n     | > step_time: 0.52230  (0.52921)\n     | > loader_time: 0.00410  (0.00316)\n\n\n\u001b[1m   --> STEP: 32/406 -- GLOBAL_STEP: 21550\u001b[0m\n     | > loss: 0.00912  (0.00441)\n     | > log_mle: -0.21032  (-0.20762)\n     | > loss_dur: 0.21945  (0.21204)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 28.05858  (18.72575)\n     | > current_lr: 0.00001 \n     | > step_time: 0.61740  (0.57249)\n     | > loader_time: 0.00680  (0.00512)\n\n\n\u001b[1m   --> STEP: 57/406 -- GLOBAL_STEP: 21575\u001b[0m\n     | > loss: 0.02056  (0.00810)\n     | > log_mle: -0.20905  (-0.20937)\n     | > loss_dur: 0.22961  (0.21748)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.11404  (20.55109)\n     | > current_lr: 0.00001 \n     | > step_time: 0.69340  (0.60833)\n     | > loader_time: 0.00420  (0.00517)\n\n\n\u001b[1m   --> STEP: 82/406 -- GLOBAL_STEP: 21600\u001b[0m\n     | > loss: 0.01412  (0.00941)\n     | > log_mle: -0.22755  (-0.21227)\n     | > loss_dur: 0.24166  (0.22168)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 37.02292  (23.58493)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79280  (0.63878)\n     | > loader_time: 0.01470  (0.00544)\n\n\n\u001b[1m   --> STEP: 107/406 -- GLOBAL_STEP: 21625\u001b[0m\n     | > loss: -0.00321  (0.00893)\n     | > log_mle: -0.22443  (-0.21553)\n     | > loss_dur: 0.22121  (0.22445)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 40.63699  (25.03461)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66370  (0.65244)\n     | > loader_time: 0.00510  (0.00567)\n\n\n\u001b[1m   --> STEP: 132/406 -- GLOBAL_STEP: 21650\u001b[0m\n     | > loss: -0.00589  (0.00828)\n     | > log_mle: -0.24317  (-0.21832)\n     | > loss_dur: 0.23728  (0.22660)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 29.37215  (26.08435)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74240  (0.67440)\n     | > loader_time: 0.00490  (0.00592)\n\n\n\u001b[1m   --> STEP: 157/406 -- GLOBAL_STEP: 21675\u001b[0m\n     | > loss: 0.00471  (0.00893)\n     | > log_mle: -0.23164  (-0.22020)\n     | > loss_dur: 0.23636  (0.22913)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 7.24929  (25.63227)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79630  (0.69033)\n     | > loader_time: 0.00540  (0.00601)\n\n\n\u001b[1m   --> STEP: 182/406 -- GLOBAL_STEP: 21700\u001b[0m\n     | > loss: 0.00573  (0.00910)\n     | > log_mle: -0.23388  (-0.22179)\n     | > loss_dur: 0.23961  (0.23089)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 24.33638  (26.34898)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91700  (0.71300)\n     | > loader_time: 0.00510  (0.00618)\n\n\n\u001b[1m   --> STEP: 207/406 -- GLOBAL_STEP: 21725\u001b[0m\n     | > loss: 0.02134  (0.00954)\n     | > log_mle: -0.22401  (-0.22326)\n     | > loss_dur: 0.24535  (0.23280)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.94623  (27.29148)\n     | > current_lr: 0.00001 \n     | > step_time: 0.86690  (0.73850)\n     | > loader_time: 0.00500  (0.00626)\n\n\n\u001b[1m   --> STEP: 232/406 -- GLOBAL_STEP: 21750\u001b[0m\n     | > loss: 0.00369  (0.00952)\n     | > log_mle: -0.24270  (-0.22484)\n     | > loss_dur: 0.24639  (0.23435)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 37.99009  (29.49886)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16350  (0.76096)\n     | > loader_time: 0.00770  (0.00640)\n\n\n\u001b[1m   --> STEP: 257/406 -- GLOBAL_STEP: 21775\u001b[0m\n     | > loss: 0.00248  (0.00950)\n     | > log_mle: -0.24026  (-0.22616)\n     | > loss_dur: 0.24274  (0.23566)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 29.22115  (30.22724)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10360  (0.78355)\n     | > loader_time: 0.00700  (0.00656)\n\n\n\u001b[1m   --> STEP: 282/406 -- GLOBAL_STEP: 21800\u001b[0m\n     | > loss: 0.01717  (0.00902)\n     | > log_mle: -0.24745  (-0.22742)\n     | > loss_dur: 0.26462  (0.23643)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.30894  (30.14783)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98900  (0.80726)\n     | > loader_time: 0.00650  (0.00670)\n\n\n\u001b[1m   --> STEP: 307/406 -- GLOBAL_STEP: 21825\u001b[0m\n     | > loss: 0.01484  (0.00935)\n     | > log_mle: -0.23372  (-0.22822)\n     | > loss_dur: 0.24856  (0.23756)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.49783  (30.68099)\n     | > current_lr: 0.00001 \n     | > step_time: 1.14000  (0.83128)\n     | > loader_time: 0.00680  (0.00692)\n\n\n\u001b[1m   --> STEP: 332/406 -- GLOBAL_STEP: 21850\u001b[0m\n     | > loss: 0.01187  (0.00933)\n     | > log_mle: -0.23250  (-0.22898)\n     | > loss_dur: 0.24437  (0.23831)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.50692  (30.74019)\n     | > current_lr: 0.00001 \n     | > step_time: 1.22790  (0.85739)\n     | > loader_time: 0.00660  (0.00702)\n\n\n\u001b[1m   --> STEP: 357/406 -- GLOBAL_STEP: 21875\u001b[0m\n     | > loss: 0.01103  (0.00932)\n     | > log_mle: -0.23191  (-0.22998)\n     | > loss_dur: 0.24294  (0.23930)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.59968  (31.09842)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16440  (0.88263)\n     | > loader_time: 0.00730  (0.00718)\n\n\n\u001b[1m   --> STEP: 382/406 -- GLOBAL_STEP: 21900\u001b[0m\n     | > loss: 0.01735  (0.00891)\n     | > log_mle: -0.23950  (-0.23096)\n     | > loss_dur: 0.25686  (0.23988)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.52289  (31.38437)\n     | > current_lr: 0.00001 \n     | > step_time: 1.26460  (0.90961)\n     | > loader_time: 0.00820  (0.00732)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00549 \u001b[0m(-0.00145)\n     | > avg_loss:\u001b[91m 0.07850 \u001b[0m(+0.01260)\n     | > avg_log_mle:\u001b[91m -0.15724 \u001b[0m(+0.01381)\n     | > avg_loss_dur:\u001b[92m 0.23573 \u001b[0m(-0.00121)\n\n\n\u001b[4m\u001b[1m > EPOCH: 54/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:44:50) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 1/406 -- GLOBAL_STEP: 21925\u001b[0m\n     | > loss: -0.03464  (-0.03464)\n     | > log_mle: -0.21687  (-0.21687)\n     | > loss_dur: 0.18223  (0.18223)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 14.43603  (14.43603)\n     | > current_lr: 0.00001 \n     | > step_time: 0.48310  (0.48311)\n     | > loader_time: 0.00580  (0.00582)\n\n\n\u001b[1m   --> STEP: 26/406 -- GLOBAL_STEP: 21950\u001b[0m\n     | > loss: 0.01010  (-0.00670)\n     | > log_mle: -0.21679  (-0.21285)\n     | > loss_dur: 0.22689  (0.20615)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.85462  (21.53262)\n     | > current_lr: 0.00001 \n     | > step_time: 0.56620  (0.56152)\n     | > loader_time: 0.00370  (0.00460)\n\n\n\u001b[1m   --> STEP: 51/406 -- GLOBAL_STEP: 21975\u001b[0m\n     | > loss: -0.01110  (0.00102)\n     | > log_mle: -0.24187  (-0.21326)\n     | > loss_dur: 0.23078  (0.21427)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 11.92503  (22.77307)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67780  (0.61129)\n     | > loader_time: 0.00700  (0.00492)\n\n\n\u001b[1m   --> STEP: 76/406 -- GLOBAL_STEP: 22000\u001b[0m\n     | > loss: -0.00551  (0.00277)\n     | > log_mle: -0.21436  (-0.21548)\n     | > loss_dur: 0.20885  (0.21824)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.56500  (24.24935)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67100  (0.63254)\n     | > loader_time: 0.00610  (0.00512)\n\n\n\u001b[1m   --> STEP: 101/406 -- GLOBAL_STEP: 22025\u001b[0m\n     | > loss: 0.00295  (0.00317)\n     | > log_mle: -0.23070  (-0.21832)\n     | > loss_dur: 0.23365  (0.22149)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.13888  (24.71945)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65320  (0.65513)\n     | > loader_time: 0.00490  (0.00561)\n\n\n\u001b[1m   --> STEP: 126/406 -- GLOBAL_STEP: 22050\u001b[0m\n     | > loss: 0.03264  (0.00277)\n     | > log_mle: -0.22240  (-0.22123)\n     | > loss_dur: 0.25503  (0.22400)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.03058  (25.56886)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70620  (0.66910)\n     | > loader_time: 0.00480  (0.00571)\n\n\n\u001b[1m   --> STEP: 151/406 -- GLOBAL_STEP: 22075\u001b[0m\n     | > loss: 0.00026  (0.00280)\n     | > log_mle: -0.23084  (-0.22349)\n     | > loss_dur: 0.23110  (0.22630)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.30595  (26.61698)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80010  (0.69326)\n     | > loader_time: 0.00510  (0.00631)\n\n\n\u001b[1m   --> STEP: 176/406 -- GLOBAL_STEP: 22100\u001b[0m\n     | > loss: 0.00875  (0.00307)\n     | > log_mle: -0.22881  (-0.22507)\n     | > loss_dur: 0.23755  (0.22813)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.89406  (27.02311)\n     | > current_lr: 0.00001 \n     | > step_time: 0.79710  (0.71721)\n     | > loader_time: 0.00870  (0.00659)\n\n\n\u001b[1m   --> STEP: 201/406 -- GLOBAL_STEP: 22125\u001b[0m\n     | > loss: 0.00237  (0.00329)\n     | > log_mle: -0.23231  (-0.22671)\n     | > loss_dur: 0.23468  (0.23000)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.90689  (27.20229)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90210  (0.74057)\n     | > loader_time: 0.00710  (0.00683)\n\n\n\u001b[1m   --> STEP: 226/406 -- GLOBAL_STEP: 22150\u001b[0m\n     | > loss: -0.00547  (0.00317)\n     | > log_mle: -0.24719  (-0.22827)\n     | > loss_dur: 0.24172  (0.23144)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 51.97218  (28.31653)\n     | > current_lr: 0.00001 \n     | > step_time: 0.87800  (0.76564)\n     | > loader_time: 0.00560  (0.00695)\n\n\n\u001b[1m   --> STEP: 251/406 -- GLOBAL_STEP: 22175\u001b[0m\n     | > loss: 0.01536  (0.00341)\n     | > log_mle: -0.23584  (-0.22968)\n     | > loss_dur: 0.25120  (0.23309)\n     | > amp_scaler: 8192.00000  (4373.41833)\n     | > grad_norm: 34.24500  (29.16648)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98150  (0.79009)\n     | > loader_time: 0.00780  (0.00705)\n\n\n\u001b[1m   --> STEP: 276/406 -- GLOBAL_STEP: 22200\u001b[0m\n     | > loss: -0.00582  (0.00304)\n     | > log_mle: -0.24457  (-0.23096)\n     | > loss_dur: 0.23875  (0.23400)\n     | > amp_scaler: 8192.00000  (4719.30435)\n     | > grad_norm: 27.96787  (29.84418)\n     | > current_lr: 0.00001 \n     | > step_time: 0.97630  (0.81569)\n     | > loader_time: 0.00880  (0.00716)\n\n\n\u001b[1m   --> STEP: 301/406 -- GLOBAL_STEP: 22225\u001b[0m\n     | > loss: -0.00692  (0.00306)\n     | > log_mle: -0.25942  (-0.23200)\n     | > loss_dur: 0.25250  (0.23507)\n     | > amp_scaler: 8192.00000  (5007.73422)\n     | > grad_norm: 23.04864  (30.05625)\n     | > current_lr: 0.00001 \n     | > step_time: 1.06740  (0.83821)\n     | > loader_time: 0.02080  (0.00735)\n\n\n\u001b[1m   --> STEP: 326/406 -- GLOBAL_STEP: 22250\u001b[0m\n     | > loss: 0.00573  (0.00323)\n     | > log_mle: -0.23638  (-0.23281)\n     | > loss_dur: 0.24211  (0.23604)\n     | > amp_scaler: 8192.00000  (5251.92638)\n     | > grad_norm: 21.39164  (30.10646)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09310  (0.86012)\n     | > loader_time: 0.00840  (0.00742)\n\n\n\u001b[1m   --> STEP: 351/406 -- GLOBAL_STEP: 22275\u001b[0m\n     | > loss: 0.01821  (0.00336)\n     | > log_mle: -0.23953  (-0.23371)\n     | > loss_dur: 0.25774  (0.23707)\n     | > amp_scaler: 8192.00000  (5461.33333)\n     | > grad_norm: 38.81852  (30.69589)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11000  (0.88339)\n     | > loader_time: 0.00920  (0.00758)\n\n\n\u001b[1m   --> STEP: 376/406 -- GLOBAL_STEP: 22300\u001b[0m\n     | > loss: 0.00291  (0.00288)\n     | > log_mle: -0.24336  (-0.23470)\n     | > loss_dur: 0.24627  (0.23758)\n     | > amp_scaler: 4096.00000  (5599.31915)\n     | > grad_norm: 28.92888  (31.25884)\n     | > current_lr: 0.00001 \n     | > step_time: 1.47190  (0.90607)\n     | > loader_time: 0.00770  (0.00770)\n\n\n\u001b[1m   --> STEP: 401/406 -- GLOBAL_STEP: 22325\u001b[0m\n     | > loss: 0.00156  (0.00273)\n     | > log_mle: -0.24691  (-0.23555)\n     | > loss_dur: 0.24847  (0.23828)\n     | > amp_scaler: 4096.00000  (5505.59601)\n     | > grad_norm: 31.12811  (31.56343)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98660  (0.92641)\n     | > loader_time: 0.00740  (0.00781)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00709 \u001b[0m(+0.00160)\n     | > avg_loss:\u001b[92m 0.04793 \u001b[0m(-0.03056)\n     | > avg_log_mle:\u001b[92m -0.18596 \u001b[0m(-0.02872)\n     | > avg_loss_dur:\u001b[92m 0.23389 \u001b[0m(-0.00184)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_22330.pth\n\n\u001b[4m\u001b[1m > EPOCH: 55/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:51:39) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 20/406 -- GLOBAL_STEP: 22350\u001b[0m\n     | > loss: 0.00724  (-0.01505)\n     | > log_mle: -0.19913  (-0.21568)\n     | > loss_dur: 0.20637  (0.20063)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 24.79160  (21.38344)\n     | > current_lr: 0.00001 \n     | > step_time: 0.57500  (0.57042)\n     | > loader_time: 0.00350  (0.00479)\n\n\n\u001b[1m   --> STEP: 45/406 -- GLOBAL_STEP: 22375\u001b[0m\n     | > loss: 0.01116  (-0.00499)\n     | > log_mle: -0.20892  (-0.21546)\n     | > loss_dur: 0.22009  (0.21047)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 25.84638  (23.05075)\n     | > current_lr: 0.00001 \n     | > step_time: 0.57900  (0.59597)\n     | > loader_time: 0.00580  (0.00532)\n\n\n\u001b[1m   --> STEP: 70/406 -- GLOBAL_STEP: 22400\u001b[0m\n     | > loss: -0.01698  (-0.00254)\n     | > log_mle: -0.23264  (-0.21853)\n     | > loss_dur: 0.21566  (0.21600)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.85576  (24.79552)\n     | > current_lr: 0.00001 \n     | > step_time: 0.62310  (0.62366)\n     | > loader_time: 0.00760  (0.00584)\n\n\n\u001b[1m   --> STEP: 95/406 -- GLOBAL_STEP: 22425\u001b[0m\n     | > loss: 0.00560  (-0.00204)\n     | > log_mle: -0.22710  (-0.22179)\n     | > loss_dur: 0.23270  (0.21975)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.08010  (26.11803)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66930  (0.64771)\n     | > loader_time: 0.01440  (0.00607)\n\n\n\u001b[1m   --> STEP: 120/406 -- GLOBAL_STEP: 22450\u001b[0m\n     | > loss: -0.01059  (-0.00206)\n     | > log_mle: -0.24039  (-0.22440)\n     | > loss_dur: 0.22980  (0.22234)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.21355  (27.40346)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74580  (0.66989)\n     | > loader_time: 0.00460  (0.00650)\n\n\n\u001b[1m   --> STEP: 145/406 -- GLOBAL_STEP: 22475\u001b[0m\n     | > loss: -0.01400  (-0.00141)\n     | > log_mle: -0.22828  (-0.22650)\n     | > loss_dur: 0.21428  (0.22508)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.93538  (28.48604)\n     | > current_lr: 0.00001 \n     | > step_time: 0.89970  (0.69089)\n     | > loader_time: 0.00810  (0.00673)\n\n\n\u001b[1m   --> STEP: 170/406 -- GLOBAL_STEP: 22500\u001b[0m\n     | > loss: -0.00836  (-0.00110)\n     | > log_mle: -0.23835  (-0.22816)\n     | > loss_dur: 0.23000  (0.22706)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 34.19994  (29.35513)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90050  (0.70996)\n     | > loader_time: 0.00510  (0.00677)\n\n\n\u001b[1m   --> STEP: 195/406 -- GLOBAL_STEP: 22525\u001b[0m\n     | > loss: 0.01149  (-0.00093)\n     | > log_mle: -0.22335  (-0.22983)\n     | > loss_dur: 0.23483  (0.22890)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.27552  (29.93786)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91230  (0.73561)\n     | > loader_time: 0.00550  (0.00670)\n\n\n\u001b[1m   --> STEP: 220/406 -- GLOBAL_STEP: 22550\u001b[0m\n     | > loss: -0.01584  (-0.00101)\n     | > log_mle: -0.26644  (-0.23129)\n     | > loss_dur: 0.25060  (0.23028)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 60.41493  (30.19587)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96080  (0.76044)\n     | > loader_time: 0.00570  (0.00685)\n\n\n\u001b[1m   --> STEP: 245/406 -- GLOBAL_STEP: 22575\u001b[0m\n     | > loss: 0.00356  (-0.00080)\n     | > log_mle: -0.24202  (-0.23279)\n     | > loss_dur: 0.24558  (0.23199)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 31.43679  (31.18749)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96950  (0.78545)\n     | > loader_time: 0.00660  (0.00697)\n\n\n\u001b[1m   --> STEP: 270/406 -- GLOBAL_STEP: 22600\u001b[0m\n     | > loss: -0.00909  (-0.00122)\n     | > log_mle: -0.24661  (-0.23400)\n     | > loss_dur: 0.23752  (0.23277)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 57.01329  (31.49819)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21050  (0.81005)\n     | > loader_time: 0.04670  (0.00716)\n\n\n\u001b[1m   --> STEP: 295/406 -- GLOBAL_STEP: 22625\u001b[0m\n     | > loss: -0.00348  (-0.00143)\n     | > log_mle: -0.24849  (-0.23495)\n     | > loss_dur: 0.24501  (0.23352)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.16745  (31.76926)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12700  (0.83260)\n     | > loader_time: 0.00670  (0.00723)\n\n\n\u001b[1m   --> STEP: 320/406 -- GLOBAL_STEP: 22650\u001b[0m\n     | > loss: 0.00111  (-0.00129)\n     | > log_mle: -0.23732  (-0.23583)\n     | > loss_dur: 0.23843  (0.23454)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 9.59054  (32.01981)\n     | > current_lr: 0.00001 \n     | > step_time: 1.19020  (0.85559)\n     | > loader_time: 0.00740  (0.00734)\n\n\n\u001b[1m   --> STEP: 345/406 -- GLOBAL_STEP: 22675\u001b[0m\n     | > loss: -0.00710  (-0.00111)\n     | > log_mle: -0.25349  (-0.23661)\n     | > loss_dur: 0.24639  (0.23550)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.03315  (32.77175)\n     | > current_lr: 0.00001 \n     | > step_time: 1.25040  (0.88061)\n     | > loader_time: 0.00900  (0.00758)\n\n\n\u001b[1m   --> STEP: 370/406 -- GLOBAL_STEP: 22700\u001b[0m\n     | > loss: -0.01188  (-0.00137)\n     | > log_mle: -0.26239  (-0.23750)\n     | > loss_dur: 0.25051  (0.23613)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 72.09212  (33.64347)\n     | > current_lr: 0.00001 \n     | > step_time: 1.18210  (0.90925)\n     | > loader_time: 0.00910  (0.00779)\n\n\n\u001b[1m   --> STEP: 395/406 -- GLOBAL_STEP: 22725\u001b[0m\n     | > loss: -0.01340  (-0.00157)\n     | > log_mle: -0.25981  (-0.23828)\n     | > loss_dur: 0.24641  (0.23671)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 39.77447  (33.90841)\n     | > current_lr: 0.00001 \n     | > step_time: 1.30340  (0.93798)\n     | > loader_time: 0.00820  (0.00797)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00696 \u001b[0m(-0.00013)\n     | > avg_loss:\u001b[92m 0.03702 \u001b[0m(-0.01091)\n     | > avg_log_mle:\u001b[92m -0.19583 \u001b[0m(-0.00988)\n     | > avg_loss_dur:\u001b[92m 0.23286 \u001b[0m(-0.00103)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_22736.pth\n\n\u001b[4m\u001b[1m > EPOCH: 56/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 20:58:35) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 14/406 -- GLOBAL_STEP: 22750\u001b[0m\n     | > loss: -0.00185  (-0.02531)\n     | > log_mle: -0.23400  (-0.22325)\n     | > loss_dur: 0.23215  (0.19793)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.01948  (23.27473)\n     | > current_lr: 0.00001 \n     | > step_time: 0.59710  (0.54371)\n     | > loader_time: 0.01210  (0.00560)\n\n\n\u001b[1m   --> STEP: 39/406 -- GLOBAL_STEP: 22775\u001b[0m\n     | > loss: 0.00966  (-0.01156)\n     | > log_mle: -0.21472  (-0.21963)\n     | > loss_dur: 0.22438  (0.20807)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.61550  (23.27056)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63150  (0.57529)\n     | > loader_time: 0.00740  (0.00533)\n\n\n\u001b[1m   --> STEP: 64/406 -- GLOBAL_STEP: 22800\u001b[0m\n     | > loss: -0.01659  (-0.00746)\n     | > log_mle: -0.21977  (-0.22079)\n     | > loss_dur: 0.20317  (0.21333)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 21.71638  (26.43906)\n     | > current_lr: 0.00001 \n     | > step_time: 0.66320  (0.61003)\n     | > loader_time: 0.02860  (0.00589)\n\n\n\u001b[1m   --> STEP: 89/406 -- GLOBAL_STEP: 22825\u001b[0m\n     | > loss: -0.00099  (-0.00649)\n     | > log_mle: -0.22336  (-0.22380)\n     | > loss_dur: 0.22238  (0.21732)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 22.44176  (26.02995)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70510  (0.63002)\n     | > loader_time: 0.00460  (0.00592)\n\n\n\u001b[1m   --> STEP: 114/406 -- GLOBAL_STEP: 22850\u001b[0m\n     | > loss: -0.02106  (-0.00671)\n     | > log_mle: -0.25301  (-0.22719)\n     | > loss_dur: 0.23195  (0.22048)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 43.67239  (26.01438)\n     | > current_lr: 0.00001 \n     | > step_time: 0.71310  (0.65691)\n     | > loader_time: 0.00900  (0.00610)\n\n\n\u001b[1m   --> STEP: 139/406 -- GLOBAL_STEP: 22875\u001b[0m\n     | > loss: 0.01235  (-0.00569)\n     | > log_mle: -0.23958  (-0.22927)\n     | > loss_dur: 0.25193  (0.22357)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 58.79185  (25.95428)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80170  (0.68470)\n     | > loader_time: 0.01460  (0.00648)\n\n\n\u001b[1m   --> STEP: 164/406 -- GLOBAL_STEP: 22900\u001b[0m\n     | > loss: -0.00519  (-0.00515)\n     | > log_mle: -0.23103  (-0.23077)\n     | > loss_dur: 0.22584  (0.22562)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 20.51684  (26.99864)\n     | > current_lr: 0.00001 \n     | > step_time: 0.86750  (0.70563)\n     | > loader_time: 0.00590  (0.00657)\n\n\n\u001b[1m   --> STEP: 189/406 -- GLOBAL_STEP: 22925\u001b[0m\n     | > loss: -0.00246  (-0.00543)\n     | > log_mle: -0.24139  (-0.23253)\n     | > loss_dur: 0.23893  (0.22710)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 36.51493  (27.00011)\n     | > current_lr: 0.00001 \n     | > step_time: 0.89480  (0.72925)\n     | > loader_time: 0.00530  (0.00681)\n\n\n\u001b[1m   --> STEP: 214/406 -- GLOBAL_STEP: 22950\u001b[0m\n     | > loss: -0.00770  (-0.00541)\n     | > log_mle: -0.24512  (-0.23378)\n     | > loss_dur: 0.23742  (0.22836)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 57.32130  (27.48669)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96300  (0.75338)\n     | > loader_time: 0.00570  (0.00683)\n\n\n\u001b[1m   --> STEP: 239/406 -- GLOBAL_STEP: 22975\u001b[0m\n     | > loss: -0.00230  (-0.00520)\n     | > log_mle: -0.25157  (-0.23528)\n     | > loss_dur: 0.24928  (0.23008)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.50964  (29.16796)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96590  (0.78002)\n     | > loader_time: 0.00580  (0.00691)\n\n\n\u001b[1m   --> STEP: 264/406 -- GLOBAL_STEP: 23000\u001b[0m\n     | > loss: 0.00504  (-0.00562)\n     | > log_mle: -0.24231  (-0.23656)\n     | > loss_dur: 0.24735  (0.23094)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 18.98105  (30.16673)\n     | > current_lr: 0.00001 \n     | > step_time: 1.00860  (0.80471)\n     | > loader_time: 0.00820  (0.00712)\n\n\n\u001b[1m   --> STEP: 289/406 -- GLOBAL_STEP: 23025\u001b[0m\n     | > loss: -0.01495  (-0.00567)\n     | > log_mle: -0.23899  (-0.23756)\n     | > loss_dur: 0.22404  (0.23189)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 33.64770  (31.19023)\n     | > current_lr: 0.00001 \n     | > step_time: 1.07630  (0.83026)\n     | > loader_time: 0.00610  (0.00725)\n\n\n\u001b[1m   --> STEP: 314/406 -- GLOBAL_STEP: 23050\u001b[0m\n     | > loss: -0.00541  (-0.00559)\n     | > log_mle: -0.25462  (-0.23846)\n     | > loss_dur: 0.24921  (0.23288)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 32.60086  (31.69752)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31620  (0.85664)\n     | > loader_time: 0.01310  (0.00749)\n\n\n\u001b[1m   --> STEP: 339/406 -- GLOBAL_STEP: 23075\u001b[0m\n     | > loss: 0.00361  (-0.00546)\n     | > log_mle: -0.25007  (-0.23926)\n     | > loss_dur: 0.25369  (0.23380)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.87385  (32.26449)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23980  (0.88299)\n     | > loader_time: 0.01400  (0.00773)\n\n\n\u001b[1m   --> STEP: 364/406 -- GLOBAL_STEP: 23100\u001b[0m\n     | > loss: -0.01223  (-0.00574)\n     | > log_mle: -0.25441  (-0.24019)\n     | > loss_dur: 0.24218  (0.23445)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 37.00473  (32.74893)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42180  (0.90931)\n     | > loader_time: 0.01540  (0.00818)\n\n\n\u001b[1m   --> STEP: 389/406 -- GLOBAL_STEP: 23125\u001b[0m\n     | > loss: -0.02114  (-0.00585)\n     | > log_mle: -0.26411  (-0.24091)\n     | > loss_dur: 0.24298  (0.23507)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 26.42220  (33.44788)\n     | > current_lr: 0.00001 \n     | > step_time: 1.42610  (0.93699)\n     | > loader_time: 0.01780  (0.00853)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00695 \u001b[0m(-0.00000)\n     | > avg_loss:\u001b[92m 0.02923 \u001b[0m(-0.00779)\n     | > avg_log_mle:\u001b[92m -0.20239 \u001b[0m(-0.00656)\n     | > avg_loss_dur:\u001b[92m 0.23163 \u001b[0m(-0.00123)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_23142.pth\n\n\u001b[4m\u001b[1m > EPOCH: 57/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:05:32) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 8/406 -- GLOBAL_STEP: 23150\u001b[0m\n     | > loss: -0.05531  (-0.03193)\n     | > log_mle: -0.23976  (-0.22358)\n     | > loss_dur: 0.18445  (0.19165)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 15.40924  (29.13526)\n     | > current_lr: 0.00001 \n     | > step_time: 0.53300  (0.53636)\n     | > loader_time: 0.00320  (0.00353)\n\n\n\u001b[1m   --> STEP: 33/406 -- GLOBAL_STEP: 23175\u001b[0m\n     | > loss: 0.02417  (-0.01087)\n     | > log_mle: -0.20130  (-0.21794)\n     | > loss_dur: 0.22547  (0.20707)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.58236  (26.80212)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60820  (0.57363)\n     | > loader_time: 0.00370  (0.00478)\n\n\n\u001b[1m   --> STEP: 58/406 -- GLOBAL_STEP: 23200\u001b[0m\n     | > loss: -0.00909  (-0.00751)\n     | > log_mle: -0.22581  (-0.21987)\n     | > loss_dur: 0.21672  (0.21237)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.98509  (28.74553)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67900  (0.61307)\n     | > loader_time: 0.00420  (0.00514)\n\n\n\u001b[1m   --> STEP: 83/406 -- GLOBAL_STEP: 23225\u001b[0m\n     | > loss: 0.00571  (-0.00732)\n     | > log_mle: -0.23312  (-0.22329)\n     | > loss_dur: 0.23883  (0.21598)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 27.62861  (28.21682)\n     | > current_lr: 0.00001 \n     | > step_time: 0.67260  (0.62984)\n     | > loader_time: 0.00590  (0.00547)\n\n\n\u001b[1m   --> STEP: 108/406 -- GLOBAL_STEP: 23250\u001b[0m\n     | > loss: -0.01566  (-0.00793)\n     | > log_mle: -0.23676  (-0.22665)\n     | > loss_dur: 0.22110  (0.21872)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 23.33269  (29.64712)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74320  (0.65368)\n     | > loader_time: 0.00470  (0.00577)\n\n\n\u001b[1m   --> STEP: 133/406 -- GLOBAL_STEP: 23275\u001b[0m\n     | > loss: -0.01428  (-0.00845)\n     | > log_mle: -0.24624  (-0.22992)\n     | > loss_dur: 0.23196  (0.22147)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.43462  (29.35641)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72740  (0.67321)\n     | > loader_time: 0.00540  (0.00601)\n\n\n\u001b[1m   --> STEP: 158/406 -- GLOBAL_STEP: 23300\u001b[0m\n     | > loss: -0.01466  (-0.00808)\n     | > log_mle: -0.25143  (-0.23201)\n     | > loss_dur: 0.23678  (0.22393)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 28.28360  (30.84809)\n     | > current_lr: 0.00001 \n     | > step_time: 0.76870  (0.69120)\n     | > loader_time: 0.00680  (0.00613)\n\n\n\u001b[1m   --> STEP: 183/406 -- GLOBAL_STEP: 23325\u001b[0m\n     | > loss: -0.01425  (-0.00821)\n     | > log_mle: -0.24964  (-0.23374)\n     | > loss_dur: 0.23539  (0.22553)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 41.52615  (33.24538)\n     | > current_lr: 0.00001 \n     | > step_time: 0.77480  (0.71432)\n     | > loader_time: 0.00530  (0.00630)\n\n\n\u001b[1m   --> STEP: 208/406 -- GLOBAL_STEP: 23350\u001b[0m\n     | > loss: -0.02612  (-0.00814)\n     | > log_mle: -0.25108  (-0.23533)\n     | > loss_dur: 0.22496  (0.22719)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 57.45819  (33.26196)\n     | > current_lr: 0.00001 \n     | > step_time: 0.90740  (0.73689)\n     | > loader_time: 0.00520  (0.00658)\n\n\n\u001b[1m   --> STEP: 233/406 -- GLOBAL_STEP: 23375\u001b[0m\n     | > loss: -0.00291  (-0.00834)\n     | > log_mle: -0.25421  (-0.23695)\n     | > loss_dur: 0.25131  (0.22860)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 33.68366  (34.92620)\n     | > current_lr: 0.00001 \n     | > step_time: 1.10660  (0.76061)\n     | > loader_time: 0.00980  (0.00670)\n\n\n\u001b[1m   --> STEP: 258/406 -- GLOBAL_STEP: 23400\u001b[0m\n     | > loss: -0.00718  (-0.00834)\n     | > log_mle: -0.24419  (-0.23824)\n     | > loss_dur: 0.23701  (0.22990)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 53.91946  (35.63539)\n     | > current_lr: 0.00001 \n     | > step_time: 1.00620  (0.78816)\n     | > loader_time: 0.00750  (0.00692)\n\n\n\u001b[1m   --> STEP: 283/406 -- GLOBAL_STEP: 23425\u001b[0m\n     | > loss: -0.00416  (-0.00869)\n     | > log_mle: -0.25088  (-0.23947)\n     | > loss_dur: 0.24672  (0.23078)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 31.22395  (35.80452)\n     | > current_lr: 0.00001 \n     | > step_time: 1.15590  (0.81582)\n     | > loader_time: 0.01390  (0.00726)\n\n\n\u001b[1m   --> STEP: 308/406 -- GLOBAL_STEP: 23450\u001b[0m\n     | > loss: -0.01791  (-0.00866)\n     | > log_mle: -0.25310  (-0.24045)\n     | > loss_dur: 0.23519  (0.23179)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 59.83964  (35.83377)\n     | > current_lr: 0.00001 \n     | > step_time: 1.26670  (0.84497)\n     | > loader_time: 0.01300  (0.00766)\n\n\n\u001b[1m   --> STEP: 333/406 -- GLOBAL_STEP: 23475\u001b[0m\n     | > loss: -0.01079  (-0.00862)\n     | > log_mle: -0.26016  (-0.24125)\n     | > loss_dur: 0.24938  (0.23262)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 48.01106  (35.71145)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21620  (0.87538)\n     | > loader_time: 0.01470  (0.00828)\n\n\n\u001b[1m   --> STEP: 358/406 -- GLOBAL_STEP: 23500\u001b[0m\n     | > loss: -0.04023  (-0.00853)\n     | > log_mle: -0.27117  (-0.24204)\n     | > loss_dur: 0.23093  (0.23351)\n     | > amp_scaler: 2048.00000  (3987.30726)\n     | > grad_norm: 22.32003  (36.96886)\n     | > current_lr: 0.00001 \n     | > step_time: 1.29590  (0.90541)\n     | > loader_time: 0.01670  (0.00884)\n\n\n\u001b[1m   --> STEP: 383/406 -- GLOBAL_STEP: 23525\u001b[0m\n     | > loss: 0.00401  (-0.00871)\n     | > log_mle: -0.25372  (-0.24286)\n     | > loss_dur: 0.25772  (0.23415)\n     | > amp_scaler: 2048.00000  (3860.72063)\n     | > grad_norm: 72.47271  (36.54573)\n     | > current_lr: 0.00001 \n     | > step_time: 1.21460  (0.93462)\n     | > loader_time: 0.00660  (0.00937)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00813 \u001b[0m(+0.00117)\n     | > avg_loss:\u001b[92m 0.00885 \u001b[0m(-0.02038)\n     | > avg_log_mle:\u001b[92m -0.21952 \u001b[0m(-0.01713)\n     | > avg_loss_dur:\u001b[92m 0.22837 \u001b[0m(-0.00326)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_23548.pth\n\n\u001b[4m\u001b[1m > EPOCH: 58/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:12:32) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 2/406 -- GLOBAL_STEP: 23550\u001b[0m\n     | > loss: -0.05085  (-0.04956)\n     | > log_mle: -0.24376  (-0.23627)\n     | > loss_dur: 0.19291  (0.18670)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 35.35830  (25.81896)\n     | > current_lr: 0.00001 \n     | > step_time: 0.47520  (0.50257)\n     | > loader_time: 0.00350  (0.00341)\n\n\n\u001b[1m   --> STEP: 27/406 -- GLOBAL_STEP: 23575\u001b[0m\n     | > loss: -0.02737  (-0.02052)\n     | > log_mle: -0.22090  (-0.22262)\n     | > loss_dur: 0.19353  (0.20210)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 25.69644  (19.46353)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64060  (0.56414)\n     | > loader_time: 0.00360  (0.00442)\n\n\n\u001b[1m   --> STEP: 52/406 -- GLOBAL_STEP: 23600\u001b[0m\n     | > loss: 0.00253  (-0.01240)\n     | > log_mle: -0.22391  (-0.22236)\n     | > loss_dur: 0.22644  (0.20996)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 27.62318  (28.03409)\n     | > current_lr: 0.00001 \n     | > step_time: 0.65320  (0.61314)\n     | > loader_time: 0.00560  (0.00537)\n\n\n\u001b[1m   --> STEP: 77/406 -- GLOBAL_STEP: 23625\u001b[0m\n     | > loss: -0.00688  (-0.01226)\n     | > log_mle: -0.22525  (-0.22581)\n     | > loss_dur: 0.21837  (0.21355)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 31.49118  (28.15279)\n     | > current_lr: 0.00001 \n     | > step_time: 0.74710  (0.64032)\n     | > loader_time: 0.00610  (0.00579)\n\n\n\u001b[1m   --> STEP: 102/406 -- GLOBAL_STEP: 23650\u001b[0m\n     | > loss: -0.03032  (-0.01236)\n     | > log_mle: -0.24999  (-0.22938)\n     | > loss_dur: 0.21967  (0.21703)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 32.23017  (32.60074)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73550  (0.65620)\n     | > loader_time: 0.00510  (0.00575)\n\n\n\u001b[1m   --> STEP: 127/406 -- GLOBAL_STEP: 23675\u001b[0m\n     | > loss: 0.00131  (-0.01310)\n     | > log_mle: -0.24151  (-0.23249)\n     | > loss_dur: 0.24282  (0.21939)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 59.36282  (33.84653)\n     | > current_lr: 0.00001 \n     | > step_time: 0.72450  (0.67921)\n     | > loader_time: 0.00470  (0.00608)\n\n\n\u001b[1m   --> STEP: 152/406 -- GLOBAL_STEP: 23700\u001b[0m\n     | > loss: -0.00001  (-0.01302)\n     | > log_mle: -0.23945  (-0.23471)\n     | > loss_dur: 0.23944  (0.22169)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 16.61144  (33.22202)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82380  (0.70083)\n     | > loader_time: 0.00520  (0.00617)\n\n\n\u001b[1m   --> STEP: 177/406 -- GLOBAL_STEP: 23725\u001b[0m\n     | > loss: -0.03771  (-0.01347)\n     | > log_mle: -0.26144  (-0.23651)\n     | > loss_dur: 0.22372  (0.22304)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 22.42047  (32.59048)\n     | > current_lr: 0.00001 \n     | > step_time: 0.88270  (0.72597)\n     | > loader_time: 0.01550  (0.00645)\n\n\n\u001b[1m   --> STEP: 202/406 -- GLOBAL_STEP: 23750\u001b[0m\n     | > loss: 0.00990  (-0.01346)\n     | > log_mle: -0.23898  (-0.23815)\n     | > loss_dur: 0.24889  (0.22469)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 44.08911  (32.33855)\n     | > current_lr: 0.00001 \n     | > step_time: 0.82500  (0.74916)\n     | > loader_time: 0.00790  (0.00653)\n\n\n\u001b[1m   --> STEP: 227/406 -- GLOBAL_STEP: 23775\u001b[0m\n     | > loss: 0.01963  (-0.01360)\n     | > log_mle: -0.23808  (-0.23968)\n     | > loss_dur: 0.25771  (0.22608)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 29.00950  (32.68164)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94720  (0.77410)\n     | > loader_time: 0.00600  (0.00659)\n\n\n\u001b[1m   --> STEP: 252/406 -- GLOBAL_STEP: 23800\u001b[0m\n     | > loss: -0.00676  (-0.01366)\n     | > log_mle: -0.25280  (-0.24116)\n     | > loss_dur: 0.24604  (0.22750)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 24.01198  (33.70533)\n     | > current_lr: 0.00001 \n     | > step_time: 1.08090  (0.79863)\n     | > loader_time: 0.00610  (0.00669)\n\n\n\u001b[1m   --> STEP: 277/406 -- GLOBAL_STEP: 23825\u001b[0m\n     | > loss: -0.02376  (-0.01439)\n     | > log_mle: -0.25092  (-0.24245)\n     | > loss_dur: 0.22716  (0.22806)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 43.98579  (33.83121)\n     | > current_lr: 0.00001 \n     | > step_time: 1.11080  (0.82437)\n     | > loader_time: 0.01290  (0.00690)\n\n\n\u001b[1m   --> STEP: 302/406 -- GLOBAL_STEP: 23850\u001b[0m\n     | > loss: -0.00758  (-0.01441)\n     | > log_mle: -0.26087  (-0.24354)\n     | > loss_dur: 0.25329  (0.22914)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 56.85335  (34.24498)\n     | > current_lr: 0.00001 \n     | > step_time: 1.16200  (0.85343)\n     | > loader_time: 0.01550  (0.00736)\n\n\n\u001b[1m   --> STEP: 327/406 -- GLOBAL_STEP: 23875\u001b[0m\n     | > loss: -0.01357  (-0.01453)\n     | > log_mle: -0.25810  (-0.24445)\n     | > loss_dur: 0.24453  (0.22991)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 39.13845  (34.79189)\n     | > current_lr: 0.00001 \n     | > step_time: 1.17150  (0.88256)\n     | > loader_time: 0.01590  (0.00791)\n\n\n\u001b[1m   --> STEP: 352/406 -- GLOBAL_STEP: 23900\u001b[0m\n     | > loss: -0.01946  (-0.01447)\n     | > log_mle: -0.25168  (-0.24543)\n     | > loss_dur: 0.23222  (0.23095)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 39.81605  (35.47332)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28090  (0.91132)\n     | > loader_time: 0.01250  (0.00845)\n\n\n\u001b[1m   --> STEP: 377/406 -- GLOBAL_STEP: 23925\u001b[0m\n     | > loss: -0.01555  (-0.01501)\n     | > log_mle: -0.25618  (-0.24647)\n     | > loss_dur: 0.24063  (0.23146)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 37.26826  (36.37727)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31670  (0.94155)\n     | > loader_time: 0.01720  (0.00897)\n\n\n\u001b[1m   --> STEP: 402/406 -- GLOBAL_STEP: 23950\u001b[0m\n     | > loss: -0.02163  (-0.01532)\n     | > log_mle: -0.26821  (-0.24740)\n     | > loss_dur: 0.24658  (0.23208)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 32.21005  (36.66245)\n     | > current_lr: 0.00001 \n     | > step_time: 0.96540  (0.96414)\n     | > loader_time: 0.00960  (0.00913)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00782 \u001b[0m(-0.00030)\n     | > avg_loss:\u001b[91m 0.03601 \u001b[0m(+0.02716)\n     | > avg_log_mle:\u001b[91m -0.18979 \u001b[0m(+0.02973)\n     | > avg_loss_dur:\u001b[92m 0.22580 \u001b[0m(-0.00257)\n\n\n\u001b[4m\u001b[1m > EPOCH: 59/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:19:37) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 21/406 -- GLOBAL_STEP: 23975\u001b[0m\n     | > loss: -0.04489  (-0.03222)\n     | > log_mle: -0.23412  (-0.22940)\n     | > loss_dur: 0.18924  (0.19719)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 36.29785  (28.59420)\n     | > current_lr: 0.00001 \n     | > step_time: 0.53270  (0.58133)\n     | > loader_time: 0.00520  (0.00413)\n\n\n\u001b[1m   --> STEP: 46/406 -- GLOBAL_STEP: 24000\u001b[0m\n     | > loss: -0.01592  (-0.02160)\n     | > log_mle: -0.22841  (-0.22685)\n     | > loss_dur: 0.21249  (0.20524)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 24.59515  (27.03674)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60420  (0.60262)\n     | > loader_time: 0.00400  (0.00500)\n\n\n\u001b[1m   --> STEP: 71/406 -- GLOBAL_STEP: 24025\u001b[0m\n     | > loss: -0.01236  (-0.01910)\n     | > log_mle: -0.23799  (-0.22953)\n     | > loss_dur: 0.22563  (0.21042)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 11.65984  (27.04178)\n     | > current_lr: 0.00001 \n     | > step_time: 0.70730  (0.63260)\n     | > loader_time: 0.00450  (0.00572)\n\n\n\u001b[1m   --> STEP: 96/406 -- GLOBAL_STEP: 24050\u001b[0m\n     | > loss: -0.02426  (-0.01889)\n     | > log_mle: -0.24675  (-0.23260)\n     | > loss_dur: 0.22249  (0.21371)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 23.53533  (30.43935)\n     | > current_lr: 0.00001 \n     | > step_time: 0.73880  (0.64864)\n     | > loader_time: 0.00630  (0.00584)\n\n\n\u001b[1m   --> STEP: 121/406 -- GLOBAL_STEP: 24075\u001b[0m\n     | > loss: -0.02982  (-0.01905)\n     | > log_mle: -0.24279  (-0.23523)\n     | > loss_dur: 0.21297  (0.21618)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 26.62933  (31.71240)\n     | > current_lr: 0.00001 \n     | > step_time: 0.76180  (0.67490)\n     | > loader_time: 0.00730  (0.00590)\n\n\n\u001b[1m   --> STEP: 146/406 -- GLOBAL_STEP: 24100\u001b[0m\n     | > loss: -0.00318  (-0.01884)\n     | > log_mle: -0.23838  (-0.23754)\n     | > loss_dur: 0.23520  (0.21870)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 66.20181  (32.51664)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75660  (0.69975)\n     | > loader_time: 0.00480  (0.00611)\n\n\n\u001b[1m   --> STEP: 171/406 -- GLOBAL_STEP: 24125\u001b[0m\n     | > loss: -0.03730  (-0.01858)\n     | > log_mle: -0.25211  (-0.23938)\n     | > loss_dur: 0.21480  (0.22080)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 47.38435  (32.88947)\n     | > current_lr: 0.00001 \n     | > step_time: 0.81050  (0.72079)\n     | > loader_time: 0.00520  (0.00628)\n\n\n\u001b[1m   --> STEP: 196/406 -- GLOBAL_STEP: 24150\u001b[0m\n     | > loss: -0.02711  (-0.01864)\n     | > log_mle: -0.25518  (-0.24106)\n     | > loss_dur: 0.22807  (0.22243)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 18.86054  (34.03935)\n     | > current_lr: 0.00001 \n     | > step_time: 0.95570  (0.74529)\n     | > loader_time: 0.00540  (0.00649)\n\n\n\u001b[1m   --> STEP: 221/406 -- GLOBAL_STEP: 24175\u001b[0m\n     | > loss: -0.00620  (-0.01884)\n     | > log_mle: -0.24653  (-0.24260)\n     | > loss_dur: 0.24034  (0.22376)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 31.61575  (33.28911)\n     | > current_lr: 0.00001 \n     | > step_time: 1.01330  (0.77010)\n     | > loader_time: 0.00650  (0.00667)\n\n\n\u001b[1m   --> STEP: 246/406 -- GLOBAL_STEP: 24200\u001b[0m\n     | > loss: -0.01676  (-0.01871)\n     | > log_mle: -0.25222  (-0.24414)\n     | > loss_dur: 0.23546  (0.22543)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 57.09922  (34.52549)\n     | > current_lr: 0.00001 \n     | > step_time: 1.38940  (0.79345)\n     | > loader_time: 0.00640  (0.00670)\n\n\n\u001b[1m   --> STEP: 271/406 -- GLOBAL_STEP: 24225\u001b[0m\n     | > loss: -0.03472  (-0.01910)\n     | > log_mle: -0.26751  (-0.24538)\n     | > loss_dur: 0.23279  (0.22628)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 48.49202  (35.86735)\n     | > current_lr: 0.00001 \n     | > step_time: 1.02930  (0.81646)\n     | > loader_time: 0.00910  (0.00681)\n\n\n\u001b[1m   --> STEP: 296/406 -- GLOBAL_STEP: 24250\u001b[0m\n     | > loss: -0.01232  (-0.01924)\n     | > log_mle: -0.25862  (-0.24645)\n     | > loss_dur: 0.24630  (0.22721)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 60.99765  (36.14459)\n     | > current_lr: 0.00001 \n     | > step_time: 1.09690  (0.84194)\n     | > loader_time: 0.01370  (0.00698)\n\n\n\u001b[1m   --> STEP: 321/406 -- GLOBAL_STEP: 24275\u001b[0m\n     | > loss: -0.01270  (-0.01919)\n     | > log_mle: -0.25118  (-0.24738)\n     | > loss_dur: 0.23849  (0.22820)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 73.21423  (36.99204)\n     | > current_lr: 0.00001 \n     | > step_time: 1.23130  (0.86866)\n     | > loader_time: 0.00870  (0.00721)\n\n\n\u001b[1m   --> STEP: 346/406 -- GLOBAL_STEP: 24300\u001b[0m\n     | > loss: -0.01704  (-0.01905)\n     | > log_mle: -0.25506  (-0.24817)\n     | > loss_dur: 0.23802  (0.22913)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 14.17595  (37.34249)\n     | > current_lr: 0.00001 \n     | > step_time: 1.35890  (0.89611)\n     | > loader_time: 0.01190  (0.00758)\n\n\n\u001b[1m   --> STEP: 371/406 -- GLOBAL_STEP: 24325\u001b[0m\n     | > loss: -0.02217  (-0.01939)\n     | > log_mle: -0.26652  (-0.24914)\n     | > loss_dur: 0.24435  (0.22975)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 45.20980  (38.16529)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31240  (0.92556)\n     | > loader_time: 0.01290  (0.00804)\n\n\n\u001b[1m   --> STEP: 396/406 -- GLOBAL_STEP: 24350\u001b[0m\n     | > loss: -0.03709  (-0.01964)\n     | > log_mle: -0.27937  (-0.25003)\n     | > loss_dur: 0.24228  (0.23039)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 53.88805  (38.65131)\n     | > current_lr: 0.00001 \n     | > step_time: 1.55270  (0.95559)\n     | > loader_time: 0.00990  (0.00840)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00860 \u001b[0m(+0.00077)\n     | > avg_loss:\u001b[91m 0.03785 \u001b[0m(+0.00184)\n     | > avg_log_mle:\u001b[91m -0.18850 \u001b[0m(+0.00129)\n     | > avg_loss_dur:\u001b[91m 0.22635 \u001b[0m(+0.00055)\n\n\n\u001b[4m\u001b[1m > EPOCH: 60/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:26:37) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 15/406 -- GLOBAL_STEP: 24375\u001b[0m\n     | > loss: -0.03360  (-0.04331)\n     | > log_mle: -0.23339  (-0.23548)\n     | > loss_dur: 0.19979  (0.19217)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 19.35928  (29.35670)\n     | > current_lr: 0.00001 \n     | > step_time: 0.55380  (0.56449)\n     | > loader_time: 0.00620  (0.00412)\n\n\n\u001b[1m   --> STEP: 40/406 -- GLOBAL_STEP: 24400\u001b[0m\n     | > loss: -0.00133  (-0.02961)\n     | > log_mle: -0.21845  (-0.23118)\n     | > loss_dur: 0.21712  (0.20157)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 16.94313  (26.67939)\n     | > current_lr: 0.00001 \n     | > step_time: 0.63910  (0.59204)\n     | > loader_time: 0.00430  (0.00504)\n\n\n\u001b[1m   --> STEP: 65/406 -- GLOBAL_STEP: 24425\u001b[0m\n     | > loss: -0.01918  (-0.02471)\n     | > log_mle: -0.24130  (-0.23189)\n     | > loss_dur: 0.22212  (0.20718)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 62.64581  (29.46662)\n     | > current_lr: 0.00001 \n     | > step_time: 0.60780  (0.62090)\n     | > loader_time: 0.00440  (0.00556)\n\n\n\u001b[1m   --> STEP: 90/406 -- GLOBAL_STEP: 24450\u001b[0m\n     | > loss: -0.02444  (-0.02406)\n     | > log_mle: -0.24725  (-0.23498)\n     | > loss_dur: 0.22280  (0.21092)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 27.89938  (29.85867)\n     | > current_lr: 0.00001 \n     | > step_time: 0.64940  (0.63879)\n     | > loader_time: 0.00740  (0.00596)\n\n\n\u001b[1m   --> STEP: 115/406 -- GLOBAL_STEP: 24475\u001b[0m\n     | > loss: -0.00969  (-0.02464)\n     | > log_mle: -0.23564  (-0.23836)\n     | > loss_dur: 0.22595  (0.21372)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 44.97309  (31.11226)\n     | > current_lr: 0.00001 \n     | > step_time: 0.75560  (0.66694)\n     | > loader_time: 0.00750  (0.00643)\n\n\n\u001b[1m   --> STEP: 140/406 -- GLOBAL_STEP: 24500\u001b[0m\n     | > loss: -0.01546  (-0.02417)\n     | > log_mle: -0.25566  (-0.24084)\n     | > loss_dur: 0.24020  (0.21668)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 37.31075  (31.29848)\n     | > current_lr: 0.00001 \n     | > step_time: 0.77960  (0.69086)\n     | > loader_time: 0.00540  (0.00654)\n\n\n\u001b[1m   --> STEP: 165/406 -- GLOBAL_STEP: 24525\u001b[0m\n     | > loss: -0.02867  (-0.02379)\n     | > log_mle: -0.26585  (-0.24259)\n     | > loss_dur: 0.23718  (0.21880)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 25.99979  (31.50186)\n     | > current_lr: 0.00001 \n     | > step_time: 0.80500  (0.70999)\n     | > loader_time: 0.00740  (0.00660)\n\n\n\u001b[1m   --> STEP: 190/406 -- GLOBAL_STEP: 24550\u001b[0m\n     | > loss: -0.02228  (-0.02416)\n     | > log_mle: -0.26047  (-0.24434)\n     | > loss_dur: 0.23819  (0.22018)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 64.65522  (31.29837)\n     | > current_lr: 0.00001 \n     | > step_time: 0.91020  (0.73436)\n     | > loader_time: 0.00720  (0.00686)\n\n\n\u001b[1m   --> STEP: 215/406 -- GLOBAL_STEP: 24575\u001b[0m\n     | > loss: -0.04059  (-0.02441)\n     | > log_mle: -0.26256  (-0.24578)\n     | > loss_dur: 0.22197  (0.22137)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 44.84377  (34.23772)\n     | > current_lr: 0.00001 \n     | > step_time: 0.94930  (0.75951)\n     | > loader_time: 0.00600  (0.00701)\n\n\n\u001b[1m   --> STEP: 240/406 -- GLOBAL_STEP: 24600\u001b[0m\n     | > loss: -0.04047  (-0.02434)\n     | > log_mle: -0.27111  (-0.24745)\n     | > loss_dur: 0.23064  (0.22311)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 25.69867  (35.50816)\n     | > current_lr: 0.00001 \n     | > step_time: 1.28030  (0.78299)\n     | > loader_time: 0.00740  (0.00695)\n\n\n\u001b[1m   --> STEP: 265/406 -- GLOBAL_STEP: 24625\u001b[0m\n     | > loss: -0.02611  (-0.02460)\n     | > log_mle: -0.26172  (-0.24860)\n     | > loss_dur: 0.23562  (0.22400)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 37.52662  (35.80593)\n     | > current_lr: 0.00001 \n     | > step_time: 0.98760  (0.80751)\n     | > loader_time: 0.00600  (0.00700)\n\n\n\u001b[1m   --> STEP: 290/406 -- GLOBAL_STEP: 24650\u001b[0m\n     | > loss: -0.03865  (-0.02481)\n     | > log_mle: -0.27113  (-0.24966)\n     | > loss_dur: 0.23248  (0.22485)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 74.52286  (36.44602)\n     | > current_lr: 0.00001 \n     | > step_time: 1.12380  (0.83302)\n     | > loader_time: 0.00610  (0.00709)\n\n\n\u001b[1m   --> STEP: 315/406 -- GLOBAL_STEP: 24675\u001b[0m\n     | > loss: -0.05447  (-0.02480)\n     | > log_mle: -0.27365  (-0.25065)\n     | > loss_dur: 0.21918  (0.22585)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 38.16019  (37.20481)\n     | > current_lr: 0.00001 \n     | > step_time: 1.14180  (0.86252)\n     | > loader_time: 0.00930  (0.00731)\n\n\n\u001b[1m   --> STEP: 340/406 -- GLOBAL_STEP: 24700\u001b[0m\n     | > loss: -0.00636  (-0.02458)\n     | > log_mle: -0.24722  (-0.25139)\n     | > loss_dur: 0.24086  (0.22682)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 45.77937  (37.75137)\n     | > current_lr: 0.00001 \n     | > step_time: 1.20720  (0.89156)\n     | > loader_time: 0.00670  (0.00751)\n\n\n\u001b[1m   --> STEP: 365/406 -- GLOBAL_STEP: 24725\u001b[0m\n     | > loss: -0.03545  (-0.02489)\n     | > log_mle: -0.26405  (-0.25237)\n     | > loss_dur: 0.22860  (0.22748)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 42.06391  (38.71030)\n     | > current_lr: 0.00001 \n     | > step_time: 1.31910  (0.92133)\n     | > loader_time: 0.01490  (0.00794)\n\n\n\u001b[1m   --> STEP: 390/406 -- GLOBAL_STEP: 24750\u001b[0m\n     | > loss: -0.03503  (-0.02497)\n     | > log_mle: -0.27185  (-0.25318)\n     | > loss_dur: 0.23682  (0.22821)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 53.07776  (38.92202)\n     | > current_lr: 0.00001 \n     | > step_time: 1.33210  (0.95113)\n     | > loader_time: 0.01700  (0.00839)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00617 \u001b[0m(-0.00243)\n     | > avg_loss:\u001b[92m -0.00763 \u001b[0m(-0.04548)\n     | > avg_log_mle:\u001b[92m -0.22958 \u001b[0m(-0.04108)\n     | > avg_loss_dur:\u001b[92m 0.22195 \u001b[0m(-0.00440)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_24766.pth\n\n\u001b[4m\u001b[1m > EPOCH: 61/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:33:43) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 9/406 -- GLOBAL_STEP: 24775\u001b[0m\n     | > loss: -0.01640  (-0.04604)\n     | > log_mle: -0.23322  (-0.23547)\n     | > loss_dur: 0.21682  (0.18943)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 23.66472  (33.73587)\n     | > current_lr: 0.00002 \n     | > step_time: 0.51320  (0.53938)\n     | > loader_time: 0.00740  (0.00522)\n\n\n\u001b[1m   --> STEP: 34/406 -- GLOBAL_STEP: 24800\u001b[0m\n     | > loss: 0.00465  (-0.03517)\n     | > log_mle: -0.22199  (-0.23382)\n     | > loss_dur: 0.22665  (0.19865)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 20.93020  (28.39702)\n     | > current_lr: 0.00002 \n     | > step_time: 0.68640  (0.58864)\n     | > loader_time: 0.00420  (0.00765)\n\n\n\u001b[1m   --> STEP: 59/406 -- GLOBAL_STEP: 24825\u001b[0m\n     | > loss: -0.01857  (-0.03003)\n     | > log_mle: -0.24521  (-0.23527)\n     | > loss_dur: 0.22664  (0.20524)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 38.28639  (30.86639)\n     | > current_lr: 0.00002 \n     | > step_time: 0.66330  (0.60913)\n     | > loader_time: 0.00960  (0.00825)\n\n\n\u001b[1m   --> STEP: 84/406 -- GLOBAL_STEP: 24850\u001b[0m\n     | > loss: -0.03704  (-0.02837)\n     | > log_mle: -0.25210  (-0.23764)\n     | > loss_dur: 0.21506  (0.20927)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 18.10099  (32.40223)\n     | > current_lr: 0.00002 \n     | > step_time: 0.75870  (0.64141)\n     | > loader_time: 0.00860  (0.00892)\n\n\n\u001b[1m   --> STEP: 109/406 -- GLOBAL_STEP: 24875\u001b[0m\n     | > loss: -0.01927  (-0.02830)\n     | > log_mle: -0.25857  (-0.24057)\n     | > loss_dur: 0.23929  (0.21227)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 45.10976  (33.80080)\n     | > current_lr: 0.00002 \n     | > step_time: 0.67300  (0.65828)\n     | > loader_time: 0.00630  (0.00903)\n\n\n\u001b[1m   --> STEP: 134/406 -- GLOBAL_STEP: 24900\u001b[0m\n     | > loss: -0.01039  (-0.02857)\n     | > log_mle: -0.25155  (-0.24336)\n     | > loss_dur: 0.24116  (0.21479)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 36.43400  (34.27200)\n     | > current_lr: 0.00002 \n     | > step_time: 0.70990  (0.68208)\n     | > loader_time: 0.01140  (0.00954)\n\n\n\u001b[1m   --> STEP: 159/406 -- GLOBAL_STEP: 24925\u001b[0m\n     | > loss: -0.01595  (-0.02802)\n     | > log_mle: -0.25271  (-0.24523)\n     | > loss_dur: 0.23676  (0.21722)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 35.91020  (35.42321)\n     | > current_lr: 0.00002 \n     | > step_time: 1.25640  (0.70712)\n     | > loader_time: 0.02570  (0.00977)\n\n\n\u001b[1m   --> STEP: 184/406 -- GLOBAL_STEP: 24950\u001b[0m\n     | > loss: -0.01322  (-0.02827)\n     | > log_mle: -0.27154  (-0.24704)\n     | > loss_dur: 0.25832  (0.21878)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 46.84997  (36.17470)\n     | > current_lr: 0.00002 \n     | > step_time: 0.85960  (0.72769)\n     | > loader_time: 0.01280  (0.01014)\n\n\n\u001b[1m   --> STEP: 209/406 -- GLOBAL_STEP: 24975\u001b[0m\n     | > loss: -0.03743  (-0.02785)\n     | > log_mle: -0.26940  (-0.24837)\n     | > loss_dur: 0.23196  (0.22052)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 28.44142  (38.53206)\n     | > current_lr: 0.00002 \n     | > step_time: 0.86380  (0.75134)\n     | > loader_time: 0.01300  (0.01043)\n\n\n\u001b[1m   --> STEP: 234/406 -- GLOBAL_STEP: 25000\u001b[0m\n     | > loss: -0.02395  (-0.02790)\n     | > log_mle: -0.27056  (-0.25005)\n     | > loss_dur: 0.24661  (0.22215)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 50.96938  (38.76075)\n     | > current_lr: 0.00002 \n     | > step_time: 1.13520  (0.77711)\n     | > loader_time: 0.00740  (0.01075)\n\n\n\u001b[1m   --> STEP: 259/406 -- GLOBAL_STEP: 25025\u001b[0m\n     | > loss: -0.03262  (-0.02802)\n     | > log_mle: -0.27302  (-0.25129)\n     | > loss_dur: 0.24040  (0.22327)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 52.23618  (39.57463)\n     | > current_lr: 0.00002 \n     | > step_time: 1.04870  (0.80177)\n     | > loader_time: 0.03510  (0.01093)\n\n\n\u001b[1m   --> STEP: 284/406 -- GLOBAL_STEP: 25050\u001b[0m\n     | > loss: -0.02474  (-0.02847)\n     | > log_mle: -0.26821  (-0.25231)\n     | > loss_dur: 0.24347  (0.22384)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 71.28950  (40.26586)\n     | > current_lr: 0.00002 \n     | > step_time: 1.09540  (0.82530)\n     | > loader_time: 0.00600  (0.01086)\n\n\n\u001b[1m   --> STEP: 309/406 -- GLOBAL_STEP: 25075\u001b[0m\n     | > loss: -0.03892  (-0.02828)\n     | > log_mle: -0.26724  (-0.25317)\n     | > loss_dur: 0.22833  (0.22490)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 39.66536  (40.42096)\n     | > current_lr: 0.00002 \n     | > step_time: 1.06520  (0.85078)\n     | > loader_time: 0.00750  (0.01095)\n\n\n\u001b[1m   --> STEP: 334/406 -- GLOBAL_STEP: 25100\u001b[0m\n     | > loss: -0.03001  (-0.02820)\n     | > log_mle: -0.27565  (-0.25395)\n     | > loss_dur: 0.24564  (0.22575)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 65.31349  (40.42217)\n     | > current_lr: 0.00002 \n     | > step_time: 1.06640  (0.87475)\n     | > loader_time: 0.00870  (0.01098)\n\n\n\u001b[1m   --> STEP: 359/406 -- GLOBAL_STEP: 25125\u001b[0m\n     | > loss: -0.03910  (-0.02838)\n     | > log_mle: -0.26394  (-0.25488)\n     | > loss_dur: 0.22484  (0.22650)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 38.82203  (41.39606)\n     | > current_lr: 0.00002 \n     | > step_time: 1.39020  (0.90047)\n     | > loader_time: 0.01810  (0.01094)\n\n\n\u001b[1m   --> STEP: 384/406 -- GLOBAL_STEP: 25150\u001b[0m\n     | > loss: -0.03260  (-0.02878)\n     | > log_mle: -0.26531  (-0.25583)\n     | > loss_dur: 0.23271  (0.22705)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 45.87089  (41.76735)\n     | > current_lr: 0.00002 \n     | > step_time: 1.33970  (0.92668)\n     | > loader_time: 0.02050  (0.01090)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.00784 \u001b[0m(+0.00167)\n     | > avg_loss:\u001b[91m 0.00854 \u001b[0m(+0.01617)\n     | > avg_log_mle:\u001b[91m -0.21262 \u001b[0m(+0.01696)\n     | > avg_loss_dur:\u001b[92m 0.22116 \u001b[0m(-0.00080)\n\n\n\u001b[4m\u001b[1m > EPOCH: 62/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:40:39) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 3/406 -- GLOBAL_STEP: 25175\u001b[0m\n     | > loss: -0.03056  (-0.05824)\n     | > log_mle: -0.22858  (-0.24186)\n     | > loss_dur: 0.19802  (0.18362)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 33.59940  (34.86519)\n     | > current_lr: 0.00002 \n     | > step_time: 0.53960  (0.55947)\n     | > loader_time: 0.00460  (0.00864)\n\n\n\u001b[1m   --> STEP: 28/406 -- GLOBAL_STEP: 25200\u001b[0m\n     | > loss: -0.00172  (-0.03076)\n     | > log_mle: -0.23364  (-0.22819)\n     | > loss_dur: 0.23192  (0.19743)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 22.01997  (29.90535)\n     | > current_lr: 0.00002 \n     | > step_time: 0.60440  (0.58017)\n     | > loader_time: 0.01470  (0.00724)\n\n\n\u001b[1m   --> STEP: 53/406 -- GLOBAL_STEP: 25225\u001b[0m\n     | > loss: -0.03768  (-0.02844)\n     | > log_mle: -0.23420  (-0.23187)\n     | > loss_dur: 0.19652  (0.20343)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 37.99734  (30.78603)\n     | > current_lr: 0.00002 \n     | > step_time: 0.68100  (0.60790)\n     | > loader_time: 0.00440  (0.00732)\n\n\n\u001b[1m   --> STEP: 78/406 -- GLOBAL_STEP: 25250\u001b[0m\n     | > loss: -0.03429  (-0.02922)\n     | > log_mle: -0.24423  (-0.23636)\n     | > loss_dur: 0.20994  (0.20715)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 30.33384  (32.04475)\n     | > current_lr: 0.00002 \n     | > step_time: 0.70140  (0.63655)\n     | > loader_time: 0.00540  (0.00779)\n\n\n\u001b[1m   --> STEP: 103/406 -- GLOBAL_STEP: 25275\u001b[0m\n     | > loss: -0.05786  (-0.03032)\n     | > log_mle: -0.27420  (-0.24069)\n     | > loss_dur: 0.21634  (0.21036)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 38.65716  (33.71304)\n     | > current_lr: 0.00002 \n     | > step_time: 0.69750  (0.65869)\n     | > loader_time: 0.00430  (0.00781)\n\n\n\u001b[1m   --> STEP: 128/406 -- GLOBAL_STEP: 25300\u001b[0m\n     | > loss: -0.04084  (-0.03094)\n     | > log_mle: -0.26293  (-0.24384)\n     | > loss_dur: 0.22209  (0.21290)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 23.52965  (34.39798)\n     | > current_lr: 0.00002 \n     | > step_time: 0.83130  (0.68388)\n     | > loader_time: 0.01940  (0.00822)\n\n\n\u001b[1m   --> STEP: 153/406 -- GLOBAL_STEP: 25325\u001b[0m\n     | > loss: -0.01966  (-0.03081)\n     | > log_mle: -0.25204  (-0.24611)\n     | > loss_dur: 0.23238  (0.21530)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 60.66245  (34.74184)\n     | > current_lr: 0.00002 \n     | > step_time: 1.00020  (0.70394)\n     | > loader_time: 0.00530  (0.00833)\n\n\n\u001b[1m   --> STEP: 178/406 -- GLOBAL_STEP: 25350\u001b[0m\n     | > loss: -0.01275  (-0.03114)\n     | > log_mle: -0.24492  (-0.24783)\n     | > loss_dur: 0.23218  (0.21669)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 19.14937  (34.79500)\n     | > current_lr: 0.00002 \n     | > step_time: 0.99690  (0.72886)\n     | > loader_time: 0.01230  (0.00870)\n\n\n\u001b[1m   --> STEP: 203/406 -- GLOBAL_STEP: 25375\u001b[0m\n     | > loss: -0.01647  (-0.03112)\n     | > log_mle: -0.25284  (-0.24949)\n     | > loss_dur: 0.23637  (0.21837)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 46.65986  (36.76882)\n     | > current_lr: 0.00002 \n     | > step_time: 0.88930  (0.75352)\n     | > loader_time: 0.02970  (0.00923)\n\n\n\u001b[1m   --> STEP: 228/406 -- GLOBAL_STEP: 25400\u001b[0m\n     | > loss: -0.04335  (-0.03164)\n     | > log_mle: -0.27367  (-0.25134)\n     | > loss_dur: 0.23032  (0.21970)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 68.66071  (37.92786)\n     | > current_lr: 0.00002 \n     | > step_time: 0.99580  (0.77824)\n     | > loader_time: 0.00600  (0.00948)\n\n\n\u001b[1m   --> STEP: 253/406 -- GLOBAL_STEP: 25425\u001b[0m\n     | > loss: -0.04323  (-0.03170)\n     | > log_mle: -0.27924  (-0.25285)\n     | > loss_dur: 0.23601  (0.22115)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 79.81648  (39.38091)\n     | > current_lr: 0.00002 \n     | > step_time: 0.99550  (0.80090)\n     | > loader_time: 0.01800  (0.00970)\n\n\n\u001b[1m   --> STEP: 278/406 -- GLOBAL_STEP: 25450\u001b[0m\n     | > loss: -0.02990  (-0.03221)\n     | > log_mle: -0.24963  (-0.25398)\n     | > loss_dur: 0.21974  (0.22177)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 48.95842  (39.97647)\n     | > current_lr: 0.00002 \n     | > step_time: 1.08280  (0.82523)\n     | > loader_time: 0.00720  (0.00977)\n\n\n\u001b[1m   --> STEP: 303/406 -- GLOBAL_STEP: 25475\u001b[0m\n     | > loss: -0.01940  (-0.03239)\n     | > log_mle: -0.26708  (-0.25516)\n     | > loss_dur: 0.24768  (0.22278)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 47.70135  (40.73776)\n     | > current_lr: 0.00002 \n     | > step_time: 1.06710  (0.84978)\n     | > loader_time: 0.00790  (0.00980)\n\n\n\u001b[1m   --> STEP: 328/406 -- GLOBAL_STEP: 25500\u001b[0m\n     | > loss: -0.02141  (-0.03259)\n     | > log_mle: -0.27200  (-0.25610)\n     | > loss_dur: 0.25059  (0.22350)\n     | > amp_scaler: 4096.00000  (2166.63415)\n     | > grad_norm: 42.39011  (41.13173)\n     | > current_lr: 0.00002 \n     | > step_time: 1.25630  (0.87598)\n     | > loader_time: 0.00810  (0.00989)\n\n\n\u001b[1m   --> STEP: 353/406 -- GLOBAL_STEP: 25525\u001b[0m\n     | > loss: -0.04056  (-0.03264)\n     | > log_mle: -0.28161  (-0.25703)\n     | > loss_dur: 0.24105  (0.22439)\n     | > amp_scaler: 4096.00000  (2303.27479)\n     | > grad_norm: 35.54040  (40.96535)\n     | > current_lr: 0.00002 \n     | > step_time: 1.30160  (0.90396)\n     | > loader_time: 0.00830  (0.00989)\n\n\n\u001b[1m   --> STEP: 378/406 -- GLOBAL_STEP: 25550\u001b[0m\n     | > loss: -0.02013  (-0.03299)\n     | > log_mle: -0.26313  (-0.25793)\n     | > loss_dur: 0.24300  (0.22495)\n     | > amp_scaler: 4096.00000  (2421.84127)\n     | > grad_norm: 122.05257  (42.17419)\n     | > current_lr: 0.00002 \n     | > step_time: 1.30000  (0.93302)\n     | > loader_time: 0.00880  (0.00984)\n\n\n\u001b[1m   --> STEP: 403/406 -- GLOBAL_STEP: 25575\u001b[0m\n     | > loss: -0.04545  (-0.03315)\n     | > log_mle: -0.26355  (-0.25873)\n     | > loss_dur: 0.21811  (0.22558)\n     | > amp_scaler: 4096.00000  (2525.69727)\n     | > grad_norm: 34.99540  (42.42979)\n     | > current_lr: 0.00002 \n     | > step_time: 0.93120  (0.95388)\n     | > loader_time: 0.00660  (0.00983)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[91m 0.01049 \u001b[0m(+0.00265)\n     | > avg_loss:\u001b[92m -0.00847 \u001b[0m(-0.01700)\n     | > avg_log_mle:\u001b[92m -0.22948 \u001b[0m(-0.01686)\n     | > avg_loss_dur:\u001b[92m 0.22101 \u001b[0m(-0.00014)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_25578.pth\n\n\u001b[4m\u001b[1m > EPOCH: 63/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:47:40) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 22/406 -- GLOBAL_STEP: 25600\u001b[0m\n     | > loss: -0.02988  (-0.04585)\n     | > log_mle: -0.22330  (-0.23805)\n     | > loss_dur: 0.19341  (0.19220)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 10.88537  (31.48271)\n     | > current_lr: 0.00002 \n     | > step_time: 0.56920  (0.58598)\n     | > loader_time: 0.00360  (0.00740)\n\n\n\u001b[1m   --> STEP: 47/406 -- GLOBAL_STEP: 25625\u001b[0m\n     | > loss: -0.01925  (-0.01957)\n     | > log_mle: -0.24587  (-0.23108)\n     | > loss_dur: 0.22662  (0.21151)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 46.42016  (33.55739)\n     | > current_lr: 0.00002 \n     | > step_time: 0.67720  (0.60519)\n     | > loader_time: 0.00340  (0.00756)\n\n\n\u001b[1m   --> STEP: 72/406 -- GLOBAL_STEP: 25650\u001b[0m\n     | > loss: -0.04379  (-0.02412)\n     | > log_mle: -0.23881  (-0.23624)\n     | > loss_dur: 0.19502  (0.21212)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 19.12154  (34.76474)\n     | > current_lr: 0.00002 \n     | > step_time: 0.67870  (0.62892)\n     | > loader_time: 0.01450  (0.00749)\n\n\n\u001b[1m   --> STEP: 97/406 -- GLOBAL_STEP: 25675\u001b[0m\n     | > loss: -0.01246  (-0.02703)\n     | > log_mle: -0.24632  (-0.24110)\n     | > loss_dur: 0.23386  (0.21408)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.56043  (36.24015)\n     | > current_lr: 0.00002 \n     | > step_time: 0.73320  (0.64578)\n     | > loader_time: 0.00490  (0.00767)\n\n\n\u001b[1m   --> STEP: 122/406 -- GLOBAL_STEP: 25700\u001b[0m\n     | > loss: -0.04145  (-0.02928)\n     | > log_mle: -0.24554  (-0.24450)\n     | > loss_dur: 0.20409  (0.21522)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 17.82437  (34.66547)\n     | > current_lr: 0.00002 \n     | > step_time: 0.83730  (0.67116)\n     | > loader_time: 0.00400  (0.00819)\n\n\n\u001b[1m   --> STEP: 147/406 -- GLOBAL_STEP: 25725\u001b[0m\n     | > loss: -0.03882  (-0.03007)\n     | > log_mle: -0.25931  (-0.24725)\n     | > loss_dur: 0.22050  (0.21718)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 65.02503  (37.54385)\n     | > current_lr: 0.00002 \n     | > step_time: 0.72580  (0.69100)\n     | > loader_time: 0.00450  (0.00842)\n\n\n\u001b[1m   --> STEP: 172/406 -- GLOBAL_STEP: 25750\u001b[0m\n     | > loss: -0.05070  (-0.03085)\n     | > log_mle: -0.26533  (-0.24927)\n     | > loss_dur: 0.21463  (0.21843)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 30.19793  (38.65537)\n     | > current_lr: 0.00002 \n     | > step_time: 0.88080  (0.71269)\n     | > loader_time: 0.01950  (0.00881)\n\n\n\u001b[1m   --> STEP: 197/406 -- GLOBAL_STEP: 25775\u001b[0m\n     | > loss: -0.03643  (-0.03163)\n     | > log_mle: -0.26484  (-0.25126)\n     | > loss_dur: 0.22841  (0.21963)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 59.03592  (39.78484)\n     | > current_lr: 0.00002 \n     | > step_time: 0.86500  (0.73637)\n     | > loader_time: 0.01140  (0.00927)\n\n\n\u001b[1m   --> STEP: 222/406 -- GLOBAL_STEP: 25800\u001b[0m\n     | > loss: -0.04812  (-0.03251)\n     | > log_mle: -0.27297  (-0.25314)\n     | > loss_dur: 0.22485  (0.22063)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 43.47553  (40.66724)\n     | > current_lr: 0.00002 \n     | > step_time: 1.01060  (0.76057)\n     | > loader_time: 0.00630  (0.00952)\n\n\n\u001b[1m   --> STEP: 247/406 -- GLOBAL_STEP: 25825\u001b[0m\n     | > loss: -0.02446  (-0.03284)\n     | > log_mle: -0.25510  (-0.25469)\n     | > loss_dur: 0.23064  (0.22186)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 54.30967  (41.64382)\n     | > current_lr: 0.00002 \n     | > step_time: 0.96680  (0.78148)\n     | > loader_time: 0.00570  (0.00972)\n\n\n\u001b[1m   --> STEP: 272/406 -- GLOBAL_STEP: 25850\u001b[0m\n     | > loss: -0.04641  (-0.03349)\n     | > log_mle: -0.27456  (-0.25596)\n     | > loss_dur: 0.22816  (0.22247)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 41.76953  (41.99820)\n     | > current_lr: 0.00002 \n     | > step_time: 1.02110  (0.80477)\n     | > loader_time: 0.00730  (0.00981)\n\n\n\u001b[1m   --> STEP: 297/406 -- GLOBAL_STEP: 25875\u001b[0m\n     | > loss: -0.04581  (-0.03389)\n     | > log_mle: -0.26931  (-0.25709)\n     | > loss_dur: 0.22349  (0.22319)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 63.15677  (42.60202)\n     | > current_lr: 0.00002 \n     | > step_time: 1.20900  (0.83076)\n     | > loader_time: 0.00760  (0.00976)\n\n\n\u001b[1m   --> STEP: 322/406 -- GLOBAL_STEP: 25900\u001b[0m\n     | > loss: -0.05087  (-0.03424)\n     | > log_mle: -0.27639  (-0.25816)\n     | > loss_dur: 0.22552  (0.22392)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 38.17508  (42.95095)\n     | > current_lr: 0.00002 \n     | > step_time: 1.13670  (0.85513)\n     | > loader_time: 0.00810  (0.00965)\n\n\n\u001b[1m   --> STEP: 347/406 -- GLOBAL_STEP: 25925\u001b[0m\n     | > loss: -0.04888  (-0.03462)\n     | > log_mle: -0.28229  (-0.25918)\n     | > loss_dur: 0.23341  (0.22456)\n     | > amp_scaler: 4096.00000  (4096.00000)\n     | > grad_norm: 92.52539  (43.32924)\n     | > current_lr: 0.00002 \n     | > step_time: 1.24430  (0.88201)\n     | > loader_time: 0.00820  (0.00960)\n\n\n\u001b[1m   --> STEP: 372/406 -- GLOBAL_STEP: 25950\u001b[0m\n     | > loss: -0.02148  (-0.03489)\n     | > log_mle: -0.25972  (-0.25997)\n     | > loss_dur: 0.23824  (0.22508)\n     | > amp_scaler: 2048.00000  (3969.37634)\n     | > grad_norm: 54.34829  (44.09114)\n     | > current_lr: 0.00002 \n     | > step_time: 1.32520  (0.91085)\n     | > loader_time: 0.00850  (0.00960)\n\n\n\u001b[1m   --> STEP: 397/406 -- GLOBAL_STEP: 25975\u001b[0m\n     | > loss: -0.04581  (-0.03527)\n     | > log_mle: -0.27286  (-0.26084)\n     | > loss_dur: 0.22705  (0.22558)\n     | > amp_scaler: 2048.00000  (3848.38287)\n     | > grad_norm: 63.50391  (44.14135)\n     | > current_lr: 0.00002 \n     | > step_time: 1.06560  (0.93860)\n     | > loader_time: 0.00920  (0.00968)\n\n\n\u001b[1m > EVALUATION \u001b[0m\n\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 131\n | > Preprocessing samples\n | > Max text length: 174\n | > Min text length: 20\n | > Avg text length: 100.76335877862596\n | \n | > Max audio length: 222643.0\n | > Min audio length: 34739.0\n | > Avg audio length: 144033.41221374046\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\nwarning: audio amplitude out of range, auto clipped.\n | > Synthesizing test sentences.\n","output_type":"stream"},{"name":"stderr","text":"\n  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n     | > avg_loader_time:\u001b[92m 0.00697 \u001b[0m(-0.00352)\n     | > avg_loss:\u001b[92m -0.01722 \u001b[0m(-0.00875)\n     | > avg_log_mle:\u001b[92m -0.24054 \u001b[0m(-0.01106)\n     | > avg_loss_dur:\u001b[91m 0.22332 \u001b[0m(+0.00231)\n\n > BEST MODEL : ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000/best_model_25984.pth\n\n\u001b[4m\u001b[1m > EPOCH: 64/1000\u001b[0m\n --> ./output/fakeljspeech/run-February-11-2023_02+03PM-0000000\n","output_type":"stream"},{"name":"stdout","text":"\n\n> DataLoader initialization\n| > Tokenizer:\n\t| > add_blank: False\n\t| > use_eos_bos: False\n\t| > use_phonemes: True\n\t| > phonemizer:\n\t\t| > phoneme language: en-us\n\t\t| > phoneme backend: gruut\n\t| > 3 not found characters:\n\t| > ͡\n\t| > “\n\t| > ”\n| > Number of instances : 12969\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m > TRAINING (2023-02-11 21:54:36) \u001b[0m\n","output_type":"stream"},{"name":"stdout","text":" | > Preprocessing samples\n | > Max text length: 188\n | > Min text length: 13\n | > Avg text length: 100.90014650319993\n | \n | > Max audio length: 222643.0\n | > Min audio length: 24499.0\n | > Avg audio length: 144984.29755570978\n | > Num. instances discarded samples: 0\n | > Batch group size: 0.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[1m   --> STEP: 16/406 -- GLOBAL_STEP: 26000\u001b[0m\n     | > loss: -0.02454  (-0.04772)\n     | > log_mle: -0.23433  (-0.23931)\n     | > loss_dur: 0.20979  (0.19159)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 29.29327  (36.50051)\n     | > current_lr: 0.00002 \n     | > step_time: 0.64130  (0.59466)\n     | > loader_time: 0.00950  (0.00772)\n\n\n\u001b[1m   --> STEP: 41/406 -- GLOBAL_STEP: 26025\u001b[0m\n     | > loss: -0.03381  (-0.03701)\n     | > log_mle: -0.23240  (-0.23606)\n     | > loss_dur: 0.19859  (0.19904)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 50.47242  (32.42015)\n     | > current_lr: 0.00002 \n     | > step_time: 0.60800  (0.60579)\n     | > loader_time: 0.00390  (0.00797)\n\n\n\u001b[1m   --> STEP: 66/406 -- GLOBAL_STEP: 26050\u001b[0m\n     | > loss: -0.00996  (-0.03561)\n     | > log_mle: -0.24516  (-0.23917)\n     | > loss_dur: 0.23520  (0.20356)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 28.56041  (38.79124)\n     | > current_lr: 0.00002 \n     | > step_time: 0.75810  (0.63076)\n     | > loader_time: 0.00490  (0.00816)\n\n\n\u001b[1m   --> STEP: 91/406 -- GLOBAL_STEP: 26075\u001b[0m\n     | > loss: -0.05265  (-0.03718)\n     | > log_mle: -0.27050  (-0.24339)\n     | > loss_dur: 0.21785  (0.20620)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 33.18447  (36.46532)\n     | > current_lr: 0.00002 \n     | > step_time: 0.64960  (0.64922)\n     | > loader_time: 0.01110  (0.00852)\n\n\n\u001b[1m   --> STEP: 116/406 -- GLOBAL_STEP: 26100\u001b[0m\n     | > loss: -0.02701  (-0.03798)\n     | > log_mle: -0.24709  (-0.24702)\n     | > loss_dur: 0.22008  (0.20905)\n     | > amp_scaler: 2048.00000  (2048.00000)\n     | > grad_norm: 42.60112  (37.83427)\n     | > current_lr: 0.00002 \n     | > step_time: 0.71730  (0.67794)\n     | > loader_time: 0.02670  (0.00919)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}